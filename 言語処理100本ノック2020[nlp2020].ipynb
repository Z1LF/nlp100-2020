{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "言語処理100本ノック2020[nlp2020].ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ExnBfpTnY-hJ",
        "qJbCeI_9Etpb",
        "N8LrAkjcDWMW",
        "TiHqMcclED6B",
        "zZ06yBtKEIJx",
        "8jkcGrsWEL_n",
        "SkHpEeLmEObx",
        "Imyqw0ffEQIv",
        "nIOikAjiESFx",
        "WhYuAmdaEUq9",
        "wLGsCLQpEWdC",
        "XYef5GAZEX9r",
        "oeaShrnkFXUu",
        "oreVQ8KdFXU0",
        "Ju_6iik5FXU0",
        "IH4RiaQbFXU1",
        "3tC88THbFXU1",
        "_nBP6zSgFXU1",
        "gH__H51uFXU1",
        "B-n_nvttFXU2",
        "MxfCADGMFXU2",
        "OJ_CfkueFXU2",
        "xCyzhPazFXU2",
        "bPOa1uS0FuFZ",
        "bXLpbW81FuFo",
        "9yQsM0QzFuFo",
        "27f_mJLkFuFp",
        "ByFpF3jyFuFp",
        "50TgE0duFuFq",
        "24m8DBMKFuFq",
        "b_TWlK7XFuFq",
        "Zkk3HaqCFuFr",
        "QTYd8NJ9FuFr",
        "tg1CPd0BFuFr",
        "8t09a_dWGBrU",
        "TKE39_IJMWvh",
        "WdfbPA5qGBrj",
        "whtu8E8XGBrk",
        "1yHL2j5KGBrl",
        "F-dC7WJ7GBrl",
        "gsh5EawtGBrl",
        "pdyOrgv8GBrm",
        "K2Z0Gp0RGBrm",
        "TIBgB5lkGBrm",
        "NvdNnDEsGBrn",
        "3uTbIWTWGBrn",
        "7mrrvSw6Sjku",
        "WhmGwHRUGhid",
        "jW31L0z5JH_D",
        "efvr7lJWGCvA",
        "NVOr6AErGCvA",
        "EkCDSwTMGCvA",
        "tki4hy54GCvB",
        "tZR-DWmxGCvB",
        "H_gcQUPXiCFJ",
        "DE2s5d33h1oo",
        "1Q1TeJ6M_ZY-",
        "KsTUgQ7qGCvB",
        "ZY2_LUvmGCvB",
        "0dFw4MJIGCvC",
        "ASM7F7ZMGCvC",
        "cNWf4DGTGUnz",
        "fFWgl4_HGUn7",
        "NMOIVGPMGUoA",
        "A_fskrYuGDe3",
        "HxRHhP-yGd-S"
      ],
      "authorship_tag": "ABX9TyNqf9sJad7WaPtwpMxEzmtl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z1LF/nlp100-2020/blob/main/%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF2020%5Bnlp2020%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 言語処理100本ノック 2020\n",
        "\n",
        "- 問題リンク[ https://nlp100.github.io/ja/ ]\n",
        "- GitHub [ https://github.com/nlp100/nlp100.github.io ]\n",
        "- 参考リンク\n",
        "  - 文字列処理に関する情報 | note.nkmk.me [ https://note.nkmk.me/string/ ] \n",
        "    >Pythonやるときはこのサイトにお世話にならないと何もできないので…\n",
        "\n",
        "    \n",
        "      "
      ],
      "metadata": {
        "id": "BAYv_FCnErn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 解答例 "
      ],
      "metadata": {
        "id": "ExnBfpTnY-hJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Links\n",
        "\n",
        "- 【言語処理100本ノック 2020】Pythonによる解答例まとめ by @yanmaru\n",
        "  - [Qiita](https://qiita.com/yamaru/items/0cac24710626333bd693)\n",
        "  > 解答例とはいえいきなり解答が書いてあって、その後に参考リンクが記されている\n",
        "    自分で悩んで解いた後に参考程度に見るのが良さそう\n",
        "    \n",
        "- 「言語処理100本ノック 2020」をPythonで解く by u++(upura)\n",
        "  - [はてなブログ](https://upura.hatenablog.com/entry/2020/04/14/024948)\n",
        "  - [GitHub](https://github.com/upura/nlp100v2020)\n",
        "  > ブログの方では解説や参考記事も載っているので見やすい。\n",
        "\n",
        "- Python-機械学習-自然言語処理-言語処理100本ノック 2020\n",
        "  - [ブログ](https://www.takapy.work/archive/category/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%202020)\n",
        "  > 個人ブログ。解説とかはなく単純に動くコードが乗っている備忘録的なもの。\n",
        "\n",
        "- 100本ノックシリーズ by tomowarkar\n",
        "  - [GitHub blog](https://tomowarkar.github.io/blog/posts/100series/)\n",
        "  > 第5章のCaboChaの導入で見つけたブログ。\n",
        "    言語処理だけじゃなくデータサイエンス100本ノックもやってるみたい。\n",
        "\n",
        "\n",
        "- Pythonで自然言語処理100本ノック 2020を解いたついでに死ぬほど詳しく解説を書いていく[第1章 準備運動] by @python_kenichi\n",
        "  - [Qiita](https://qiita.com/python_kenichi/items/b1fcecc4274511e4c26e)\n",
        "  > 第1章のみだが丁寧に考え方や必要な知識を解説してくれている。続きがないのが惜しまれる。"
      ],
      "metadata": {
        "id": "7n5fEAYeZMbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第1章: 準備運動"
      ],
      "metadata": {
        "id": "qJbCeI_9Etpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 00. 文字列の逆順\n",
        "文字列\"stressed\"の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．\n",
        "\n"
      ],
      "metadata": {
        "id": "N8LrAkjcDWMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8LNmNEUDVdc"
      },
      "outputs": [],
      "source": [
        "str = \"stressed\"\n",
        "ans = str[::-1]\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01. 「パタトクカシーー」\n",
        "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．\n"
      ],
      "metadata": {
        "id": "TiHqMcclED6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str=\"パタトクカシーー\"\n",
        "print(str[::2])"
      ],
      "metadata": {
        "id": "ymEKDM4oEbw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
        "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．\n",
        "\n"
      ],
      "metadata": {
        "id": "zZ06yBtKEIJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = \"パトカー\"\n",
        "str2 = \"タクシー\"\n",
        "ans = \"\".join([(str1[i]+str2[i]) for i in range(4)])\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "PE0gdpBVEHzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 03. 円周率\n",
        "\"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．\n",
        "\n",
        "- `str.split(\"X\")` = 文字Xを基準に分割を行う\n",
        "  - デフォの引数はスペースなので別にスペース入れる必要はなかったかも\n",
        "- `str.strip(\"X\")` = 文字列の先頭・末尾にある文字Xを除去する\n",
        "\n",
        "> 最初普通にsplitだけで「できたやん！」と思ったけど出力されたリストが\n",
        "  [3,1,4,1,6...]←！？となっていてカンマの存在に気づくなど"
      ],
      "metadata": {
        "id": "8jkcGrsWEL_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
        "sep_str = str.split()\n",
        "[len(s.strip(\",. \")) for s in sep_str]"
      ],
      "metadata": {
        "id": "6mcyVFiOEOHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04. 元素記号\n",
        "\"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．\n",
        "\n",
        "- `str.strip`を使わなくても`str.replace`で良かった…てかコッチのほうが汎用性高いし…\n",
        "- len(str)=intはイテラブルなオブジェクトではないのでちゃんとrange()する。\n",
        "- `値1 if [条件式1] else 値2 if [条件式2] else 値3`\n",
        "  - というふうにするとPythonで三項演算子が実装できる。\n",
        "  - 条件式1がTrueのときは値1…という感じ。\n",
        "- 辞書は`{}`を使って定義。\n",
        "  - `Dict[Key]=Value`で辞書DictのKeyの項目に値Valueを追加する\n",
        "\n",
        "> Pythonでも三項演算子のようなものが使えるらしい。リスト内包表記といい、後置といい、ifの置き方が自由すぎる\n",
        "\n",
        "> 連想配列、どの言語でも表記が思い出せなくてうまく使えないがち。\n",
        "\n",
        "#### enumerate\n",
        "`enumerate(Object)`とすると`[インデックス番号,要素]`が戻ってくる\n",
        "- コレを使うと簡単に書ける(追記コードブロック)"
      ],
      "metadata": {
        "id": "SkHpEeLmEObx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
        "numList = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
        "str = str.replace((\",\"or\".\"),\"\").split(\" \")\n",
        "ans = {}\n",
        "\n",
        "for i in range(len(str)):\n",
        "  str_head = str[i][0] if (i+1 in numList) else str[i][0:2]\n",
        "  #print(str_head)\n",
        "  ans[str_head] = i\n",
        "\n",
        "ans"
      ],
      "metadata": {
        "id": "D6B4gJsVEPqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_str = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
        "numList = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
        "str = raw_str.replace((\",\"or\".\"),\"\").split(\" \")\n",
        "ans = []\n",
        "\n",
        "for i, word in enumerate(str):\n",
        "  str_head = word[0] if (i+1 in numList) else word[:2] \n",
        "  ans.append([str_head,i])\n",
        "\n",
        "dict(ans)"
      ],
      "metadata": {
        "id": "MtrIrjFy1AJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 05. n-gram\n",
        "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，\"I am an NLPer\"という文から単語bi-gram，文字bi-gramを得よ．\n",
        "\n",
        "> まず「n-gram」ってなんやねん！！！\n",
        "  参考 → N-gramの作り方 @kazmaw | Qiita [https://qiita.com/kazmaw/items/4df328cba6429ec210fb]\n",
        "  n-gramは文字列などにおける連続したn個のまとまりでグループ化したもの\n",
        "\n",
        "- n個連続で拾う際にカウントは「最後の文字が含まれるまで」なので範囲は`len(X)-n+1`となることに注意\n",
        "- for文で一旦書いてから(できそうなら)内包表記に変換するとやりやすいかも(主観)\n",
        "\n",
        "> n-gramがわかれば大したことはしていない\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Imyqw0ffEQIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N_gram(n,X): # オブジェクトXのn-gramを返す関数\n",
        "  ## nがオブジェクト全体の長さよりも大きい場合はとりあえず全部返すようにしようか…わかんないけど…\n",
        "  if len(X) < n:\n",
        "    return [X]\n",
        "\n",
        "  \"\"\"\n",
        "  # for文による表記\n",
        "  ret=[]\n",
        "  for i in range(len(X)-n+1):\n",
        "    ret.append(X[i:i+n])\n",
        "  \"\"\"\n",
        "  ret = [X[i:i+n] for i in range(len(X)-n+1)]\n",
        "  \n",
        "  return ret\n",
        "\n",
        "raw_text = \"I am an NLPer\"\n",
        "words = raw_text.split(\" \")\n",
        "\n",
        "print(\"--- 文字 n-gram ---\")\n",
        "print(N_gram(1,raw_text))\n",
        "print(N_gram(2,raw_text))\n",
        "print(N_gram(3,raw_text))\n",
        "\n",
        "\n",
        "print(\"--- 単語 n-gram ---\")\n",
        "print(N_gram(1,words))\n",
        "print(N_gram(2,words))\n",
        "print(N_gram(3,words))\n"
      ],
      "metadata": {
        "id": "2zS9UmqWEcpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 06. 集合\n",
        "\"paraparaparadise\"と\"paragraph\"に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．\n",
        "\n",
        "- `set`型オブジェクト [ https://note.nkmk.me/python-set/ ]\n",
        "  - set型は重複しない要素の集合オブジェクト。\n",
        "  - 定義の際は`{}`で生成できる\n",
        "    - 空のsetを作りたいときは単に`s={}`とするとdict型になってしまうので、`s=set()`と明示的にコンストラクタを呼んで定義する必要がある\n",
        "  - 内部には異なる型のオブジェクトを(変更不能なイミュータブルな型であれば)含むことが出来る。\n",
        "    - 変数やlistのような更新可能な(ミュータブルな)オブジェクトは不可能\n",
        "  - set()を利用してリストやタプルから重複要素を取り除けるが、順序は保持されない。\n",
        "\n",
        "  - 関数など\n",
        "    - `set_A | set_B` または `set_A.union(set_B)` でAとBの **和集合** を得られる\n",
        "    - `set_A & set_B` または `set_A.intersection(set_B)` でAとBの **積集合** を得られる\n",
        "    - `set_A - set_B` または `set_A.difference(set_B)` でAとBの **差集合** を得られる (AがBより大きい場合)\n",
        "    - `set_A ^ set_B` または `set_A.symmetric_difference(set_B)` でAとBの **対称差集合** を得られる (XOR=どちらか片方にのみ含まれる要素の集合)\n",
        "    - `set_A.isdisjoint(set_B)` でAとBが **互いに素かどうか** を判定できる\n",
        "\n",
        "- print内で使えるformat関数\n",
        "  - `print(\"hoge : {0}, fuga : {1}\".format(ans1, ans2)` みたいに書くと勝手にansが代入できる。\n",
        "  - 文字列と変数を同時にprintしたいときに便利\n",
        "  - 毎回使おうとするけど表記が思い出せないやつ……。\n",
        "\n",
        "- ｆ文字列 [Python v3.6以降]\n",
        "  - `f\"hoge : {ans1}, fuga : {ans2}\"`で上記のformat記法と同じ結果が出せるやつ。\n",
        "  - 書き方としてはコッチのほうがかなりわかりやすい"
      ],
      "metadata": {
        "id": "nIOikAjiESFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N_gram(n,X): # オブジェクトXのn-gramを返す関数\n",
        "  if len(X) < n:\n",
        "    return [X]\n",
        "\n",
        "  ret = [X[i:i+n] for i in range(len(X)-n+1)]\n",
        "  return ret\n",
        "\n",
        "text1 = \"paraparaparadise\"\n",
        "text2 = \"paragraph\"\n",
        "\n",
        "# 文字 bi-gram の集合\n",
        "X = N_gram(2,text1)\n",
        "setX = set(X)\n",
        "Y = N_gram(2,text2)\n",
        "setY = set(Y)\n",
        "\n",
        "\n",
        "print(\"X | Y : {0}\".format(setX | setY))\n",
        "\n",
        "print(\"X & Y : {0}\".format(setX & setY))\n",
        "\n",
        "print(\"X - Y : {0}\".format(setX - setY))\n",
        "\n",
        "\n",
        "print('\"se\" in X : {0}'.format(\"se\" in setX))\n",
        "\n",
        "print('\"se\" in Y : {0}'.format(\"se\" in setY))"
      ],
      "metadata": {
        "id": "0DA_btFBERzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 07. テンプレートによる文生成\n",
        "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=\"気温\", z=22.4として，実行結果を確認せよ．\n",
        "\n",
        "> 偶然前の問題でformat関数を解説してしまったので一瞬で解けて終わった…"
      ],
      "metadata": {
        "id": "WhYuAmdaEUq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def templete(x,y,z):\n",
        "  return \"{0}時の{1}は{2}\".format(x,y,z)\n",
        "\n",
        "print(templete(12,\"気温\",22.4))"
      ],
      "metadata": {
        "id": "YFFP2kiQEdNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 08. 暗号文\n",
        "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
        "\n",
        "* 英小文字ならば(219 - 文字コード)の文字に置換\n",
        "* その他の文字はそのまま出力\n",
        "\n",
        "この関数を用い，英語のメッセージを暗号化・復号化せよ．\n",
        "\n",
        "#### 大文字/小文字の判定\n",
        "- `str.islower()`文字列strが全て小文字のときにTrueを返す\n",
        "  - 全角文字であっても大文字小文字の区別があれば判定される\n",
        "  - 全て数字や日本語の様に大文字と小文字の区別がない文字種の時はfalse\n",
        "\n",
        "- 同様に`str.isupper()`は文字列strが全て大文字のときにTrueを返す\n",
        "- `str.istitle()`は文字列strが最初の文字だけ大文字、他はすべて小文字のときにTrueを返す\n",
        "\n",
        "#### 文字コードの変換\n",
        "- `ord('文字') = asciiコード`\n",
        "- `chr(数値) = 文字`\n",
        "\n",
        "\n",
        "> どっちの作業も無知だたので…、"
      ],
      "metadata": {
        "id": "wLGsCLQpEWdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cipher(text):\n",
        "  ans = \"\"\n",
        "  for c in text:\n",
        "    ans += chr(219-ord(c)) if(c.islower()) else c\n",
        "  return ans\n",
        "\n",
        "message = \"This Text is 2022年 Marchに 作られた 暗号です !!\"\n",
        "print(cipher(message))\n",
        "## out > Tsrh Tvcg rh 2022年 Mzixsに 作られた 暗号です !!\n",
        "\n",
        "## 219-ord(c)で暗号化したものは、219-(219-ord(c))=ord(c)なので同じ関数で複合可能\n",
        "encrypted_message = \"Tsrh Tvcg rh 2022年 Mzixsに 作られた 暗号です !!\"\n",
        "print(cipher(encrypted_message))"
      ],
      "metadata": {
        "id": "iJ71003AEWMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 09. Typoglycemia\n",
        "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば\"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"）を与え，その実行結果を確認せよ．\n",
        "\n",
        "#### ランダムな並び替え [ https://note.nkmk.me/python-random-shuffle/ ]\n",
        "- `import random`でrandomモジュールを使用可能\n",
        "- `random.shuffle(list)`で元のリストをシャッフルする。\n",
        "- `random.sample(list)`で元のリストは変更せず、シャッフルされた新たなリストを作成する。\n",
        "  - 文字列やタプルは **変更不能(イミュータブル)な** オブジェクトなので`shuffle`を使うとエラーになる！注意\n",
        "  - 適用するときは`sample`の方を使おう\n",
        "- listしかシャッフルできないのでstrもリストの形式にネスト[list]する必要がある。\n",
        "\n",
        "- 文字列の結合は`\"<Space>\".join(list)`ですね……(第2問)"
      ],
      "metadata": {
        "id": "XYef5GAZEX9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def Typoglycemia(str):\n",
        "  if len(str)<=4:\n",
        "    return str\n",
        "  else:\n",
        "    shuffle = random.sample(str[1:-1], len(str[1:-1]))\n",
        "    return \"\".join([str[0]] + shuffle + [str[-1]])\n",
        "\n",
        "text = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
        "ans = [Typoglycemia(words) for words in text.split(\" \")]\n",
        "ans = \" \".join(ans)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "IVwxEd1FEYPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第2章: UNIXコマンド\n",
        "\n",
        "popular-names.txtは，アメリカで生まれた赤ちゃんの「名前」「性別」「人数」「年」をタブ区切り形式で格納したファイルである．以下の処理を行うプログラムを作成し，popular-names.txtを入力ファイルとして実行せよ．さらに，同様の処理をUNIXコマンドでも実行し，プログラムの実行結果を確認せよ．\n",
        "\n",
        "#### Linuxコマンドの使用\n",
        "Google Colaboratory上ではコマンド部に`!`をPrefixとしてLinuxコマンドが使用できる\n",
        "\n",
        "#### データのダウンロード\n",
        "Google Colaboratory上で指定データを使用するにはwgetコマンドを使用する\n",
        "> `!wget https://nlp100.github.io/data/popular-names.txt`\n",
        "<br>これを実行するとカレントディレクトリに対象のテキストファイルを置くことが出来る"
      ],
      "metadata": {
        "id": "oeaShrnkFXUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/popular-names.txt"
      ],
      "metadata": {
        "id": "n9AOmWoLT64c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cC8xCmK74J4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. 行数のカウント\n",
        "行数をカウントせよ．確認にはwcコマンドを用いよ．\n",
        "\n",
        "#### データの読み込み\n",
        "- pandasを利用 [ https://note.nkmk.me/python-pandas-read-csv-tsv/ ]\n",
        "  ##### 読み込み時メモ\n",
        "  - `pd.read_csv`は区切り文字が`,`で固定されているだけで`pd.read_table`と根本的に同じ。\n",
        "    > `pd.read_csv(hoge)` == `pd.read_table(hoge, sep=\",\")`\n",
        "  - read時に`names`オプションでカラム名を名付ける\n",
        "\n",
        "#### 確認\n",
        "- Linuxの`wc`コマンドは`$wc <filename>`で使用する\n",
        "  - 順に[行数, 単語数(空白で区切られた文字列の数), ファイル容量(byte)]で結果が表示される\n",
        "\n",
        "> Google Colaboratoryでローカルなデータ扱うの不利じゃない！？と思ったけど普通にできるんだ。<br>\n",
        "  Linuxのコマンドも普通に使えるっぽいですね"
      ],
      "metadata": {
        "id": "oreVQ8KdFXU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXfRnG9YFXU0"
      },
      "outputs": [],
      "source": [
        "# データがディレクトリに存在するか確認\n",
        "!ls\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print(df)\n",
        "\n",
        "print(f\"行数 : {len(df)}\\n\")\n",
        "\n",
        "!wc popular-names.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. タブをスペースに置換\n",
        "タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．\n",
        "\n",
        "#### ~~「確認」~~→置換に使用するコマンド\n",
        "- sedコマンド : *__S__tream __ED__itor* `sed [option] <filename>`\n",
        "  - sedは指定したファイルをコマンドに従って処理し、標準出力へと出力する。のでパイプとして使うのが一般的。\n",
        "  - オプション`-e`で処理内容を設定(省略時はオプション以外の第一引数が処理内容)\n",
        "    - `$ cat ... | sed -e ... -e ...`のように連続した処理も可能\n",
        "  - `-s` : 正規表現を使った置換を行う。\n",
        "- trコマンド : _**TR**anslate/**TR**ansliterate_ `tr [置換前の文字セット] [置換後の文字セット] <in_filename> <out>`\n",
        "  - `$ tr 012 abc`とすると [0→a,1→b,2→c] という置換が行われる。\n",
        "    - 1対1対応なので常に長さを揃える必要がある\n",
        "  -\n",
        "- expandコマンド `expand [option] <filename>`\n",
        "  - タブを空白に置き換えるコマンド\n",
        "    - `-t`オプションでタブを幾つの空白文字に置き換えるか設定する。デフォルトは8個分。\n",
        "  - 逆に、unexpandコマンド `unexpand [option] <filename>`は空白をタブに置換する\n",
        "\n",
        "\n",
        "#### 本当に確認用のコマンド\n",
        "- `head <filename>`\n",
        "  - 指定したファイルを頭から表示する。デフォルトでは先頭10行が表示される。\n",
        "  - `-c`で文字数、`-n`で行数を指定する\n",
        "\n",
        "> なぜかsedだけTabの幅がSpaceでも残ってしまった…どうすれば消えてくれるんだ…まあべつにええか…"
      ],
      "metadata": {
        "id": "Ju_6iik5FXU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -e \"s/<tab>/ /g\" popular-names.txt | head -v -n 5\n",
        "!cat popular-names.txt | tr \"\\t\" \" \"  | head -v -n 5\n",
        "!expand popular-names.txt -t 1 | head -v -n 5"
      ],
      "metadata": {
        "id": "J7jprPLSFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. 1列目をcol1.txtに，2列目をcol2.txtに保存\n",
        "各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．\n",
        "\n",
        "- cutコマンド : `cut [option] <filename>`\n",
        "  - ファイルを読み込んで、それぞれの行から指定した部分だけを切り出すコマンド。\n",
        "  - 切り出す部分の指定は`-[範囲指定オプション] <数字>`で指定する\n",
        "    - `-b`でバイト単位、`-c`で文字数単位、`-f`でフィールド(区切り文字毎)\n",
        "    > `cut -f 1,7 --delim=\":\" hoge.txt` -> hoge.txtの\":\"で区切られた部分の1番目と7番目の列を拾ってくる。\n",
        "  - `-d`,`--delimiter` オプションでどの文字を区切りとするか指定できる。デフォルトではタブが区切り文字になっている。\n",
        "\n",
        "#### pandasを使用する場合\n",
        "pandasではデフォルトで列切り出しが可能\n",
        "- `df[n]`でn列目を拾ってこれる\n",
        "  - カラム名を指定していたらその文字列で指定する必要がある\n",
        "- `df.to_csv(<filename>, index=False)`とかやれば書き出しができる\n",
        "  - 行番号を消すオプションは`index=False`"
      ],
      "metadata": {
        "id": "IH4RiaQbFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | head -n 5\n",
        "!cut -f 1 popular-names.txt > col1.txt\n",
        "!cut -f 2 popular-names.txt | head -n 5\n",
        "!cut -f 2 popular-names.txt > col2.txt\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print(df[\"Name\"][:5])\n",
        "print(df[\"Sex\"][:5])\n"
      ],
      "metadata": {
        "id": "WkpEVhdTFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. col1.txtとcol2.txtをマージ\n",
        "12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．\n",
        "\n",
        "- Pasteコマンド : `$ paste <file_1> <file_2>`\n",
        "  - 複数のファイルを行で連結するコマンド。\n",
        "  - `-d`オプションを使うと連結時の区切り文字を変更できる。デフォルトではタブ。\n",
        "\n",
        "#### pandasを使用する場合\n",
        "特定の行・列を抽出する場合はlocが結局無難\n",
        "- `df.loc[行 , 列]` : 指定した行と列を拾ってくる。\n",
        "  - 行・列は複数のlist表記も可能。\n",
        "- `df.iloc[行 , 列]` : 「数字で」指定した行と列を拾ってくる。\n",
        "  - locの指定がindex名やcolumn名なのに対してこちらは番号で指定できる"
      ],
      "metadata": {
        "id": "3tC88THbFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!paste col1.txt col2.txt | head -n 5\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print( df.iloc[0:5,0:2] )"
      ],
      "metadata": {
        "id": "CnuWxJRTFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. 先頭からN行を出力\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．\n",
        "\n",
        "#### headコマンド\n",
        "ファイルの上からn行を表示する。\n",
        "> もう今まで確認で散々使っとるやんけ……………！！！！\n",
        "\n",
        "\n",
        "#### Pythonでコマンドライン引数を受け取る\n",
        "`import sys`モジュール内の`argv`を使用する。\n",
        "- `args = sys.argv`で呼び出し時にコマンドラインによって与えた引数がargsに格納される\n",
        "- `args[0]`は実行ファイル名(`hoge.py`)になる\n"
      ],
      "metadata": {
        "id": "_nBP6zSgFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head popular-names.txt -n 5\n",
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowHead(df,n):\n",
        "  print(df.head(n))\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "\n",
        "ShowHead(df,args[1]) if len(args)==2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "8VFMVOOIFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. 末尾のN行を出力\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．\n",
        "\n",
        "> 末尾を参照するTailになっただけでやることは1個前と完全に同じ。"
      ],
      "metadata": {
        "id": "gH__H51uFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tail popular-names.txt -n 5\n",
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowTail(df,n):\n",
        "  print(df.tail(n))\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "ShowTail(df, 3)\n",
        "ShowTail(df,args[1]) if len(args)==2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "1j0SXdREFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. ファイルをN分割する\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．\n",
        "\n",
        "#### splitコマンド\n",
        "`!split -n 5 popular-names.txt`みたいな感じにすれば分割できるけど、分割したらファイルが勝手に出力されるので面倒なことになってしまう。Pandasだけでええか…\n",
        "\n",
        "\n",
        "> これもやってることは「分割の計算」→「切り出し」なので、表示さえできればやるだけ"
      ],
      "metadata": {
        "id": "B-n_nvttFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!split -n 5 popular-names.txt | head -n 5\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowHeadAndTail(df,n=2): ## 全部表示すると流石に面倒なので上下のn行だけ表示するようにして確認\n",
        "  print(df.head(n))\n",
        "  print(\" ……… \")\n",
        "  print(df.tail(n))\n",
        "\n",
        "def Separate(df,n):\n",
        "  size = int(len(df)/n) # 先に区切りの数を決めておくとキャスティングが統一されてラク\n",
        "\n",
        "  for i in range(n):\n",
        "    ShowHeadAndTail(df[i*size:(i+1)*size], 2)\n",
        "    print(\"----- -----\")\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "args[1] = 5 if len(args)>=2 else args[1]\n",
        "Separate(df,args[1]) if len(args)>=2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "428FqOK8FXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. １列目の文字列の異なり\n",
        "1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはcut, sort, uniqコマンドを用いよ．\n",
        "\n",
        "#### uniq コマンド\n",
        "重複している行を取り除くコマンド。 [ https://atmarkit.itmedia.co.jp/ait/articles/1611/14/news021.html ]\n",
        "- ただし、**uniqコマンドは隣り合った行しか比較しない**ので、先にsortコマンドを使って並べ替えるのが一般的らしい\n",
        "\n",
        "ちなみに、sortコマンドに`-u`という重複行を取り除くオプションがあるらしいのであんまり意味ないな…\n",
        "\n",
        "\n",
        "#### Pythonを使用した解答\n",
        "> 先の問題で「set」を使用した重複の排除があったので思わずそっちを使ったが、Pandasくんに便利な機能がある\n",
        "\n",
        "- `df.duplicated()` : 重複した行を抽出する\n",
        "  - 戻り値は重複した行がTrueとなるBool値の行。デフォルトでは全ての列要素が一致したときに同じ行とみなされる。\n",
        "  - 引数`subset`を指定することで同じ要素であるときに重複と判定するカラムを指定できる\n",
        "    - `df.duplicated(subset=\"Name\")`みたいな感じ\n",
        "- `df.drop_duplicates()` : 重複した行を削除する\n",
        "  - この辺のメソッドを使用すると重複した行をカウントできる"
      ],
      "metadata": {
        "id": "MxfCADGMFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | sort | uniq | wc"
      ],
      "metadata": {
        "id": "8EXYXKElDMpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "Column_1_set = set(df.iloc[:,0])\n",
        "len(Column_1_set)"
      ],
      "metadata": {
        "id": "H8RZ6MkJFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. 各行を3コラム目の数値の降順にソート\n",
        "各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．"
      ],
      "metadata": {
        "id": "OJ_CfkueFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sMC0YogFFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "df.sort_values([\"Population\"])"
      ],
      "metadata": {
        "id": "fDnTABk3Enmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる\n",
        "各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．\n",
        "\n",
        "- `uniq -c`でuniqの重複時に出現回数をカウントする\n",
        "- `sort -n`でソート時に数字として認識する\n",
        "  - コレをしないと辞書順で判定されるため [1743] が [94] より先に出てくることになる\n",
        "\n",
        "#### Pandasを使用した解答\n",
        "Pandasを利用するのであれば`pandas.Series.value_counts()`を使用することでユニークな要素のindexと個数を取得できる\n",
        "- デフォルトでは出現回数が多いものから順にソートされる\n",
        "  - 引数ascending=Trueとすると昇順にソートされ、引数sort=Falseとするとソートされない。"
      ],
      "metadata": {
        "id": "xCyzhPazFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | sort | uniq -c | sort -n -r | head -n 10"
      ],
      "metadata": {
        "id": "e5jxJB9_F2M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "df[\"Name\"].value_counts()"
      ],
      "metadata": {
        "id": "4S_PWR7RFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第3章: 正規表現\n",
        "\n",
        "Wikipediaの記事を以下のフォーマットで書き出したファイルjawiki-country.json.gzがある．\n",
        "- 1行に1記事の情報がJSON形式で格納される\n",
        "- 各行には記事名が”title”キーに，記事本文が”text”キーの辞書オブジェクトに格納され，そのオブジェクトがJSON形式で書き出される\n",
        "- ファイル全体はgzipで圧縮される\n",
        "\n",
        "らしいので例によってまずデータをローカルに保存します…。\n",
        "- `!wget https://nlp100.github.io/data/jawiki-country.json.gz`\n",
        "- `!gunzip jawiki-country.json.gz`\n",
        "\n",
        "とすると`jawiki-country.json`がローカルに配置される。\n",
        "> ちなみに、gzipを解凍(伸張)すると元の圧縮ファイルは自動で消去される。\n",
        "  そのため実質的には変換を行っているのと同等。"
      ],
      "metadata": {
        "id": "bPOa1uS0FuFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/jawiki-country.json.gz"
      ],
      "metadata": {
        "id": "ndIAHDnkJySw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip jawiki-country.json.gz"
      ],
      "metadata": {
        "id": "T2vF6vFhKR2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "VyJWZWXKzbz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. JSONデータの読み込み\n",
        "Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．\n",
        "**問題21-29では，ここで抽出した記事本文に対して実行**せよ．\n",
        "\n",
        "#### Pandasを使用したJSONの扱い\n",
        "- `pd.read_json(<filename>)`でJSON形式のファイルを読み込める\n",
        "  - JSON Lines形式の場合は引数を`lines=True`とする必要がある\n",
        "    - JSON LinesとよばれるJSONが改行で区切られたフォーマット(拡張子が`.jsonl`の時もある)\n",
        "  - `compression='infer'`と引数を設定すると**対応する圧縮方式が自動で選ばれて圧縮ファイルを読み込める**\n",
        "    - 対応する圧縮方式は{`.gz`,`.bz2`,`zip`,`xz`}など。\n",
        "\n",
        "> PandasってJSONファイルも読めんの！！\n",
        "<br>えっしかもgzファイルのまま読めんの！？！？？！？すご！？？！？！\n",
        "\n",
        "- pandas.dataframeはbool値をもつリストでカバーをすることが出来る\n",
        "  - `df = [A,B,C]`とあったときに、`X = [False,True,False]`のListXを使って\n",
        "    `df[X]`とすると、`df[X] -> B`となる\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXLpbW81FuFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzAmFuTOFuFo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "\n",
        "print(df[\"title\"]==\"イギリス\")\n",
        "print(df[df[\"title\"]==\"イギリス\"])\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "#print(uk_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(uk_text)"
      ],
      "metadata": {
        "id": "Aqn47zdm4rCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. カテゴリ名を含む行を抽出\n",
        "記事中でカテゴリ名を宣言している行を抽出せよ．\n",
        "\n",
        "textをみるとカテゴリ宣言は`Category:イギリス`のようにされているのでこの部分を含む行を抽出できるようにすれば良い\n",
        "\n",
        "#### Pythonを使用した文字列の検索(grep的処理)\n",
        "[https://note.nkmk.me/python-grep-like/]\n",
        "\n",
        "\n",
        "#### Python内で正規表現を扱う`re`モジュール\n",
        "[https://note.nkmk.me/python-re-match-search-findall-etc/]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yQsM0QzFuFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forによる表記\n",
        "\"\"\"\n",
        "for line in uk_text.split(\"\\n\"):\n",
        "  if \"[Category:\" in line:\n",
        "    print(line)\n",
        "\"\"\"\n",
        "# リスト内包表記 + if条件式\n",
        "Category_lines = [line for line in uk_text.split(\"\\n\") if \"[Category\" in line]\n",
        "print(Category_lines)"
      ],
      "metadata": {
        "id": "m0al_VEzFuFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. カテゴリ名の抽出\n",
        "記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．\n",
        "\n",
        "#### reモジュールで正規表現\n",
        "- `match()` : マッチオブジェクト\n",
        "  - 位置を取得 : 先頭`start()`, 末尾`end()`, 範囲`span()`\n",
        "  - 文字列を取得 : `group()`\n",
        "  - 各グループの文字列を取得 : `groups()`\n",
        "- `search()` : 文字列全てから検索し、マッチした場合にマッチオブジェクトを返す。\n",
        "  - ただし、マッチする部分が複数ある場合は最初のマッチのみが返されるため、1つずつ処理したい時以外は他のを使うことになる\n",
        "- `findall()` : マッチするすべての部分文字列をリストにして返す。\n",
        "- `finditer()` : マッチするすべての部分文字列のマッチオブジェクトをイテレータで返す。\n",
        "  > 多分この2種を使う。\n",
        "  - 正規表現の一部に()を使用すると、マッチングした際に()内の部分のみ[Group]を返すようになる。\n",
        "  - 括弧で囲む際に先頭に`?P<hoge>`と記述することで該当グループに任意の名前\"hoge\"をつけることが出来る。\n",
        "    - このグループ名はマッチングオブジェクトに対してgroup()などで名前を指定して取得できる様になる。\n",
        "  - \n",
        "\n",
        "\n",
        "\n",
        "#### エスケープシーケンスの無効化 (raw文字列)\n",
        "[https://note.nkmk.me/python-raw-string-escape/]\n",
        "正規表現の際は`\\(=バックスラッシュ)`を多用することが多い。これがエスケープシーケンスとして認識されるのを防ぐため、**raw文字列**として扱う。\n",
        "- 文字列リテラル`\"hoge\"`などを表記する際、その直前に`r`または`R`をつけるとそのままの値が文字列となる\n",
        "  - `\"a\\tb\\nA\\tB\"` → \"a[tab]b[改行]A[tab]B\" // 普通に入れるとエスケープシーケンスが展開される\n",
        "  - `a\\\\tb\\\\nA\\\\tB` → \"a\\tb\\nA\\tB\" // バックスラッシュをエスケープシーケンスで表記した場合\n",
        "  - `r\"a\\tb\\nA\\tB\"` → \"a\\tb\\nA\\tB\" // エスケープシーケンスを無効化したraw文字列\n",
        "\n",
        "#### マッチングの除外\n",
        "今回は「イギリス｜＊」のようにマッチングしているが拾わないで欲しい文字列も存在している。\n",
        "こういうときに`(?:hoge)`みたいな表記したら良い\n",
        "> らしいけどちょっとココ以外のソースが見つからんかったので詳しくはよくわかりません…\n",
        "  [https://qiita.com/yamaru/items/255d0c5dcb2d1d4ccc14#22-%E3%82%AB%E3%83%86%E3%82%B4%E3%83%AA%E5%90%8D%E3%81%AE%E6%8A%BD%E5%87%BA]\n",
        "\n",
        "- Pythonの公式Docに書いてあった！[https://docs.python.org/ja/3/library/re.html]\n",
        "\n",
        "> `(?:...)`\n",
        "普通の丸括弧の、キャプチャしない版です。丸括弧で囲まれた正規表現にマッチしますが、このグループがマッチした部分文字列は、マッチを実行したあとで回収することも、そのパターン中で以降参照することもできません 。"
      ],
      "metadata": {
        "id": "27f_mJLkFuFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# forによる表記\n",
        "Category_Matches = re.findall(r\"\\[Category:(.+?)(?:\\|.*)?\\]\",uk_text)\n",
        "\n",
        "for iter in Category_Matches:\n",
        "  print(iter)\n",
        "\n"
      ],
      "metadata": {
        "id": "rP6hJJB2FuFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. セクション構造\n",
        "記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ．\n",
        "\n",
        "> textを読む感じ、多分`==hoge==`だと1で`===hoge===`だと2みたいな感じなんだと思う。わからんけど。\n",
        "\n",
        "> 正規表現とにらめっこしつつ試行錯誤してたらうまくハマってキレイに取り出せたのでそのままlen()でレベル判定して終わり。"
      ],
      "metadata": {
        "id": "ByFpF3jyFuFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# forによる表記\n",
        "Section_Matches = re.findall(r\"(=+=)(?P<SectionName>[^=]+)(=+=)\",uk_text)\n",
        "\n",
        "for iter in Section_Matches:\n",
        "  #print(iter)\n",
        "  print(f\"SectionLevel : {iter[1]} = {len(iter[0])-1}\")"
      ],
      "metadata": {
        "id": "yAzfIE3NFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. ファイル参照の抽出\n",
        "記事から参照されているメディアファイルをすべて抜き出せ．\n",
        "\n",
        "> メディアファイルを探してみると\n",
        "  `[ファイル:Royal Coat of Arms of the United Kingdom.svg|85px|イギリスの国章]`みたいな感じなので\n",
        "  `\"[ファイル:(.*)|*|*]\"`みたいなかんじで取り出せそう\n",
        "\n",
        "- `|`は複数条件(OR)として機能するのでエスケープする必要がありました\n",
        "- 大カッコ [ ] で囲まれているものの、カッコ内の説明は割と不統一な感じなのでファイル名だけ取得できれば充分みたい"
      ],
      "metadata": {
        "id": "50TgE0duFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#Media_Matches = re.findall(r\"\\[(ファイル:)(.+)\\|(.+)\\|(.+?)\\]\",uk_text)\n",
        "Media_Matches = re.findall(r\"\\[(ファイル:)(.+?)\\|\",uk_text)\n",
        "\n",
        "for iter in Media_Matches:\n",
        "  #print(iter)\n",
        "  print(f\"Media File : {iter[1]}\")"
      ],
      "metadata": {
        "id": "DOD2R7vZFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. テンプレートの抽出\n",
        "記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．\n",
        "\n",
        "> 「基礎情報」を探してみると\n",
        "  `{{基礎情報 |hoge = fuga ... }}`となっていたので、\n",
        "  一旦基礎情報だけぶっこ抜いてきてその中でリスト作るようにしたほうが良いかも\n",
        "\n",
        "#### re.compile()\n",
        "今まで普通に`re.findall()`でやってきたけど、正規表現を記述してコンパイルした正規表現パターンのオブジェクトを作って、そのメソッドとして実行するのが割と一般的な使い方っぽい\n",
        "- `pattern = re.compile(<使いたい正規表現>)` : 正規表現パターンオブジェクトの作成\n",
        "  - `pattern.match(<String>)`\n",
        "  - `re.<任意のメソッド>(pattern, text)`\n",
        "  \n",
        "  のような使い方が可能\n",
        "\n",
        "\n",
        "\n",
        "#### 改行を含むマッチング\n",
        "- `re.S` = `re.DOTALL` : 使うとワイルドカード文字`.`が改行を含むあらゆる文字にマッチする\n",
        "  - 通常のドットは改行文字は含まれない\n",
        "- `re.M` = `re.MULTILINE` : 使うと複数行に対して検索を行う"
      ],
      "metadata": {
        "id": "24m8DBMKFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "#print(Basic_Info_Text) ## 基礎情報の部分のみ一旦テキスト抽出\n",
        "\n",
        "# p_Basic_Info = re.compile(r\"\\|(.+)[\\s*]=[\\s*](.+)\")\n",
        "# >> これだと公式国名みたいに途中で改行が入ってるやつが拾えない…\n",
        "p_Basic_Info = re.compile(r\"(?:\\|)(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)|(?=\\n))\",re.M+re.S)\n",
        "# > 2時間くらい格闘したけどわからなかったのでちょっとパスします…\n",
        "# >> 諦めてたら次の問題でコレの回答を使うみたいだったので悩みながら他の人の答え(たかぴーさんのブログ)をみたら(?=\\n)の前にスペースを入れるだけで解決した。なぜ？？？\n",
        "# >>> [|標語 : ...]となって標語だけうまく取り出せてない\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "print(Basic_Info_List)\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "## test ##\n",
        "Basic_Info_Dict[\"公式国名\"]\n",
        "for k,v in Basic_Info_Dict.items():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "1XqoqDsHFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 26. 強調マークアップの除去\n",
        "25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ\n",
        "- (参考: マークアップ早見表[ http://ja.wikipedia.org/wiki/Help:%E6%97%A9%E8%A6%8B%E8%A1%A8 ])\n",
        "\n",
        "> リンク先によると\n",
        "- `''他との区別''`\t他との区別\n",
        "- `'''強調'''`\t強調\n",
        "- `'''''斜体と強調'''''`\t斜体と強調\n",
        "\n",
        "> らしいので、'の2個以上のマッチングを調べて取り除けば良さそう\n",
        "\n",
        "\n",
        "> 色々元テキストから消そうと努力してたけど先に作ってる辞書で表示するときだけ`'`を弾けば良いのか……賢いな………"
      ],
      "metadata": {
        "id": "b_TWlK7XFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"\\|(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)| (?=\\n))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "# 強調を消す\n",
        "# Basic_Info_NoEmphasis = {i[0]:re.sub(r\"\\'{2,}\", \"\", i[1]) for i in Basic_Info_List}\n",
        "\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "for k,v in Basic_Info_Dict.items():\n",
        "  v = re.sub(r\"\\'{2,}\", \"\", v)\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "JhzVyAnbFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 27. 内部リンクの除去\n",
        "26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: マークアップ早見表）\n",
        "\n",
        "> マッチングした文字列の一部を置換結果として利用したい……。\n",
        "\n",
        "#### `re.sub()`を使用した置換にマッチング文字列の一部を使用する\n",
        "`re.sub()`では正規表現の中でキャプチャグループが含まれている場合、置換する文字列の中で `\\1, \\2, ...` を使用することでキャプチャグループでキャプチャされた文字列を参照することができます。\n",
        "- 参考リンク [ https://www.javadrive.jp/python/regex/index10.html ]\n",
        "\n",
        "> なぜかこれは **0ではなくて1始まり** なので注意\n",
        "\n",
        "色々頑張ったけど、なぜか\n",
        "  - 国章リンク =（[[イギリスの国章|国章]]）<br>\n",
        "\n",
        "だけが消えなかった。\n",
        "正規表現チェッカー[ https://weblabo.oscasierra.net/tools/regex/ ]で確認してもちゃんとヒットしてるのになぜ？？？"
      ],
      "metadata": {
        "id": "Zkk3HaqCFuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"\\|(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)| (?=\\n))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "# 強調を消す\n",
        "p_remove_Emphasis = re.compile(r\"\\'{2,}\",re.S)\n",
        "Basic_Info_NoEmphasis = {i[0]:p_remove_Emphasis.sub(\"\", i[1]) for i in Basic_Info_List}\n",
        "\n",
        "# 内部リンクを消す\n",
        "p_remove_Link = re.compile(r\"\\[\\[(?:[^|]*\\|)*?([^|]*?)\\]\\]\",re.S+re.M)\n",
        "# [[hoge]] -> hoge , [[hoge(#foo)|fuga]] -> fuga にしたい\n",
        "## [[(.+)]] , [[.+|(.+)]]\n",
        "Basic_Info_NoLink = {i[0]:p_remove_Link.sub(r\"\\1\", i[1]) for i in Basic_Info_NoEmphasis.items()}\n",
        "\n",
        "ans = dict(Basic_Info_NoLink)\n",
        "\n",
        "for k,v in ans.items():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "QJUucQvMFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 28. MediaWikiマークアップの除去\n",
        "27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．\n",
        "\n",
        "> 普通にマークアップを何処まで消して良いのかわからんからわからん……\n",
        "  とりあえず見てて気になるところだけやった"
      ],
      "metadata": {
        "id": "QTYd8NJ9FuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.*?$(.*?)^\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "def remove_markup(text):\n",
        "\n",
        "  # 強調を消す\n",
        "  p_remove_Emphasis = re.compile(r\"\\'{2,}\",re.S)\n",
        "  text = p_remove_Emphasis.sub(\"\", text)\n",
        "\n",
        "  # 内部リンクを消す\n",
        "  p_remove_Link = re.compile(r\"\\[\\[(?:[^|]*\\|)*?([^|]*?)\\]\\]\",re.S+re.M)\n",
        "  text = p_remove_Link.sub(r\"\\1\", text)\n",
        "\n",
        "  # 箇条書き消す\n",
        "  p_remove_kajou = re.compile(r\"^\\*\",re.S+re.M)\n",
        "  text = p_remove_kajou.sub(\"\", text)\n",
        "\n",
        "  # 外部リンク消す\n",
        "  p_remove_otherLink = re.compile(r\"[\\{\\[].*http.+[\\}\\]]\",re.S+re.M)\n",
        "  text = p_remove_otherLink.sub(\"\", text)\n",
        "\n",
        "  # HTMLタグ消す\n",
        "  p_remove_HTMLtag = re.compile(r\"<.+?>\",re.S+re.M)\n",
        "  text = p_remove_HTMLtag.sub(\"\", text)\n",
        "  \n",
        "  # {{langとか仮リンクとか|なんちゃら|かんちゃら}}←みたいなやつの最後だけ取り出す(よくわからん)\n",
        "  p_remove_Nazo = re.compile(r\"\\{\\{(?:.*\\|)*(.*?)\\}\\}\",re.S+re.M)\n",
        "  text = p_remove_Nazo.sub(r\"\\1\", text)\n",
        "\n",
        "  # 改行消す\n",
        "  p_remove_kaigyou = re.compile(r\"$|\\n\",re.S+re.M)\n",
        "  text = p_remove_kaigyou.sub(\"\", text)\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  \n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "ans = dict(Basic_Info_List)\n",
        "\n",
        "for k,v in ans.items():\n",
        "  print(f\"{k} : {remove_markup(v)}\")"
      ],
      "metadata": {
        "id": "sPM9IzeTFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 29. 国旗画像のURLを取得する\n",
        "テンプレートの内容を利用し，国旗画像のURLを取得せよ．\n",
        "（ヒント: MediaWiki APIのimageinfoを呼び出して，ファイル参照をURLに変換すればよい）\n",
        "\n",
        "> 急にAPIの話出てきた～～～何もわかんね～～～！\n",
        "\n"
      ],
      "metadata": {
        "id": "tg1CPd0BFuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.*?$(.*?)^\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "uk_flag_URL = Basic_Info_Dict[\"国旗画像\"]\n",
        "\n",
        "# ヒントのページ見たけどな～～～んもわからんかったので流石にココだけ他の方の答えのソースを無心でコピペしました…\n",
        "import requests\n",
        "uk_flag_URL = uk_flag_URL.replace(\" \",\"_\")\n",
        "url = 'https://commons.wikimedia.org/w/api.php?action=query&titles=File:' + uk_flag_URL + '&prop=imageinfo&iiprop=url&format=json'\n",
        "data = requests.get(url)\n",
        "\n",
        "print(re.search(r'\"url\":\"(.+?)\"',data.text).group(1))\n",
        "# 正直webAPI周りの箇所がマジで何やってるか何もわからん……恥ずかし……"
      ],
      "metadata": {
        "id": "PNyrDcUNFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第4章: 形態素解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "なお，問題37, 38, 39はmatplotlibもしくはGnuplotを用いるとよい．\n",
        "\n",
        "> らしいので例によってデータをローカルに(ry\n",
        "\n",
        "- `!wget https://nlp100.github.io/data/neko.txt`\n",
        "\n",
        "> 調べてようやく知ったけど`wget`コマンドで`-O`を指定すると常にファイル名を指定して保存できるので、名前が被ったときに「`hoge.tmp`と`hoge1.tmp`ができちゃった～～」みたいな事にならないですむらしい。"
      ],
      "metadata": {
        "id": "8t09a_dWGBrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/neko.txt -O neko.txt\n",
        "!ls"
      ],
      "metadata": {
        "id": "aiHHBe0FgYs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> いきなり当然の権利のようにmecabを使ってみろと言ってくるので…\n",
        "\n",
        "#### MeCabのインストール\n",
        "MeCabは普通に外部ライブラリなのでインストールする必要がある。\n",
        "\n",
        "- 一応念の為 `!pip show mecab`してみたけど出ませんでした\n",
        "  > `WARNING: Package(s) not found: mecab`\n",
        "  \n",
        "  - 後で気付いたけどパッケージ名は`mecab-python3`らしいです…\n",
        "\n",
        "\n",
        "- ColaboratoryでMeCabを使えようにする。 - Qiita\n",
        "  - [ https://qiita.com/pytry3g/items/897ae738b8fbd3ae7893 ]\n",
        "\n",
        "コチラの記事を参考にpipを使ってMeCabをインストールする。\n",
        "\n",
        "\n",
        "あ、でも今回の問題はMeCab使って形態素解析の結果を出す作業が要るので一旦コマンドラインでMeCabを叩いてみることにします。\n",
        "\n",
        "> この手のやつ、Google Colaboratoryだと落ちるたびに上から順にやっていかないといけないからめんどくさいなぁ。\n",
        "  などと…\n",
        "  "
      ],
      "metadata": {
        "id": "TKE39_IJMWvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loeNRp3eGBrk"
      },
      "outputs": [],
      "source": [
        "!apt install aptitude\n",
        "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.7\n",
        "\n",
        "# グラフ表示用\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "!pip show mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mecab neko.txt -o neko.txt.mecab\n",
        "!head neko.txt.mecab -n 20"
      ],
      "metadata": {
        "id": "TtpdVo1BnioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 30. 形態素解析結果の読み込み\n",
        "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
        "\n",
        "\n",
        "#### 形態素解析について\n",
        "さらっと「各形態素は～として表現せよ」って言われたけど何もわからんね…\n",
        "\n",
        "> MeCabが形態素解析結果として返す情報は次のような内容である：\n",
        "\n",
        "    表層形  品詞  品詞細分類1  品詞細分類2  品詞細分類3  活用形  活用型  原形  読み  発音\n",
        "\n",
        "> 「一文を形態素のリストとして表現せよ」とあるので\n",
        "  \n",
        "    [<surface(表層形)>, <base(基本形)>, <pos(品詞)>, <pos1(品詞細分類1)>]\n",
        "    をもとに、\n",
        "    list(dict[\"<形態素名(品詞とか)>\"])\n",
        "という感じで取り出せるようにすればいいっぽい\n",
        "\n",
        "……\n",
        "\n",
        "やれば良いことはわかったけど、ファイルの読み込み方とか全くわからん…。\n",
        "最初の問題なのでおとなしく写経しようかしら…\n",
        "\n",
        "> ちまちまやってたら出来た。\n",
        "  作成方針は以下の流れ\n",
        "\n",
        "1. (形態素解析の結果の)ファイルを行ごとに読み込む\n",
        "2. 形態素解析の結果を元に辞書を作る(とりあえず全部)\n",
        "  - このとき `\"\\n\"` や `\"\"(空白文字)` を省く\n",
        "3. 上記の処理を節毎(`\"EOS\"`で区切る)にする。\n",
        "4. 節毎に辞書をリストにして追加するようにする\n"
      ],
      "metadata": {
        "id": "WdfbPA5qGBrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[0:30]:\n",
        "    if line != \"EOS\\n\":\n",
        "      # line = 初めて\t副詞,一般,*,*,*,*,初めて,ハジメテ,ハジメテ ←こんなかんじ\n",
        "      # [表層形  品詞  品詞細分類1  品詞細分類2  品詞細分類3  活用形  活用型  原形  読み  発音]\n",
        "      ## ここから [表層形(0番目), 基本形(?????多分原型？だとしたら7番目), 品詞(1番目), 品詞細分類1(2番目)] を取り出す\n",
        "      word = line.split(\"\\t\")\n",
        "      # word = ['吾輩', '名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\\n'] ←こんなかんじ\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        # 改行文字、空白文字を省く\n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      # 節末で溜めた形態素解析の結果を「辞書のリスト」として追加\n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "IPuDD6i_PLyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 31. 動詞\n",
        "動詞の表層形をすべて抽出せよ．\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ\n"
      ],
      "metadata": {
        "id": "whtu8E8XGBrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if (m[\"pos\"]==\"動詞\") : print(m[\"surface\"]) "
      ],
      "metadata": {
        "id": "IIAtIF5mGBrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32. 動詞の基本形\n",
        "動詞の基本形をすべて抽出せよ．\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ2\n",
        "  31番の `\"surface\"` を `\"base\"` にするだけなのでマジで5秒とかで解ける\n"
      ],
      "metadata": {
        "id": "1yHL2j5KGBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if (m[\"pos\"]==\"動詞\") : print(m[\"base\"]) "
      ],
      "metadata": {
        "id": "q6ub1Uf3GBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33. 「AのB」\n",
        "2つの名詞が「の」で連結されている名詞句を抽出せよ．\n",
        "\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ3\n",
        "\n",
        "> …かと思ったけどそうでもなかった\n",
        "  とはいっても「の」の前後を取り出して名詞かどうか判定するだけ"
      ],
      "metadata": {
        "id": "F-dC7WJ7GBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def is_MeishiNoMeishi(sentence):\n",
        "  ans = []\n",
        "  for s in sentence:\n",
        "    for i in range(len(s)-1):\n",
        "      if s[i][\"surface\"]==\"の\":\n",
        "        if (s[i-1][\"pos\"]==\"名詞\") & (s[i+1][\"pos\"]==\"名詞\"):\n",
        "          ans.append(s[i-1][\"surface\"]+s[i][\"surface\"]+s[i+1][\"surface\"])\n",
        "  return ans\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = is_MeishiNoMeishi(sentence)\n",
        "print(len(ans))\n",
        "print(ans)\n"
      ],
      "metadata": {
        "id": "8OxWIj3GGBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 34. 名詞の連接\n",
        "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．\n",
        "\n",
        "> 最長一致…貪欲だな！(思考完)\n",
        "\n",
        "> …というわけにもいかんかった\n",
        "  連接なので1(その他→名詞→その他)のときは拾ってはいけないので面倒\n",
        "  でも普通に長さを保持する変数入れるだけで解決した…\n",
        "  あんまり汎用性の高い書き方じゃないから通用はしなさそうだけど…"
      ],
      "metadata": {
        "id": "gsh5EawtGBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def Meishi_renzoku(sentence):\n",
        "  ans = []\n",
        "  for s in sentence:\n",
        "    tmp = \"\"\n",
        "    length = 0\n",
        "    for m in s:\n",
        "      if(m[\"pos\"]==\"名詞\"):\n",
        "        tmp += m[\"surface\"]\n",
        "        length += 1\n",
        "      else:\n",
        "        if(length>=2):\n",
        "          ans.append(tmp)\n",
        "          print(tmp)\n",
        "        tmp = \"\"\n",
        "        length = 0\n",
        "  return ans\n",
        "\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = Meishi_renzoku(sentence)\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "Dv2BboHrGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 35. 単語の出現頻度\n",
        "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．\n",
        "\n",
        "#### リストからDataFrameの作成\n",
        "参考 [ https://www.delftstack.com/ja/howto/python-pandas/pandas-create-dataframe-from-list/ ]\n",
        "\n",
        "リストからDataFrameの作成ってできるんだ…\n",
        "いやまあそりゃできるか…ファイルの読み込みで使う機会が多いというだけで…\n",
        "\n",
        "> 出現頻度の計算は以前のノック(19番)でやっているのでそれを活用\n",
        "  普通に単語でDict作ってカウントしてもいいけどなんとなくコッチのほうが良いかなって…\n",
        "  表層形と基本形どっちでカウントするのが良いのかわからなかったけど、解答例見る感じ基本形(Base)でやっていたのでそっちを採用。\n",
        "  >> 言語処理だとコッチが一般的？\n",
        "\n",
        "> 句点と読点とか、カギカッコは除外しても良かったかもしれんわね…"
      ],
      "metadata": {
        "id": "pdyOrgv8GBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    #print(m.values())\n",
        "    ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "print(df.value_counts(\"base\")[:20])"
      ],
      "metadata": {
        "id": "N-2DUVBTGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 36. 頻度上位10語\n",
        "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．\n",
        "\n",
        "> なんかグラフのときはmatplotlibかGNUplotを使ったほうが良いみたいなの言ってたな\n",
        "\n",
        "軽く調べたらGnuplotは元々外部のフリーソフトらしいので、今回はPythonで汎用的に使えそうなmatplotlibの方を採用。\n",
        "\n",
        "> …と思ったが、調べていたらpandasのDataFrameには\n",
        "  `DataFrame.plot`というド直球なメソッドが存在しているらしくてすごいなあと思いました(小並感)\n",
        "\n"
      ],
      "metadata": {
        "id": "K2Z0Gp0RGBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    #print(m.values())\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = (df.value_counts(\"base\"))[0:10]\n",
        "print(top_words)\n",
        "\n",
        "# グラフ化\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "top_words.plot.bar()\n",
        "\n",
        "\"\"\"\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(top_words[:10])\n",
        "pyplot.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nBZLXxluGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 37. 「猫」と共起頻度の高い上位10語\n",
        "「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．\n",
        "\n",
        "> 「共起頻度」is なに？？？？\n",
        "\n",
        "#### 共起頻度\n",
        "あんまり詳しい解説はなかったんだけど…\n",
        "\n",
        "    共起 : コロケーション (Co-location)\n",
        "    ある単語同士が同一文中に出現する組み合わせ\n",
        "\n",
        "つまり、「共起頻度はある単語の組み合わせで同一文中に登場する頻度の高い組み合わせ」\n",
        "というのがざっとした理解。\n",
        "\n",
        "> なので今回は文章中に「猫」という単語を含んでいるものを取り出し、\n",
        "  そこから単語1つ1つをカウントするだけで良さそう。\n",
        "  \n",
        ">> 本来は「組み合わせ」なので[A-B]という組み合わせで取り出すべきだが、\n",
        "  今回は「猫」という単語を指定されているので1つずつ見てカウントしていくだけで良い"
      ],
      "metadata": {
        "id": "TIBgB5lkGBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  #print([word[\"base\"] for word in s])\n",
        "  if not \"猫\" in [word[\"base\"] for word in s]: continue\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "df_CatDrop = df[df[\"base\"]!=\"猫\"]\n",
        "\n",
        "top_words = (df_CatDrop.value_counts(\"base\"))\n",
        "print(top_words[0:10])\n",
        "\n",
        "# グラフ化\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "top_words[0:10].plot.bar()\n",
        "\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HslXE1SRGBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 38. ヒストグラム\n",
        "単語の出現頻度のヒストグラムを描け．ただし，横軸は出現頻度を表し，1から単語の出現頻度の最大値までの線形目盛とする．縦軸はx軸で示される出現頻度となった単語の異なり数（種類数）である．\n",
        "\n",
        "\n",
        "> Pandasからヒストグラムの書き方わからなかったからForで全探索しようかと思ったけど調べたら普通に関数一発で描けるらしいです\n",
        "> - 【histogram入門】pandasとmatplotlibでヒストグラムを描いてみた [ https://qiita.com/MuAuan/items/849df1fffb0727f0dd09 ]\n",
        "\n",
        "を参考に `df.hist(bins=100<区間の個数>)`してplotするだけでした…"
      ],
      "metadata": {
        "id": "NvdNnDEsGBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = pd.DataFrame(df.value_counts(\"base\"))\n",
        "print(type(top_words))\n",
        "\n",
        "\"\"\"\n",
        "for w in top_words.index:\n",
        "  print(w)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QztgN5yzGBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words.hist(bins=100)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "kaCHpx6rYd-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 39. Zipfの法則\n",
        "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．\n",
        "\n",
        "- ジップの法則(Wikipedia)\n",
        "  - https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%83%E3%83%97%E3%81%AE%E6%B3%95%E5%89%87\n",
        "- Pandasのplotの全引数を解説 - Rosyuku\n",
        "  - https://own-search-and-study.xyz/2016/08/03/pandas%E3%81%AEplot%E3%81%AE%E5%85%A8%E5%BC%95%E6%95%B0%E3%82%92%E4%BD%BF%E3%81%84%E3%81%93%E3%81%AA%E3%81%99/\n",
        "\n",
        "\n",
        ">「出現頻度順位」と「出現頻度」の必要な値は揃っているのであとは対数グラフにプロットするだけ。\n",
        "  調べたら`df.plot()`の引数で`(loglog=True)`とすると両対数グラフになるらしい。\n",
        "  そして`df.plot?`でヘルプが表示される。便利だねぇ"
      ],
      "metadata": {
        "id": "3uTbIWTWGBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = pd.DataFrame(df.value_counts(\"base\"))\n",
        "#print(top_words)\n",
        "\n",
        "\n",
        "top_words.plot(loglog=True)"
      ],
      "metadata": {
        "id": "3k762OO3GBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第5章: 係り受け解析\n",
        "\n",
        "日本語Wikipediaの「人工知能」に関する記事からテキスト部分を抜き出したファイルがai.ja.zipに収録されている． この文章をCaboChaやKNP等のツールを利用して係り受け解析を行い，その結果をai.ja.txt.parsedというファイルに保存せよ．このファイルを読み込み，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "- CaboCha/南瓜: Yet Another Japanese Dependency Structure Analyzer\n",
        "  - 公式ページ https://taku910.github.io/cabocha/\n",
        "  - 奈良先端技術大学院の工藤拓氏による日本語係り受け解析器"
      ],
      "metadata": {
        "id": "triCPO2JGCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 事前準備\n",
        "\n",
        "例によってまたデータがいるので拾って読み込んでくるまでをやる。\n",
        "必要に応じて適宜ライブラリの追加などもココでやる。\n",
        "\n",
        "↓の実行ボタンを押したら事前準備が完了するようになってる。はず。\n"
      ],
      "metadata": {
        "id": "7mrrvSw6Sjku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### やってること\n",
        "- `ai.ja.zip`の保存と解凍(データの準備)\n",
        "- ライブラリの導入\n",
        "  - MeCab (+MeCab-Python3)\n",
        "  - CaboCha \n",
        "    - 配布はGoogle Driveから [ https://drive.google.com/drive/folders/0B4y35FiV1wh7cGRCUUJHVTNJRnM ]\n",
        "    - 2022-03-23時点で最新はcabocha-0.69なのでそれを使用します\n",
        "  - CRF++ (CaboChaの依存パッケージとして必要)\n",
        "    - 配布はGoogle Driveから [ https://drive.google.com/drive/folders/0B4y35FiV1wh7fngteFhHQUN2Y1B5eUJBNHZUemJYQV9VWlBUb3JlX0xBdWVZTWtSbVBneU0 ]\n",
        "    - 2022-03-23時点で最新はCRF++-0.58なのでそれを使用します。(最終更新が2015年なので多分ずっとコレだと思うけど…)\n",
        "- グラフ描画用\n",
        "  - matplotlib の読み込み\n",
        "  - japanize_matplotlib の読み込み (日本語でグラフ作成)\n",
        "  - `%matplotlib inline` の実行(グラフをインラインで描画するように)\n",
        "- 係り受け解析\n",
        "  - 係り受け解析結果を`ai.ja.txt.parsed`として出力\n",
        "\n",
        "\n",
        "#### [参考]\n",
        "- Google Colab で MeCab と CaboCha を使う最強の方法 by @tomowarkar\n",
        "  - Qiita https://qiita.com/tomowarkar/items/b6a89145c06956618542\n",
        "\n",
        "- Google Colab で MeCab と CaboCha を使う。 by tomowarkar\n",
        "  - github Blog https://tomowarkar.github.io/blog/posts/colab_mecab/\n",
        "\n",
        "> ……で頑張ってたけどbashだとどうしてもエラーが出てうまくいかない！！\n",
        "\n",
        "- Colabratory に Cabocha をインストールする by @iimuz\n",
        "  - Qiita https://qiita.com/iimuz/items/30a7e02772ffd3445f3b \n",
        "- ▲心くじけず言語処理100本ノック＝＝5章下準備＝＝ by tbtech\n",
        "  - はてなブログ https://ds-blog.tbtech.co.jp/entry/2020/06/08/%E2%96%B2%E5%BF%83%E3%81%8F%E3%81%98%E3%81%91%E3%81%9A%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%EF%BC%9D%EF%BC%9D5%E7%AB%A0%E4%B8%8B%E6%BA%96%E5%82%99%EF%BC%9D%EF%BC%9D\n",
        "\n",
        "> MeCabは謎の記法してるけど、CRF++とCaboChaをPythonベースで導入してて此方の記事のおかげで無事に使えました…感謝🙏\n",
        "  ていうか環境構築周りだけはマジで言語処理ノック側でもサポートしてくれって思う、そんなとこで躓くのは本来の課題じゃないだろうに\n",
        "\n",
        "- curlやwgetで公開済みGoogle Driveデータをダウンロードする by @namakemono\n",
        "  - Qiita https://qiita.com/namakemono/items/c963e75e0af3f7eed732\n",
        "\n",
        "> Google Driveで大きいサイズのときに出るウイルスチェック出来ませんみたいな警告のせいであのクソ長いwget文になるみたい。\n",
        "  そのままDLすると `cabocha.tar.bz2 is not a bzip2 file` と表示されて!tarや!bzipで解凍できない。困る。\n",
        "  ていうかなんで他の人は普通にwgetで通るの？？\n",
        "\n",
        "結果として\n",
        "- `cabocha.tar.bz2`のDLまでは心くじけず～の部分、\n",
        "- tarの解凍～make～Pythonライブラリのインストールをtomowarkar氏のものでやったらうまくいきました。"
      ],
      "metadata": {
        "id": "tcDLyeh9XOu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なデータの準備\n",
        "!wget https://nlp100.github.io/data/ai.ja.zip -O ai.ja.zip\n",
        "!unzip -o ai.ja.zip\n",
        "!pwd\n",
        "!ls\n",
        "!head -10 ai.ja.txt"
      ],
      "metadata": {
        "id": "w7A8mPqDUfPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d360547-e8df-4e0d-8518-4d53c2c553da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-05 03:19:48--  https://nlp100.github.io/data/ai.ja.zip\n",
            "Resolving nlp100.github.io (nlp100.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to nlp100.github.io (nlp100.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17516 (17K) [application/zip]\n",
            "Saving to: ‘ai.ja.zip’\n",
            "\n",
            "\rai.ja.zip             0%[                    ]       0  --.-KB/s               \rai.ja.zip           100%[===================>]  17.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-05 03:19:48 (60.0 MB/s) - ‘ai.ja.zip’ saved [17516/17516]\n",
            "\n",
            "Archive:  ai.ja.zip\n",
            "  inflating: ai.ja.txt               \n",
            "  inflating: readme.ai.ja.md         \n",
            "/content\n",
            "ai.ja.txt  ai.ja.zip  readme.ai.ja.md  sample_data\n",
            "人工知能\n",
            "\n",
            "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\n",
            "\n",
            "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\n",
            "人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある。1956年にダートマス会議でジョン・マッカーシーにより命名された。現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。\n",
            "\n",
            "プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている。人工的な知能の実現へのアプローチとしては、「ファジィ理論」や「ニューラルネットワーク」などのようなアプローチも知られているが、従来の人工知能である (Good Old Fashioned AI) との差は記述の記号的明示性にある。その後「サポートベクターマシン」が注目を集めた。また、自らの経験を元に学習を行う強化学習という手法もある。「この宇宙において、知性とは最も強力な形質である（レイ・カーツワイル）」という言葉通り、知性を機械的に表現し実装するということは極めて重要な作業である。\n",
            "\n",
            "2006年のディープラーニング（深層学習）の登場と2010年代以降のビッグデータの登場により、一過性の流行を超えて社会に浸透して行った。2016年から2017年にかけて、ディープラーニングを導入したAIが完全情報ゲームである囲碁などのトップ棋士、さらに不完全情報ゲームであるポーカーの世界トップクラスのプレイヤーも破り、麻雀では「Microsoft Suphx (Super Phoenix)」がAIとして初めて十段に到達するなど、時代の最先端技術となった。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリの導入\n",
        "!apt install aptitude\n",
        "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.7\n"
      ],
      "metadata": {
        "id": "kYJESPXxYs1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81ead1f-4807-4661-8f88-796e36200be3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 0s (20.0 MB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-jumandic\n",
            "  mecab-jumandic-utf8 mecab-utils\n",
            "0 upgraded, 8 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 29.0 MB of archives.\n",
            "After this operation, 277 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.0 MB in 2s (18.8 MB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 156669 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../3-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../4-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../5-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../6-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../7-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3==0.7\n",
            "  Downloading mecab-python3-0.7.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 378 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp37-cp37m-linux_x86_64.whl size=156610 sha256=af9331db42cb54a61302695ce095a2c93216bfb9c6fd68f631e63bf271f1db78\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/46/95/3748ec2c4936cb69ee4d248a85e862064ea1e84819344c5292\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CRF++のインストール\n",
        "import os\n",
        "filename_crfpp = 'crfpp.tar.gz'\n",
        "!wget \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "%cd CRF++-0.58\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "%cd ../\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib' "
      ],
      "metadata": {
        "id": "M9meLf7q1__t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2570a326-18ac-44bd-c5f3-d685de0e6127"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-05 03:20:50--  https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.73.206, 2607:f8b0:4004:829::200e\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.73.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/455eemnsu7rbsho01ddg6anggsv8rolp/1649128800000/13553212398903315502/*/0B4y35FiV1wh7QVR6VXJ5dWExSTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-05 03:20:51--  https://doc-08-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/455eemnsu7rbsho01ddg6anggsv8rolp/1649128800000/13553212398903315502/*/0B4y35FiV1wh7QVR6VXJ5dWExSTQ?e=download\n",
            "Resolving doc-08-74-docs.googleusercontent.com (doc-08-74-docs.googleusercontent.com)... 172.253.122.132, 2607:f8b0:4004:c09::84\n",
            "Connecting to doc-08-74-docs.googleusercontent.com (doc-08-74-docs.googleusercontent.com)|172.253.122.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 790570 (772K) [application/x-gzip]\n",
            "Saving to: ‘crfpp.tar.gz’\n",
            "\n",
            "crfpp.tar.gz        100%[===================>] 772.04K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-05 03:20:51 (37.5 MB/s) - ‘crfpp.tar.gz’ saved [790570/790570]\n",
            "\n",
            "CRF++-0.58/\n",
            "CRF++-0.58/INSTALL\n",
            "CRF++-0.58/python/\n",
            "CRF++-0.58/python/test.py\n",
            "CRF++-0.58/python/README\n",
            "CRF++-0.58/python/CRFPP.py\n",
            "CRF++-0.58/python/setup.py\n",
            "CRF++-0.58/python/CRFPP_wrap.cxx\n",
            "CRF++-0.58/Makefile.in\n",
            "CRF++-0.58/encoder.cpp\n",
            "CRF++-0.58/node.h\n",
            "CRF++-0.58/param.cpp\n",
            "CRF++-0.58/swig/\n",
            "CRF++-0.58/swig/CRFPP_wrap.c\n",
            "CRF++-0.58/swig/version.h\n",
            "CRF++-0.58/swig/version.h.in\n",
            "CRF++-0.58/swig/Makefile\n",
            "CRF++-0.58/swig/CRFPP.i\n",
            "CRF++-0.58/config.h.in\n",
            "CRF++-0.58/feature_cache.cpp\n",
            "CRF++-0.58/config.guess\n",
            "CRF++-0.58/scoped_ptr.h\n",
            "CRF++-0.58/node.cpp\n",
            "CRF++-0.58/README\n",
            "CRF++-0.58/timer.h\n",
            "CRF++-0.58/feature_index.h\n",
            "CRF++-0.58/config.sub\n",
            "CRF++-0.58/ltmain.sh\n",
            "CRF++-0.58/common.h\n",
            "CRF++-0.58/configure\n",
            "CRF++-0.58/crf_learn.cpp\n",
            "CRF++-0.58/darts.h\n",
            "CRF++-0.58/winmain.h\n",
            "CRF++-0.58/doc/\n",
            "CRF++-0.58/doc/html/\n",
            "CRF++-0.58/doc/html/search/\n",
            "CRF++-0.58/doc/html/search/nomatches.html\n",
            "CRF++-0.58/doc/html/search/search_r.png\n",
            "CRF++-0.58/doc/html/search/search_l.png\n",
            "CRF++-0.58/doc/html/search/mag_sel.png\n",
            "CRF++-0.58/doc/html/search/search.css\n",
            "CRF++-0.58/doc/html/search/search_m.png\n",
            "CRF++-0.58/doc/html/search/close.png\n",
            "CRF++-0.58/doc/html/search/search.js\n",
            "CRF++-0.58/doc/html/nav_f.png\n",
            "CRF++-0.58/doc/html/crfpp_8h_source.html\n",
            "CRF++-0.58/doc/html/jquery.js\n",
            "CRF++-0.58/doc/html/nav_h.png\n",
            "CRF++-0.58/doc/html/bc_s.png\n",
            "CRF++-0.58/doc/html/index.html\n",
            "CRF++-0.58/doc/html/closed.png\n",
            "CRF++-0.58/doc/html/tab_h.png\n",
            "CRF++-0.58/doc/html/tab_a.png\n",
            "CRF++-0.58/doc/html/tab_b.png\n",
            "CRF++-0.58/doc/html/installdox\n",
            "CRF++-0.58/doc/html/doxygen.css\n",
            "CRF++-0.58/doc/html/open.png\n",
            "CRF++-0.58/doc/html/tab_s.png\n",
            "CRF++-0.58/doc/html/files.html\n",
            "CRF++-0.58/doc/html/doxygen.png\n",
            "CRF++-0.58/doc/html/tabs.css\n",
            "CRF++-0.58/doc/latex/\n",
            "CRF++-0.58/doc/latex/refman.tex\n",
            "CRF++-0.58/doc/latex/doxygen.sty\n",
            "CRF++-0.58/doc/latex/Makefile\n",
            "CRF++-0.58/doc/index.html\n",
            "CRF++-0.58/doc/default.css\n",
            "CRF++-0.58/doc/doxygen/\n",
            "CRF++-0.58/doc/doxygen/tab_l.gif\n",
            "CRF++-0.58/doc/doxygen/namespacemembers.html\n",
            "CRF++-0.58/doc/doxygen/nav_f.png\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h_source.html\n",
            "CRF++-0.58/doc/doxygen/namespaces.html\n",
            "CRF++-0.58/doc/doxygen/nav_h.png\n",
            "CRF++-0.58/doc/doxygen/namespaceCRFPP.html\n",
            "CRF++-0.58/doc/doxygen/globals.html\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h-source.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Tagger-members.html\n",
            "CRF++-0.58/doc/doxygen/tab_b.gif\n",
            "CRF++-0.58/doc/doxygen/functions.html\n",
            "CRF++-0.58/doc/doxygen/tab_r.gif\n",
            "CRF++-0.58/doc/doxygen/bc_s.png\n",
            "CRF++-0.58/doc/doxygen/namespacemembers_func.html\n",
            "CRF++-0.58/doc/doxygen/index.html\n",
            "CRF++-0.58/doc/doxygen/closed.png\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Model.html\n",
            "CRF++-0.58/doc/doxygen/tab_h.png\n",
            "CRF++-0.58/doc/doxygen/functions_func.html\n",
            "CRF++-0.58/doc/doxygen/tab_a.png\n",
            "CRF++-0.58/doc/doxygen/globals_defs.html\n",
            "CRF++-0.58/doc/doxygen/classes.html\n",
            "CRF++-0.58/doc/doxygen/tab_b.png\n",
            "CRF++-0.58/doc/doxygen/globals_type.html\n",
            "CRF++-0.58/doc/doxygen/doxygen.css\n",
            "CRF++-0.58/doc/doxygen/open.png\n",
            "CRF++-0.58/doc/doxygen/tab_s.png\n",
            "CRF++-0.58/doc/doxygen/globals_func.html\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Model-members.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Tagger.html\n",
            "CRF++-0.58/doc/doxygen/files.html\n",
            "CRF++-0.58/doc/doxygen/doxygen.png\n",
            "CRF++-0.58/doc/doxygen/tabs.css\n",
            "CRF++-0.58/doc/doxygen/annotated.html\n",
            "CRF++-0.58/doc/crfpp.cfg\n",
            "CRF++-0.58/crf_test.cpp\n",
            "CRF++-0.58/mmap.h\n",
            "CRF++-0.58/Makefile.msvc.in\n",
            "CRF++-0.58/ChangeLog\n",
            "CRF++-0.58/feature_index.cpp\n",
            "CRF++-0.58/COPYING\n",
            "CRF++-0.58/libcrfpp.cpp\n",
            "CRF++-0.58/NEWS\n",
            "CRF++-0.58/mkinstalldirs\n",
            "CRF++-0.58/freelist.h\n",
            "CRF++-0.58/AUTHORS\n",
            "CRF++-0.58/merge-models.pl\n",
            "CRF++-0.58/java/\n",
            "CRF++-0.58/java/test.java\n",
            "CRF++-0.58/java/README\n",
            "CRF++-0.58/java/.am\n",
            "CRF++-0.58/java/Makefile\n",
            "CRF++-0.58/java/org/\n",
            "CRF++-0.58/java/org/chasen/\n",
            "CRF++-0.58/java/org/chasen/crfpp/\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_float.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPPJNI.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_p_char.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPP.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_int.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPPConstants.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/Model.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/Tagger.java\n",
            "CRF++-0.58/java/CRFPP_wrap.cxx\n",
            "CRF++-0.58/encoder.h\n",
            "CRF++-0.58/crfpp.h\n",
            "CRF++-0.58/perl/\n",
            "CRF++-0.58/perl/test.pl\n",
            "CRF++-0.58/perl/README\n",
            "CRF++-0.58/perl/Makefile.PL\n",
            "CRF++-0.58/perl/CRFPP_wrap.cxx\n",
            "CRF++-0.58/perl/CRFPP.pm\n",
            "CRF++-0.58/aclocal.m4\n",
            "CRF++-0.58/lbfgs.cpp\n",
            "CRF++-0.58/lbfgs.h\n",
            "CRF++-0.58/install-sh\n",
            "CRF++-0.58/tagger.cpp\n",
            "CRF++-0.58/param.h\n",
            "CRF++-0.58/missing\n",
            "CRF++-0.58/feature_cache.h\n",
            "CRF++-0.58/feature.cpp\n",
            "CRF++-0.58/depcomp\n",
            "CRF++-0.58/stream_wrapper.h\n",
            "CRF++-0.58/sdk/\n",
            "CRF++-0.58/sdk/example.cpp\n",
            "CRF++-0.58/ruby/\n",
            "CRF++-0.58/ruby/extconf.rb\n",
            "CRF++-0.58/ruby/CRFPP_wrap.cpp\n",
            "CRF++-0.58/ruby/README\n",
            "CRF++-0.58/ruby/test.rb\n",
            "CRF++-0.58/configure.in\n",
            "CRF++-0.58/path.cpp\n",
            "CRF++-0.58/path.h\n",
            "CRF++-0.58/tagger.h\n",
            "CRF++-0.58/thread.h\n",
            "CRF++-0.58/example/\n",
            "CRF++-0.58/example/JapaneseNE/\n",
            "CRF++-0.58/example/JapaneseNE/train.data\n",
            "CRF++-0.58/example/JapaneseNE/test.data\n",
            "CRF++-0.58/example/JapaneseNE/exec.sh\n",
            "CRF++-0.58/example/JapaneseNE/template\n",
            "CRF++-0.58/example/chunking/\n",
            "CRF++-0.58/example/chunking/train.data\n",
            "CRF++-0.58/example/chunking/test.data\n",
            "CRF++-0.58/example/chunking/exec.sh\n",
            "CRF++-0.58/example/chunking/template\n",
            "CRF++-0.58/example/seg/\n",
            "CRF++-0.58/example/seg/train.data\n",
            "CRF++-0.58/example/seg/test.data\n",
            "CRF++-0.58/example/seg/exec.sh\n",
            "CRF++-0.58/example/seg/template\n",
            "CRF++-0.58/example/basenp/\n",
            "CRF++-0.58/example/basenp/train.data\n",
            "CRF++-0.58/example/basenp/test.data\n",
            "CRF++-0.58/example/basenp/exec.sh\n",
            "CRF++-0.58/example/basenp/template\n",
            "CRF++-0.58/Makefile.am\n",
            "/content/CRF++-0.58\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... gcc3\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... gcc3\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking for library containing strerror... none required\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... no\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7077: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... no\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for ANSI C header files... (cached) no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking math.h usability... yes\n",
            "checking math.h presence... yes\n",
            "checking for math.h... yes\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for size_t... yes\n",
            "checking for pow in -lm... yes\n",
            "checking for exp in -lm... yes\n",
            "checking for log in -lm... yes\n",
            "checking for pthread_create in -lpthread... yes\n",
            "checking for pthread_join in -lpthread... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking whether make is GNU Make... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports stl <sstream> (required)... yes\n",
            "checking if g++ supports stl <stdexcept> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports dynamic_cast<> (required)... yes\n",
            "checking if g++ supports exception handler (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ supports _SC_NPROCESSORS_CONF (optional)... yes\n",
            "checking if g++ environment provides all required features... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating Makefile.msvc\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "make  all-am\n",
            "make[1]: Entering directory '/content/CRF++-0.58'\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o libcrfpp.lo libcrfpp.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c libcrfpp.cpp  -fPIC -DPIC -o .libs/libcrfpp.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c libcrfpp.cpp -o libcrfpp.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o lbfgs.lo lbfgs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c lbfgs.cpp  -fPIC -DPIC -o .libs/lbfgs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c lbfgs.cpp -o lbfgs.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o encoder.lo encoder.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c encoder.cpp  -fPIC -DPIC -o .libs/encoder.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c encoder.cpp -o encoder.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature.lo feature.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature.cpp  -fPIC -DPIC -o .libs/feature.o\n",
            "In file included from \u001b[01m\u001b[Ktagger.h:14:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfeature.cpp:12\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:34:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget CRFPP::{anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature.cpp -o feature.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature_cache.lo feature_cache.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_cache.cpp  -fPIC -DPIC -o .libs/feature_cache.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_cache.cpp -o feature_cache.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature_index.lo feature_index.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_index.cpp  -fPIC -DPIC -o .libs/feature_index.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_index.cpp -o feature_index.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o node.lo node.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c node.cpp  -fPIC -DPIC -o .libs/node.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c node.cpp -o node.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o path.lo path.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c path.cpp  -fPIC -DPIC -o .libs/path.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c path.cpp -o path.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o tagger.lo tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c tagger.cpp  -fPIC -DPIC -o .libs/tagger.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c tagger.cpp -o tagger.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o libcrfpp.la -rpath /usr/local/lib libcrfpp.lo lbfgs.lo param.lo encoder.lo feature.lo feature_cache.lo feature_index.lo node.lo path.lo tagger.lo  -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/libcrfpp.o .libs/lbfgs.o .libs/param.o .libs/encoder.o .libs/feature.o .libs/feature_cache.o .libs/feature_index.o .libs/node.o .libs/path.o .libs/tagger.o   -lpthread -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libcrfpp.so.0 -o .libs/libcrfpp.so.0.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcrfpp.so.0\" && ln -s \"libcrfpp.so.0.0.0\" \"libcrfpp.so.0\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcrfpp.so\" && ln -s \"libcrfpp.so.0.0.0\" \"libcrfpp.so\")\n",
            "libtool: link: ar cru .libs/libcrfpp.a  libcrfpp.o lbfgs.o param.o encoder.o feature.o feature_cache.o feature_index.o node.o path.o tagger.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libcrfpp.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libcrfpp.la\" && ln -s \"../libcrfpp.la\" \"libcrfpp.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o crf_learn.o crf_learn.cpp\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o crf_learn crf_learn.o libcrfpp.la -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++ -O3 -Wall -o .libs/crf_learn crf_learn.o  ./.libs/libcrfpp.so -lpthread -lm\n",
            "g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o crf_test.o crf_test.cpp\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o crf_test crf_test.o libcrfpp.la  -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++ -O3 -Wall -o .libs/crf_test crf_test.o  ./.libs/libcrfpp.so -lpthread -lm\n",
            "make[1]: Leaving directory '/content/CRF++-0.58'\n",
            "make[1]: Entering directory '/content/CRF++-0.58'\n",
            "test -z \"/usr/local/lib\" || /bin/mkdir -p \"/usr/local/lib\"\n",
            " /bin/bash ./libtool   --mode=install /usr/bin/install -c   libcrfpp.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.so.0.0.0 /usr/local/lib/libcrfpp.so.0.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcrfpp.so.0.0.0 libcrfpp.so.0 || { rm -f libcrfpp.so.0 && ln -s libcrfpp.so.0.0.0 libcrfpp.so.0; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcrfpp.so.0.0.0 libcrfpp.so || { rm -f libcrfpp.so && ln -s libcrfpp.so.0.0.0 libcrfpp.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.lai /usr/local/lib/libcrfpp.la\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.a /usr/local/lib/libcrfpp.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libcrfpp.a\n",
            "libtool: install: ranlib /usr/local/lib/libcrfpp.a\n",
            "libtool: finish: PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            "  /bin/bash ./libtool   --mode=install /usr/bin/install -c crf_learn crf_test '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/crf_learn /usr/local/bin/crf_learn\n",
            "libtool: install: /usr/bin/install -c .libs/crf_test /usr/local/bin/crf_test\n",
            "test -z \"/usr/local/include\" || /bin/mkdir -p \"/usr/local/include\"\n",
            " /usr/bin/install -c -m 644 crfpp.h '/usr/local/include'\n",
            "make[1]: Leaving directory '/content/CRF++-0.58'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CaboChaのインストール\n",
        "## > @yamaru氏の言語処理100本ノック第5章記事より引用\n",
        "FILE_ID = \"0B4y35FiV1wh7SDd1Q1dUQkZQaUU\"\n",
        "FILE_NAME = \"cabocha.tar.bz2\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt\n",
        "!tar -xvf cabocha.tar.bz2\n",
        "%cd cabocha-0.69\n",
        "\n",
        "!./configure -with-charset=utf-8 && make && make check && make install && ldconfig\n",
        "\n",
        "%cd ~/../content"
      ],
      "metadata": {
        "id": "Kx7FR3SzOZZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c12eb7-228c-48c0-9a46-d1fcdfde4b71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-05 03:21:40--  https://docs.google.com/uc?export=download&confirm=t&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.122.139, 172.253.122.113, 172.253.122.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.122.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9nu4d4tnr6eco0b3817h4ua1for40877/1649128875000/13553212398903315502/*/0B4y35FiV1wh7SDd1Q1dUQkZQaUU?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-05 03:21:40--  https://doc-04-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9nu4d4tnr6eco0b3817h4ua1for40877/1649128875000/13553212398903315502/*/0B4y35FiV1wh7SDd1Q1dUQkZQaUU?e=download\n",
            "Resolving doc-04-74-docs.googleusercontent.com (doc-04-74-docs.googleusercontent.com)... 172.253.122.132, 2607:f8b0:4004:c09::84\n",
            "Connecting to doc-04-74-docs.googleusercontent.com (doc-04-74-docs.googleusercontent.com)|172.253.122.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84638995 (81M) [application/x-bzip2]\n",
            "Saving to: ‘cabocha.tar.bz2’\n",
            "\n",
            "cabocha.tar.bz2     100%[===================>]  80.72M  43.5MB/s    in 1.9s    \n",
            "\n",
            "2022-04-05 03:21:42 (43.5 MB/s) - ‘cabocha.tar.bz2’ saved [84638995/84638995]\n",
            "\n",
            "cabocha-0.69/\n",
            "cabocha-0.69/cabocha-config.in\n",
            "cabocha-0.69/compile\n",
            "cabocha-0.69/swig/\n",
            "cabocha-0.69/swig/version.h.in\n",
            "cabocha-0.69/swig/Makefile\n",
            "cabocha-0.69/swig/version.h\n",
            "cabocha-0.69/swig/CaboCha.i\n",
            "cabocha-0.69/missing\n",
            "cabocha-0.69/java/\n",
            "cabocha-0.69/java/test.java\n",
            "cabocha-0.69/java/Makefile\n",
            "cabocha-0.69/java/org/\n",
            "cabocha-0.69/java/org/chasen/\n",
            "cabocha-0.69/java/org/chasen/cabocha/\n",
            "cabocha-0.69/java/org/chasen/cabocha/FormatType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/OutputLayerType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Token.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboChaConstants.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/ParserType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/ParsingAlgorithm.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Chunk.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/InputLayerType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboCha.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboChaJNI.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/PossetType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Tree.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CharsetType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Parser.java\n",
            "cabocha-0.69/java/CaboCha_wrap.cxx\n",
            "cabocha-0.69/ltmain.sh\n",
            "cabocha-0.69/config.guess\n",
            "cabocha-0.69/man/\n",
            "cabocha-0.69/man/Makefile.in\n",
            "cabocha-0.69/man/cabocha.1\n",
            "cabocha-0.69/man/Makefile.am\n",
            "cabocha-0.69/BSD\n",
            "cabocha-0.69/python/\n",
            "cabocha-0.69/python/test.py\n",
            "cabocha-0.69/python/CaboCha.py\n",
            "cabocha-0.69/python/CaboCha_wrap.cxx\n",
            "cabocha-0.69/python/setup.py\n",
            "cabocha-0.69/AUTHORS\n",
            "cabocha-0.69/ruby/\n",
            "cabocha-0.69/ruby/CaboCha_wrap.cpp\n",
            "cabocha-0.69/ruby/extconf.rb\n",
            "cabocha-0.69/ruby/test.rb\n",
            "cabocha-0.69/Makefile.in\n",
            "cabocha-0.69/NEWS\n",
            "cabocha-0.69/install-sh\n",
            "cabocha-0.69/cabocha.iss.in\n",
            "cabocha-0.69/ChangeLog\n",
            "cabocha-0.69/configure\n",
            "cabocha-0.69/src/\n",
            "cabocha-0.69/src/string_buffer.cpp\n",
            "cabocha-0.69/src/tree_allocator.cpp\n",
            "cabocha-0.69/src/dep.h\n",
            "cabocha-0.69/src/dep_learner.cpp\n",
            "cabocha-0.69/src/tree_allocator.h\n",
            "cabocha-0.69/src/svm.h\n",
            "cabocha-0.69/src/svm.cpp\n",
            "cabocha-0.69/src/ucstable.h\n",
            "cabocha-0.69/src/utils.h\n",
            "cabocha-0.69/src/selector.cpp\n",
            "cabocha-0.69/src/chunk_learner.cpp\n",
            "cabocha-0.69/src/string_buffer.h\n",
            "cabocha-0.69/src/ucs.cpp\n",
            "cabocha-0.69/src/ne.cpp\n",
            "cabocha-0.69/src/eval.cpp\n",
            "cabocha-0.69/src/cabocha.cpp\n",
            "cabocha-0.69/src/Makefile.in\n",
            "cabocha-0.69/src/scoped_ptr.h\n",
            "cabocha-0.69/src/chunker.h\n",
            "cabocha-0.69/src/normalizer.rule\n",
            "cabocha-0.69/src/common.h\n",
            "cabocha-0.69/src/normalizer_rule.sh\n",
            "cabocha-0.69/src/darts.h\n",
            "cabocha-0.69/src/learner.cpp\n",
            "cabocha-0.69/src/cabocha.h\n",
            "cabocha-0.69/src/morph.h\n",
            "cabocha-0.69/src/svm_learn.cpp\n",
            "cabocha-0.69/src/Makefile.msvc.in\n",
            "cabocha-0.69/src/timer.h\n",
            "cabocha-0.69/src/chunker.cpp\n",
            "cabocha-0.69/src/utils.cpp\n",
            "cabocha-0.69/src/param.h\n",
            "cabocha-0.69/src/winmain.h\n",
            "cabocha-0.69/src/normalizer.h\n",
            "cabocha-0.69/src/param.cpp\n",
            "cabocha-0.69/src/parser.cpp\n",
            "cabocha-0.69/src/ne.h\n",
            "cabocha-0.69/src/normalizer_rule.h\n",
            "cabocha-0.69/src/svm_learn.h\n",
            "cabocha-0.69/src/ucs.h\n",
            "cabocha-0.69/src/cabocha-model-index.cpp\n",
            "cabocha-0.69/src/mmap.h\n",
            "cabocha-0.69/src/analyzer.h\n",
            "cabocha-0.69/src/make.bat\n",
            "cabocha-0.69/src/tree.cpp\n",
            "cabocha-0.69/src/char_category.h\n",
            "cabocha-0.69/src/Makefile.am\n",
            "cabocha-0.69/src/dep.cpp\n",
            "cabocha-0.69/src/morph.cpp\n",
            "cabocha-0.69/src/selector_pat.h\n",
            "cabocha-0.69/src/cabocha-system-eval.cpp\n",
            "cabocha-0.69/src/cabocha-learn.cpp\n",
            "cabocha-0.69/src/stream_wrapper.h\n",
            "cabocha-0.69/src/selector.h\n",
            "cabocha-0.69/src/libcabocha.cpp\n",
            "cabocha-0.69/src/normalizer.cpp\n",
            "cabocha-0.69/src/freelist.h\n",
            "cabocha-0.69/perl/\n",
            "cabocha-0.69/perl/test.pl\n",
            "cabocha-0.69/perl/Makefile.PL\n",
            "cabocha-0.69/perl/CaboCha_wrap.o\n",
            "cabocha-0.69/perl/CaboCha.bs\n",
            "cabocha-0.69/perl/blib/\n",
            "cabocha-0.69/perl/blib/bin/\n",
            "cabocha-0.69/perl/blib/bin/.exists\n",
            "cabocha-0.69/perl/blib/arch/\n",
            "cabocha-0.69/perl/blib/arch/.exists\n",
            "cabocha-0.69/perl/blib/arch/auto/\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/.exists\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/CaboCha.so\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/CaboCha.bs\n",
            "cabocha-0.69/perl/blib/lib/\n",
            "cabocha-0.69/perl/blib/lib/.exists\n",
            "cabocha-0.69/perl/blib/lib/auto/\n",
            "cabocha-0.69/perl/blib/lib/auto/CaboCha/\n",
            "cabocha-0.69/perl/blib/lib/auto/CaboCha/.exists\n",
            "cabocha-0.69/perl/blib/lib/CaboCha.pm\n",
            "cabocha-0.69/perl/blib/man1/\n",
            "cabocha-0.69/perl/blib/man1/.exists\n",
            "cabocha-0.69/perl/blib/script/\n",
            "cabocha-0.69/perl/blib/script/.exists\n",
            "cabocha-0.69/perl/blib/man3/\n",
            "cabocha-0.69/perl/blib/man3/.exists\n",
            "cabocha-0.69/perl/CaboCha_wrap.cxx\n",
            "cabocha-0.69/perl/pm_to_blib\n",
            "cabocha-0.69/perl/CaboCha.pm\n",
            "cabocha-0.69/perl/MYMETA.yml\n",
            "cabocha-0.69/config.rpath\n",
            "cabocha-0.69/TODO\n",
            "cabocha-0.69/configure.in\n",
            "cabocha-0.69/config.sub\n",
            "cabocha-0.69/LGPL\n",
            "cabocha-0.69/tools/\n",
            "cabocha-0.69/tools/kc2cabocha.pl\n",
            "cabocha-0.69/tools/irex2cabocha.pl\n",
            "cabocha-0.69/tools/chasen2mecab.pl\n",
            "cabocha-0.69/tools/kc2juman.pl\n",
            "cabocha-0.69/tools/KyotoCorpus.pm\n",
            "cabocha-0.69/tools/KNBC2KC.pl\n",
            "cabocha-0.69/cabocharc.in\n",
            "cabocha-0.69/INSTALL\n",
            "cabocha-0.69/aclocal.m4\n",
            "cabocha-0.69/README\n",
            "cabocha-0.69/config.h.in\n",
            "cabocha-0.69/COPYING\n",
            "cabocha-0.69/example/\n",
            "cabocha-0.69/example/example2.cpp\n",
            "cabocha-0.69/example/example.c\n",
            "cabocha-0.69/Makefile.am\n",
            "cabocha-0.69/model/\n",
            "cabocha-0.69/model/dep.ipa.txt\n",
            "cabocha-0.69/model/ne.juman.txt\n",
            "cabocha-0.69/model/dep.juman.txt\n",
            "cabocha-0.69/model/Makefile.in\n",
            "cabocha-0.69/model/dep.unidic.txt\n",
            "cabocha-0.69/model/chunk.ipa.txt\n",
            "cabocha-0.69/model/chunk.unidic.txt\n",
            "cabocha-0.69/model/ne.ipa.txt\n",
            "cabocha-0.69/model/ne.unidic.txt\n",
            "cabocha-0.69/model/chunk.juman.txt\n",
            "cabocha-0.69/model/Makefile.am\n",
            "cabocha-0.69/doc/\n",
            "cabocha-0.69/doc/README.txt\n",
            "cabocha-0.69/doc/doxygen/\n",
            "cabocha-0.69/doc/doxygen/classes.html\n",
            "cabocha-0.69/doc/doxygen/ftv2plastnode.png\n",
            "cabocha-0.69/doc/doxygen/nav_g.png\n",
            "cabocha-0.69/doc/doxygen/files.html\n",
            "cabocha-0.69/doc/doxygen/tab_b.gif\n",
            "cabocha-0.69/doc/doxygen/nav_h.png\n",
            "cabocha-0.69/doc/doxygen/namespaceCaboCha.html\n",
            "cabocha-0.69/doc/doxygen/functions_vars.html\n",
            "cabocha-0.69/doc/doxygen/tab_s.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_eval.html\n",
            "cabocha-0.69/doc/doxygen/ftv2pnode.png\n",
            "cabocha-0.69/doc/doxygen/cabocha_8h.html\n",
            "cabocha-0.69/doc/doxygen/open.png\n",
            "cabocha-0.69/doc/doxygen/globals_func.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__token__t.html\n",
            "cabocha-0.69/doc/doxygen/doxygen.css\n",
            "cabocha-0.69/doc/doxygen/ftv2node.png\n",
            "cabocha-0.69/doc/doxygen/functions_func.html\n",
            "cabocha-0.69/doc/doxygen/ftv2mnode.png\n",
            "cabocha-0.69/doc/doxygen/ftv2doc.png\n",
            "cabocha-0.69/doc/doxygen/globals_enum.html\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Tree.html\n",
            "cabocha-0.69/doc/doxygen/functions.html\n",
            "cabocha-0.69/doc/doxygen/ftv2folderopen.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers.html\n",
            "cabocha-0.69/doc/doxygen/globals.html\n",
            "cabocha-0.69/doc/doxygen/ftv2link.png\n",
            "cabocha-0.69/doc/doxygen/ftv2folderclosed.png\n",
            "cabocha-0.69/doc/doxygen/structcabocha__token__t-members.html\n",
            "cabocha-0.69/doc/doxygen/bdwn.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_func.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__chunk__t.html\n",
            "cabocha-0.69/doc/doxygen/bc_s.png\n",
            "cabocha-0.69/doc/doxygen/cabocha_8h_source.html\n",
            "cabocha-0.69/doc/doxygen/globals_eval.html\n",
            "cabocha-0.69/doc/doxygen/ftv2mo.png\n",
            "cabocha-0.69/doc/doxygen/doxygen.png\n",
            "cabocha-0.69/doc/doxygen/index.html\n",
            "cabocha-0.69/doc/doxygen/tab_b.png\n",
            "cabocha-0.69/doc/doxygen/closed.png\n",
            "cabocha-0.69/doc/doxygen/nav_f.png\n",
            "cabocha-0.69/doc/doxygen/ftv2lastnode.png\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Tree-members.html\n",
            "cabocha-0.69/doc/doxygen/tabs.css\n",
            "cabocha-0.69/doc/doxygen/ftv2vertline.png\n",
            "cabocha-0.69/doc/doxygen/ftv2cl.png\n",
            "cabocha-0.69/doc/doxygen/tab_h.png\n",
            "cabocha-0.69/doc/doxygen/globals_type.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__chunk__t-members.html\n",
            "cabocha-0.69/doc/doxygen/globals_defs.html\n",
            "cabocha-0.69/doc/doxygen/annotated.html\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_type.html\n",
            "cabocha-0.69/doc/doxygen/tab_l.gif\n",
            "cabocha-0.69/doc/doxygen/tab_a.png\n",
            "cabocha-0.69/doc/doxygen/sync_off.png\n",
            "cabocha-0.69/doc/doxygen/ftv2ns.png\n",
            "cabocha-0.69/doc/doxygen/tab_r.gif\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Parser-members.html\n",
            "cabocha-0.69/doc/doxygen/ftv2splitbar.png\n",
            "cabocha-0.69/doc/doxygen/ftv2mlastnode.png\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Parser.html\n",
            "cabocha-0.69/doc/doxygen/namespaces.html\n",
            "cabocha-0.69/doc/doxygen/sync_on.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_enum.html\n",
            "cabocha-0.69/doc/doxygen/dir_68267d1309a1af8e8297ef4c3efbcdba.html\n",
            "cabocha-0.69/doc/doxygen/dynsections.js\n",
            "cabocha-0.69/doc/doxygen/ftv2blank.png\n",
            "cabocha-0.69/doc/cabocha.cfg\n",
            "/content/cabocha-0.69\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports nested variables... yes\n",
            "checking whether to enable maintainer-specific portions of Makefiles... no\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking whether gcc understands -c and -o together... yes\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... none\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... none\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... dlltool\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7604: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for library containing strerror... none required\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for shared library run path origin... done\n",
            "checking for iconv... yes\n",
            "checking for working iconv... yes\n",
            "checking for iconv declaration... \n",
            "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
            "checking for ANSI C header files... (cached) yes\n",
            "checking for an ANSI C-conforming const... yes\n",
            "checking whether byte ordering is bigendian... no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking dirent.h usability... yes\n",
            "checking dirent.h presence... yes\n",
            "checking for dirent.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking io.h usability... no\n",
            "checking io.h presence... no\n",
            "checking for io.h... no\n",
            "checking windows.h usability... no\n",
            "checking windows.h presence... no\n",
            "checking for windows.h... no\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for off_t... yes\n",
            "checking for size_t... yes\n",
            "checking size of char... 1\n",
            "checking size of short... 2\n",
            "checking size of int... 4\n",
            "checking size of long... 8\n",
            "checking size of long long... 8\n",
            "checking size of size_t... 8\n",
            "checking for size_t... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for getenv... yes\n",
            "checking for opendir... yes\n",
            "checking for snprintf... yes\n",
            "checking for mecab-config... /usr/bin/mecab-config\n",
            "checking whether iconv supports EUC-JP-MS and CP932... checking for main in -lstdc++... yes\n",
            "checking for crfpp_new in -lcrfpp... yes\n",
            "checking for mecab_new in -lmecab... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <list> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <queue> (required)... yes\n",
            "checking if g++ supports stl <functional> (required)... yes\n",
            "checking if g++ supports stl <algorithm> (required)... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <strstream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports dynamic_cast<> (required)... yes\n",
            "checking if g++ supports reinterpret_cast<> (required)... yes\n",
            "checking if g++ supports exception handler (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ environment provides all required features... yes\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating src/Makefile\n",
            "config.status: creating src/Makefile.msvc\n",
            "config.status: creating model/Makefile\n",
            "config.status: creating man/Makefile\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating cabocha-config\n",
            "config.status: creating cabocharc\n",
            "config.status: creating cabocha.iss\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "config.status: executing default commands\n",
            "make  all-recursive\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "Making all in src\n",
            "make[2]: Entering directory '/content/cabocha-0.69/src'\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o chunk_learner.lo chunk_learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunk_learner.cpp  -fPIC -DPIC -o .libs/chunk_learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunk_learner.cpp -o chunk_learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o chunker.lo chunker.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunker.cpp  -fPIC -DPIC -o .libs/chunker.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunker.cpp -o chunker.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o dep.lo dep.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep.cpp  -fPIC -DPIC -o .libs/dep.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep.cpp -o dep.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o dep_learner.lo dep_learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep_learner.cpp  -fPIC -DPIC -o .libs/dep_learner.o\n",
            "In file included from \u001b[01m\u001b[Kdep_learner.cpp:17:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep_learner.cpp -o dep_learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o eval.lo eval.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c eval.cpp  -fPIC -DPIC -o .libs/eval.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c eval.cpp -o eval.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o learner.lo learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c learner.cpp  -fPIC -DPIC -o .libs/learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c learner.cpp -o learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o libcabocha.lo libcabocha.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c libcabocha.cpp  -fPIC -DPIC -o .libs/libcabocha.o\n",
            "In file included from \u001b[01m\u001b[Klibcabocha.cpp:18:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c libcabocha.cpp -o libcabocha.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o morph.lo morph.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c morph.cpp  -fPIC -DPIC -o .libs/morph.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c morph.cpp -o morph.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o ne.lo ne.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ne.cpp  -fPIC -DPIC -o .libs/ne.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ne.cpp -o ne.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o normalizer.lo normalizer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c normalizer.cpp  -fPIC -DPIC -o .libs/normalizer.o\n",
            "\u001b[01m\u001b[Knormalizer.cpp:\u001b[m\u001b[K In static member function '\u001b[01m\u001b[Kstatic void CaboCha::Normalizer::normalize(int, const char*, size_t, std::__cxx11::string*)\u001b[m\u001b[K':\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c normalizer.cpp -o normalizer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o parser.lo parser.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c parser.cpp  -fPIC -DPIC -o .libs/parser.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c parser.cpp -o parser.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o selector.lo selector.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c selector.cpp  -fPIC -DPIC -o .libs/selector.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c selector.cpp -o selector.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o tree_allocator.lo tree_allocator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree_allocator.cpp  -fPIC -DPIC -o .libs/tree_allocator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree_allocator.cpp -o tree_allocator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o string_buffer.lo string_buffer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c string_buffer.cpp  -fPIC -DPIC -o .libs/string_buffer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c string_buffer.cpp -o string_buffer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o svm.lo svm.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm.cpp  -fPIC -DPIC -o .libs/svm.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm.cpp -o svm.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o svm_learn.lo svm_learn.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm_learn.cpp  -fPIC -DPIC -o .libs/svm_learn.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm_learn.cpp -o svm_learn.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o tree.lo tree.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree.cpp  -fPIC -DPIC -o .libs/tree.o\n",
            "In file included from \u001b[01m\u001b[Kstring_buffer.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Ktree.cpp:14\u001b[m\u001b[K:\n",
            "utils.h: In instantiation of '\u001b[01m\u001b[Ksize_t CaboCha::tokenizeCSV(char*, Iterator, size_t) [with Iterator = char**; size_t = long unsigned int]\u001b[m\u001b[K':\n",
            "\u001b[01m\u001b[Ktree.cpp:480:57:\u001b[m\u001b[K   required from here\n",
            "\u001b[01m\u001b[Kutils.h:127:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable '\u001b[01m\u001b[Kinquote\u001b[m\u001b[K' set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     bool \u001b[01;35m\u001b[Kinquote\u001b[m\u001b[K = false;\n",
            "          \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree.cpp -o tree.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o ucs.lo ucs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ucs.cpp  -fPIC -DPIC -o .libs/ucs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ucs.cpp -o ucs.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o utils.lo utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c utils.cpp  -fPIC -DPIC -o .libs/utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c utils.cpp -o utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall -no-undefined -version-info 5:0:0  -o libcabocha.la -rpath /usr/local/lib chunk_learner.lo chunker.lo dep.lo dep_learner.lo eval.lo learner.lo libcabocha.lo morph.lo ne.lo normalizer.lo param.lo parser.lo selector.lo tree_allocator.lo string_buffer.lo svm.lo svm_learn.lo tree.lo ucs.lo utils.lo  -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/chunk_learner.o .libs/chunker.o .libs/dep.o .libs/dep_learner.o .libs/eval.o .libs/learner.o .libs/libcabocha.o .libs/morph.o .libs/ne.o .libs/normalizer.o .libs/param.o .libs/parser.o .libs/selector.o .libs/tree_allocator.o .libs/string_buffer.o .libs/svm.o .libs/svm_learn.o .libs/tree.o .libs/ucs.o .libs/utils.o   /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libcabocha.so.5 -o .libs/libcabocha.so.5.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcabocha.so.5\" && ln -s \"libcabocha.so.5.0.0\" \"libcabocha.so.5\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcabocha.so\" && ln -s \"libcabocha.so.5.0.0\" \"libcabocha.so\")\n",
            "libtool: link: ar cru .libs/libcabocha.a  chunk_learner.o chunker.o dep.o dep_learner.o eval.o learner.o libcabocha.o morph.o ne.o normalizer.o param.o parser.o selector.o tree_allocator.o string_buffer.o svm.o svm_learn.o tree.o ucs.o utils.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libcabocha.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libcabocha.la\" && ln -s \"../libcabocha.la\" \"libcabocha.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha.o cabocha.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha cabocha.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha cabocha.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-model-index.o cabocha-model-index.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-model-index cabocha-model-index.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-model-index cabocha-model-index.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-learn.o cabocha-learn.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-learn cabocha-learn.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-learn cabocha-learn.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-system-eval.o cabocha-system-eval.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-system-eval cabocha-system-eval.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-system-eval cabocha-system-eval.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making all in model\n",
            "make[2]: Entering directory '/content/cabocha-0.69/model'\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.ipa.txt chunk.ipa.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.juman.txt chunk.juman.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.unidic.txt chunk.unidic.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.ipa.txt dep.ipa.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2340864\n",
            "trie         size : 22100992\n",
            "feature size      : 122541\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 113308\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 1.98193e-07\n",
            "Done!\n",
            "14.02 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.juman.txt dep.juman.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2892800\n",
            "trie         size : 22047744\n",
            "feature size      : 149674\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 78200\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 2.27711e-07\n",
            "Done!\n",
            "13.98 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.unidic.txt dep.unidic.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2159616\n",
            "trie         size : 19891200\n",
            "feature size      : 121120\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 92412\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 2.27189e-07\n",
            "Done!\n",
            "12.55 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.ipa.txt ne.ipa.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.juman.txt ne.juman.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.unidic.txt ne.unidic.model\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making all in man\n",
            "make[2]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[2]: Entering directory '/content/cabocha-0.69'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "Making check in src\n",
            "make[1]: Entering directory '/content/cabocha-0.69/src'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making check in model\n",
            "make[1]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making check in man\n",
            "make[1]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "Making install in src\n",
            "make[1]: Entering directory '/content/cabocha-0.69/src'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/src'\n",
            " /bin/mkdir -p '/usr/local/lib'\n",
            " /bin/bash ../libtool   --mode=install /usr/bin/install -c   libcabocha.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.so.5.0.0 /usr/local/lib/libcabocha.so.5.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcabocha.so.5.0.0 libcabocha.so.5 || { rm -f libcabocha.so.5 && ln -s libcabocha.so.5.0.0 libcabocha.so.5; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcabocha.so.5.0.0 libcabocha.so || { rm -f libcabocha.so && ln -s libcabocha.so.5.0.0 libcabocha.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.lai /usr/local/lib/libcabocha.la\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.a /usr/local/lib/libcabocha.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libcabocha.a\n",
            "libtool: install: ranlib /usr/local/lib/libcabocha.a\n",
            "libtool: finish: PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            " /bin/mkdir -p '/usr/local/bin'\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c cabocha '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha /usr/local/bin/cabocha\n",
            " /bin/mkdir -p '/usr/local/libexec/cabocha'\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c cabocha-model-index cabocha-learn cabocha-system-eval '/usr/local/libexec/cabocha'\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-model-index /usr/local/libexec/cabocha/cabocha-model-index\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-learn /usr/local/libexec/cabocha/cabocha-learn\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-system-eval /usr/local/libexec/cabocha/cabocha-system-eval\n",
            " /bin/mkdir -p '/usr/local/include'\n",
            " /usr/bin/install -c -m 644 cabocha.h '/usr/local/include'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/src'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making install in model\n",
            "make[1]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/lib/cabocha/model'\n",
            " /usr/bin/install -c -m 644 chunk.ipa.model chunk.juman.model chunk.unidic.model dep.ipa.model dep.juman.model dep.unidic.model ne.ipa.model ne.juman.model ne.unidic.model '/usr/local/lib/cabocha/model'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/model'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making install in man\n",
            "make[1]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/share/man/man1'\n",
            " /usr/bin/install -c -m 644 cabocha.1 '/usr/local/share/man/man1'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "make[2]: Entering directory '/content/cabocha-0.69'\n",
            " /bin/mkdir -p '/usr/local/bin'\n",
            " /usr/bin/install -c cabocha-config '/usr/local/bin'\n",
            " /bin/mkdir -p '/usr/local/etc'\n",
            " /usr/bin/install -c -m 644 cabocharc '/usr/local/etc'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cabocha-pythonのインストール\n",
        "%cd ~/../content/cabocha-0.69/python\n",
        "!python setup.py build_ext\n",
        "!python setup.py install\n",
        "!pwd\n",
        "!ls\n",
        "%cd ~/../content"
      ],
      "metadata": {
        "id": "1ojGkcV_PyUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f23aba-666a-453f-938f-5a6bd0d1b81b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cabocha-0.69/python\n",
            "running build_ext\n",
            "building '_CaboCha' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/include -I/usr/include/python3.7m -c CaboCha_wrap.cxx -o build/temp.linux-x86_64-3.7/CaboCha_wrap.o\n",
            "warning: no library file corresponding to '-L/usr/lib/x86_64inux-gnu' found (skipping)\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/CaboCha_wrap.o -L/usr/local/lib -lcabocha -lcrfpp -lmecab -lmecab -lstdc++ -o build/lib.linux-x86_64-3.7/_CaboCha.cpython-37m-x86_64-linux-gnu.so\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "copying CaboCha.py -> build/lib.linux-x86_64-3.7\n",
            "running build_ext\n",
            "running install_lib\n",
            "copying build/lib.linux-x86_64-3.7/CaboCha.py -> /usr/local/lib/python3.7/dist-packages\n",
            "copying build/lib.linux-x86_64-3.7/_CaboCha.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/CaboCha.py to CaboCha.cpython-37.pyc\n",
            "running install_egg_info\n",
            "Writing /usr/local/lib/python3.7/dist-packages/cabocha_python-0.69.egg-info\n",
            "/content/cabocha-0.69/python\n",
            "build  CaboCha.py  CaboCha_wrap.cxx  setup.py  test.py\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語をmatplotlibで表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# グラフ表示用\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "9-bEGSsEY0XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03abbfab-f48e-409f-ad74-64b1b0ccad20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 28.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from japanize-matplotlib) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->japanize-matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->japanize-matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120275 sha256=1c50130a63ba3061b1935d620b6d8d252b6abfa329a3f2e456fea8c4ff29d8a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/97/6b/e9e0cde099cc40f972b8dd23367308f7705ae06cd6d4714658\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 導入確認\n",
        "!mecab -v\n",
        "!pip show mecab-python3\n",
        "!cabocha -v\n",
        "!pip show cabocha-python"
      ],
      "metadata": {
        "id": "Pm42fDyZcnt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfbd41c-1e09-4ae5-e7d2-26547e081ee9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mecab of 0.996\n",
            "\n",
            "Name: mecab-python3\n",
            "Version: 0.7\n",
            "Summary: python wrapper for mecab: Morphological Analysis engine\n",
            "Home-page: https://github.com/SamuraiT/mecab-python3\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "cabocha of 0.69\n",
            "Name: cabocha-python\n",
            "Version: 0.69\n",
            "Summary: UNKNOWN\n",
            "Home-page: UNKNOWN\n",
            "Author: UNKNOWN\n",
            "Author-email: UNKNOWN\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MeCab TEST CODE ##\n",
        "\n",
        "import MeCab\n",
        "tagger = MeCab.Tagger(\"-Odump\")\n",
        "print(tagger.parse(\"隣の客はよく柿食う客だ。\"))\n",
        "print(tagger.parse(\"今日は一日中家にいることにしている。\"))"
      ],
      "metadata": {
        "id": "chRTBEz96TvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9d456c-6dfa-4d20-b063-1bae8f277ac1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0\n",
            "2 隣 名詞,一般,*,*,*,*,隣,トナリ,トナリ 0 3 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 6894\n",
            "6 の 助詞,連体化,*,*,*,*,の,ノ,ノ 3 6 368 368 24 6 0 1 0.000000 0.000000 0.000000 7268\n",
            "9 客 名詞,一般,*,*,*,*,客,キャク,キャク 6 9 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 10360\n",
            "13 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 9 12 261 261 16 6 0 1 0.000000 0.000000 0.000000 10380\n",
            "24 よく 副詞,一般,*,*,*,*,よく,ヨク,ヨク 12 18 1281 1281 34 6 0 1 0.000000 0.000000 0.000000 12183\n",
            "31 柿 名詞,一般,*,*,*,*,柿,カキ,カキ 18 21 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 18979\n",
            "37 食う 動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ 21 27 817 817 31 2 0 1 0.000000 0.000000 0.000000 24452\n",
            "45 客 名詞,一般,*,*,*,*,客,キャク,キャク 27 30 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 28626\n",
            "48 だ 助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ 30 33 453 453 25 6 0 1 0.000000 0.000000 0.000000 30648\n",
            "49 。 記号,句点,*,*,*,*,。,。,。 33 36 8 8 7 3 0 1 0.000000 0.000000 0.000000 26508\n",
            "51 EOS BOS/EOS,*,*,*,*,*,*,*,* 36 36 0 0 0 0 3 1 0.000000 0.000000 0.000000 24972\n",
            "\n",
            "0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0\n",
            "7 今日 名詞,副詞可能,*,*,*,*,今日,キョウ,キョー 0 6 1314 1314 67 2 0 1 0.000000 0.000000 0.000000 3947\n",
            "22 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 6 9 261 261 16 6 0 1 0.000000 0.000000 0.000000 4822\n",
            "28 一日中 名詞,一般,*,*,*,*,一日中,イチニチジュウ,イチニチジュー 9 18 1285 1285 38 8 0 1 0.000000 0.000000 0.000000 11799\n",
            "62 家 名詞,接尾,一般,*,*,*,家,カ,カ 18 21 1298 1298 51 2 0 1 0.000000 0.000000 0.000000 14853\n",
            "69 に 助詞,格助詞,一般,*,*,*,に,ニ,ニ 21 24 151 151 13 6 0 1 0.000000 0.000000 0.000000 14880\n",
            "80 いる 動詞,自立,*,*,一段,基本形,いる,イル,イル 24 30 619 619 31 6 0 1 0.000000 0.000000 0.000000 19162\n",
            "90 こと 名詞,非自立,一般,*,*,*,こと,コト,コト 30 36 1310 1310 63 6 0 1 0.000000 0.000000 0.000000 20026\n",
            "103 に 助詞,格助詞,一般,*,*,*,に,ニ,ニ 36 39 151 151 13 6 0 1 0.000000 0.000000 0.000000 19975\n",
            "110 し 動詞,自立,*,*,サ変・スル,連用形,する,シ,シ 39 42 610 610 31 6 0 1 0.000000 0.000000 0.000000 23328\n",
            "117 て 助詞,接続助詞,*,*,*,*,て,テ,テ 42 45 307 307 18 6 0 1 0.000000 0.000000 0.000000 21835\n",
            "128 いる 動詞,非自立,*,*,一段,基本形,いる,イル,イル 45 51 919 919 33 6 0 1 0.000000 0.000000 0.000000 22384\n",
            "134 。 記号,句点,*,*,*,*,。,。,。 51 54 8 8 7 3 0 1 0.000000 0.000000 0.000000 19114\n",
            "136 EOS BOS/EOS,*,*,*,*,*,*,*,* 54 54 0 0 0 0 3 1 0.000000 0.000000 0.000000 17578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CaboCha TEST CODE ##\n",
        "\n",
        "import CaboCha\n",
        "cp = CaboCha.Parser()\n",
        "sentence = '猫は道路を渡る犬を見た。'\n",
        "print(cp.parseToString(sentence))"
      ],
      "metadata": {
        "id": "M1QuOi6K1c-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7820fbaa-2524-4aaf-a4eb-4f26b7c9c4b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  猫は-------D\n",
            "  道路を-D   |\n",
            "      渡る-D |\n",
            "        犬を-D\n",
            "        見た。\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cabocha -f1 ai.ja.txt -o ai.ja.txt.parsed\n",
        "!head -25 ai.ja.txt.parsed"
      ],
      "metadata": {
        "id": "c49V6FTf4qVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25b8ec4-5af8-43a7-c471-0e281053815c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* 0 -1D 1/1 0.000000\n",
            "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
            "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
            "EOS\n",
            "EOS\n",
            "* 0 17D 1/1 0.388993\n",
            "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
            "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
            "* 1 17D 2/3 0.613549\n",
            "（\t記号,括弧開,*,*,*,*,（,（,（\n",
            "じん\t名詞,一般,*,*,*,*,じん,ジン,ジン\n",
            "こうち\t名詞,一般,*,*,*,*,こうち,コウチ,コーチ\n",
            "のう\t助詞,終助詞,*,*,*,*,のう,ノウ,ノー\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "* 2 3D 0/0 0.758984\n",
            "AI\t名詞,一般,*,*,*,*,*\n",
            "* 3 17D 1/5 0.517898\n",
            "〈\t記号,括弧開,*,*,*,*,〈,〈,〈\n",
            "エーアイ\t名詞,固有名詞,一般,*,*,*,*\n",
            "〉\t記号,括弧閉,*,*,*,*,〉,〉,〉\n",
            "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
            "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CaboChaの出力形式\n",
        "cabochaを使うと以下のような形式で出力される\n",
        "\n",
        "    * 0 1D 0/1 2.206035\n",
        "    隣\t名詞,一般,*,*,*,*,隣,トナリ,トナリ\n",
        "    の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
        "    * 1 5D 0/1 -0.593304\n",
        "    客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
        "    は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
        "    * 2 4D 0/0 0.538813\n",
        "    よく\t副詞,一般,*,*,*,*,よく,ヨク,ヨク\n",
        "    * 3 4D 0/0 1.985106\n",
        "    柿\t名詞,一般,*,*,*,*,柿,カキ,カキ\n",
        "    * 4 5D 0/0 -0.593304\n",
        "    食う\t動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ\n",
        "    * 5 -1D 0/1 0.000000\n",
        "    客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
        "    だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
        "    EOS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhmGwHRUGhid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"隣の客はよく柿食う客だ\" | cabocha -f1 "
      ],
      "metadata": {
        "id": "oHuTIxZEIfpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bfd3a9-bec1-462a-fa40-51e7d3b5ead4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* 0 1D 0/1 2.206035\n",
            "隣\t名詞,一般,*,*,*,*,隣,トナリ,トナリ\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "* 1 5D 0/1 -0.593304\n",
            "客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "* 2 4D 0/0 0.538813\n",
            "よく\t副詞,一般,*,*,*,*,よく,ヨク,ヨク\n",
            "* 3 4D 0/0 1.985106\n",
            "柿\t名詞,一般,*,*,*,*,柿,カキ,カキ\n",
            "* 4 5D 0/0 -0.593304\n",
            "食う\t動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ\n",
            "* 5 -1D 0/1 0.000000\n",
            "客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
            "だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
            "EOS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "これは各文節毎に区切ってあり、以下のように読む\n",
        "\n",
        "- 1行目\n",
        "  1. *\n",
        "  2. 文節番号\n",
        "  3. 係り先の文節番号(係り先なし:-1)\n",
        "  4. 主辞の形態素番号/機能語の形態素番号\n",
        "  5. 係り関係のスコア(大きい方が係りやすい)\n",
        "- 2行目\n",
        "  1. 表層形 （Tab区切り）\n",
        "  2. 品詞\n",
        "  3. 品詞細分類1\n",
        "  4. 品詞細分類2\n",
        "  5. 品詞細分類3\n",
        "  6. 活用形\n",
        "  7. 活用型\n",
        "  8. 原形\n",
        "  9. 読み\n",
        "  10. 発音\n",
        "\n",
        "#### [参考]\n",
        "- CaboChaで始める係り受け解析 by @nezuq\n",
        "  - Qiita https://qiita.com/nezuq/items/f481f07fc0576b38e81d"
      ],
      "metadata": {
        "id": "jW31L0z5JH_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 40. 係り受け解析結果の読み込み（形態素）\n",
        "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．\n",
        "\n"
      ],
      "metadata": {
        "id": "efvr7lJWGCvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsUvj8-OGCvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bc4e28-91ab-4b5a-e0b6-63046e45a5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'surface': '人工', 'base': '人工', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '知能', 'base': '知能', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '人工', 'base': '人工', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '知能', 'base': '知能', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '（', 'base': '（', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': 'じん', 'base': 'じん', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': 'こうち', 'base': 'こうち', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': 'のう', 'base': 'のう', 'pos': '助詞', 'pos1': '終助詞'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': 'AI', 'base': '*\\n', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '〈', 'base': '〈', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': 'エーアイ', 'base': '*\\n', 'pos': '名詞', 'pos1': '固有名詞'}\n",
            "{'surface': '〉', 'base': '〉', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': '）', 'base': '）', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': 'と', 'base': 'と', 'pos': '助詞', 'pos1': '格助詞'}\n",
            "{'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': '「', 'base': '「', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '『', 'base': '『', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '計算', 'base': '計算', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
            "{'surface': '（', 'base': '（', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '）', 'base': '）', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': '』', 'base': '』', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': 'という', 'base': 'という', 'pos': '助詞', 'pos1': '格助詞'}\n"
          ]
        }
      ],
      "source": [
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, morph):\n",
        "    self.surface = morph[\"surface\"]  # 表層形\n",
        "    self.base = morph[\"base\"]  # 基本形\n",
        "    self.pos = morph[\"pos\"]  # 品詞\n",
        "    self.pos1 = morph[\"pos1\"]  # 品詞細分類1\n",
        "\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(morphemes[0]) : word[0], # 表層形\n",
        "    str(morphemes[1]) : morpheme[6], # 基本形\n",
        "    str(morphemes[2]) : morpheme[0], # 品詞\n",
        "    str(morphemes[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "\n",
        "morph_list = []\n",
        "sentence = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        sentence.append(Morph(make_mapping(word)))\n",
        "\n",
        "\n",
        "for m in sentence[:25]:\n",
        "  print(vars(m))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
        "\n"
      ],
      "metadata": {
        "id": "NVOr6AErGCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 苦悩のログ\n",
        "> 1行ずつ読んで、Morphを作って、EOSか文節情報が来たら直前の文節を格納…ン？？？？どうやんの？？？？\n",
        "  → 1行ずつ読むのではなくて、`EOS\\n` で分割してブロックごとに処理すると以下のメリットが有る\n",
        "  - EOSは出てこない\n",
        "  - 文節ごとに処理するので行読みしても文節頭の係り受け情報が保持できる\n",
        "\n",
        "> あと文節の係り元の方では「DPじゃないし逆引き無理じゃない？」と思ってたら普通に他の人も1周for回してやってた。そらそうか\n",
        "\n",
        "> といって安直にforを回すと第2文以降でまたCaboChaによる係り番号が0にリセットされるので頭の方だけ以上に係り受けすることになってしまった。\n",
        "  - Class : Chunkに文節番号を保持する要素を付加することで半ば無理やり解決\n",
        "\n",
        "\n",
        "なぜか1文目の34語目までしか処理されず、35語以降が処理されていない…そんなことある？\n",
        "\n",
        "> $「…情報処理システムの設計や実現に関する/研究分野」ともされる。」$\n",
        "> の、`/`部分までしか処理されていない…\n",
        "\n",
        "    # 33  形態素: ['設計', 'や'],\t 係り先: 34, 係り元: []\n",
        "    # 34  形態素: ['実現', 'に関する'],\t 係り先: 35, 係り元: [33, 33]\n",
        "    #  1  形態素: ['『', '日本', '大', '百科全書', '(', 'ニッポニカ', ')』', 'の'],\t 係り先: 5, 係り元: []\n",
        "    #  2  形態素: ['解説', 'で', '、'],\t 係り先: 3, 係り元: []\n",
        "\n",
        ">> → 文章末尾の処理でforの内外の位置を間違えていただけでした\n",
        "\n",
        "> (環境構築含め)実質4日(週跨ぎなので6日？)くらいかけてようやくできた…\n",
        "  自作Classを含むClass、のリスト、のリストという面倒なことでしかも通し番号とか処理してたら本当に面倒になってた"
      ],
      "metadata": {
        "id": "68hUkbcLlnCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "i = 0\n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    #print(f\"block:{block}\")\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      #print(line)\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          #print(line)\n",
        "          #print(f\"morphemes:{[m.surface for m in morphemes]}\")\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        #print(word)\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      #print(f\"morphemes:{[m.surface for m in morphemes]}\")\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0 : all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(all_text[:10]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence:\n",
        "    print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4G8JMAmzGCvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bee0fa7-8494-4fda-cb0e-4244159a51fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "#  0  形態素: ['人工', '知能'],\t 係り先: -1, 係り元: []\n",
            "--- Sentence 0001 ---\n",
            "#  0  形態素: ['人工', '知能'],\t 係り先: 17, 係り元: []\n",
            "#  1  形態素: ['（', 'じん', 'こうち', 'のう', '、', '、'],\t 係り先: 17, 係り元: []\n",
            "#  2  形態素: ['AI'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['〈', 'エーアイ', '〉', '）', 'と', 'は', '、'],\t 係り先: 17, 係り元: [2]\n",
            "#  4  形態素: ['「', '『', '計算'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['（', '）', '』', 'という'],\t 係り先: 9, 係り元: [4]\n",
            "#  6  形態素: ['概念', 'と'],\t 係り先: 9, 係り元: []\n",
            "#  7  形態素: ['『', 'コンピュータ'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['（', '）', '』', 'という'],\t 係り先: 9, 係り元: [7]\n",
            "#  9  形態素: ['道具', 'を'],\t 係り先: 10, 係り元: [5, 6, 8]\n",
            "# 10  形態素: ['用い', 'て'],\t 係り先: 12, 係り元: [9]\n",
            "# 11  形態素: ['『', '知能', '』', 'を'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['研究', 'する'],\t 係り先: 13, 係り元: [10, 11]\n",
            "# 13  形態素: ['計算', '機', '科学'],\t 係り先: 14, 係り元: [12]\n",
            "# 14  形態素: ['（', '）', 'の'],\t 係り先: 15, 係り元: [13]\n",
            "# 15  形態素: ['一', '分野', '」', 'を'],\t 係り先: 16, 係り元: [14]\n",
            "# 16  形態素: ['指す'],\t 係り先: 17, 係り元: [15]\n",
            "# 17  形態素: ['語', '。'],\t 係り先: 34, 係り元: [0, 1, 3, 16]\n",
            "# 18  形態素: ['「', '言語', 'の'],\t 係り先: 20, 係り元: []\n",
            "# 19  形態素: ['理解', 'や'],\t 係り先: 20, 係り元: []\n",
            "# 20  形態素: ['推論', '、'],\t 係り先: 21, 係り元: [18, 19]\n",
            "# 21  形態素: ['問題', '解決', 'など', 'の'],\t 係り先: 22, 係り元: [20]\n",
            "# 22  形態素: ['知的', '行動', 'を'],\t 係り先: 24, 係り元: [21]\n",
            "# 23  形態素: ['人間', 'に'],\t 係り先: 24, 係り元: []\n",
            "# 24  形態素: ['代わっ', 'て'],\t 係り先: 26, 係り元: [22, 23]\n",
            "# 25  形態素: ['コンピューター', 'に'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['行わ', 'せる'],\t 係り先: 27, 係り元: [24, 25]\n",
            "# 27  形態素: ['技術', '」', '、', 'または', '、'],\t 係り先: 34, 係り元: [26]\n",
            "# 28  形態素: ['「', '計算', '機'],\t 係り先: 29, 係り元: []\n",
            "# 29  形態素: ['（', 'コンピュータ', '）', 'による'],\t 係り先: 31, 係り元: [28]\n",
            "# 30  形態素: ['知的', 'な'],\t 係り先: 31, 係り元: []\n",
            "# 31  形態素: ['情報処理', 'システム', 'の'],\t 係り先: 33, 係り元: [29, 30]\n",
            "# 32  形態素: ['設計', 'や'],\t 係り先: 33, 係り元: []\n",
            "# 33  形態素: ['実現', 'に関する'],\t 係り先: 34, 係り元: [31, 32]\n",
            "# 34  形態素: ['研究', '分野', '」', 'と', 'も'],\t 係り先: 35, 係り元: [17, 27, 33]\n",
            "# 35  形態素: ['さ', 'れる', '。'],\t 係り先: -1, 係り元: [34]\n",
            "--- Sentence 0002 ---\n",
            "#  0  形態素: ['『', '日本', '大', '百科全書', '(', 'ニッポニカ', ')』', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['解説', 'で', '、'],\t 係り先: 5, 係り元: [0]\n",
            "#  2  形態素: ['情報', '工学', '者', '・', '通信', '工学', '者', 'の'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['佐藤', '理', '史', 'は'],\t 係り先: 5, 係り元: [2]\n",
            "#  4  形態素: ['次', 'の', 'よう', 'に'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['述べ', 'て', 'いる', '。'],\t 係り先: -1, 係り元: [1, 3, 4]\n",
            "--- Sentence 0003 ---\n",
            "#  0  形態素: ['人間', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['知的', '能力', 'を'],\t 係り先: 3, 係り元: [0]\n",
            "#  2  形態素: ['コンピュータ', '上', 'で'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['実現', 'する', '、'],\t 係り先: 5, 係り元: [1, 2]\n",
            "#  4  形態素: ['様々', 'な'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['技術', '・', 'ソフトウェア', '・', 'コンピュータ', 'システム', '。'],\t 係り先: 19, 係り元: [3, 4]\n",
            "#  6  形態素: ['応用', '例', 'は'],\t 係り先: 19, 係り元: []\n",
            "#  7  形態素: ['自然', '言語', '処理'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['（', '機械', '翻訳', '・', 'かな漢字', '変換', '・', '構文', '解析', '等', '）', '、'],\t 係り先: 9, 係り元: [7]\n",
            "#  9  形態素: ['専門', '家', 'の'],\t 係り先: 10, 係り元: [8]\n",
            "# 10  形態素: ['推論', '・', '判断', 'を'],\t 係り先: 11, 係り元: [9]\n",
            "# 11  形態素: ['模倣', 'する'],\t 係り先: 12, 係り元: [10]\n",
            "# 12  形態素: ['エキスパート', 'システム', '、'],\t 係り先: 18, 係り元: [11]\n",
            "# 13  形態素: ['画像', 'データ', 'を'],\t 係り先: 14, 係り元: []\n",
            "# 14  形態素: ['解析', 'し', 'て'],\t 係り先: 17, 係り元: [13]\n",
            "# 15  形態素: ['特定', 'の'],\t 係り先: 16, 係り元: []\n",
            "# 16  形態素: ['パターン', 'を'],\t 係り先: 17, 係り元: [15]\n",
            "# 17  形態素: ['検出', '・', '抽出', 'し', 'たり', 'する'],\t 係り先: 18, 係り元: [14, 16]\n",
            "# 18  形態素: ['画像', '認識', '等', 'が'],\t 係り先: 19, 係り元: [12, 17]\n",
            "# 19  形態素: ['ある', '。'],\t 係り先: 23, 係り元: [5, 6, 18]\n",
            "# 20  形態素: ['1956', '年', 'に'],\t 係り先: 23, 係り元: []\n",
            "# 21  形態素: ['ダート', 'マス', '会議', 'で'],\t 係り先: 23, 係り元: []\n",
            "# 22  形態素: ['ジョン', '・', 'マッカーシー', 'により'],\t 係り先: 23, 係り元: []\n",
            "# 23  形態素: ['命名', 'さ', 'れ', 'た', '。'],\t 係り先: 35, 係り元: [19, 20, 21, 22]\n",
            "# 24  形態素: ['現在', 'で', 'は', '、'],\t 係り先: 35, 係り元: []\n",
            "# 25  形態素: ['記号', '処理', 'を'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['用い', 'た'],\t 係り先: 27, 係り元: [25]\n",
            "# 27  形態素: ['知能', 'の'],\t 係り先: 28, 係り元: [26]\n",
            "# 28  形態素: ['記述', 'を'],\t 係り先: 30, 係り元: [27]\n",
            "# 29  形態素: ['主体', 'と'],\t 係り先: 30, 係り元: []\n",
            "# 30  形態素: ['する'],\t 係り先: 32, 係り元: [28, 29]\n",
            "# 31  形態素: ['情報処理', 'や'],\t 係り先: 32, 係り元: []\n",
            "# 32  形態素: ['研究', 'で', 'の'],\t 係り先: 33, 係り元: [30, 31]\n",
            "# 33  形態素: ['アプローチ', 'という'],\t 係り先: 34, 係り元: [32]\n",
            "# 34  形態素: ['意味あい', 'でも'],\t 係り先: 35, 係り元: [33]\n",
            "# 35  形態素: ['使わ', 'れ', 'て', 'いる', '。'],\t 係り先: 43, 係り元: [23, 24, 34]\n",
            "# 36  形態素: ['家庭', '用', '電気', '機械', '器具', 'の'],\t 係り先: 37, 係り元: []\n",
            "# 37  形態素: ['制御', 'システム', 'や'],\t 係り先: 39, 係り元: [36]\n",
            "# 38  形態素: ['ゲーム', 'ソフト', 'の'],\t 係り先: 39, 係り元: []\n",
            "# 39  形態素: ['思考', 'ルーチン', 'も'],\t 係り先: 41, 係り元: [37, 38]\n",
            "# 40  形態素: ['こう'],\t 係り先: 41, 係り元: []\n",
            "# 41  形態素: ['呼ば', 'れる'],\t 係り先: 42, 係り元: [39, 40]\n",
            "# 42  形態素: ['こと', 'も'],\t 係り先: 43, 係り元: [41]\n",
            "# 43  形態素: ['ある', '。'],\t 係り先: -1, 係り元: [35, 42]\n",
            "--- Sentence 0004 ---\n",
            "#  0  形態素: ['プログラミング', '言語', 'による'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['「', '」', 'という'],\t 係り先: 2, 係り元: [0]\n",
            "#  2  形態素: ['カウンセラー', 'を'],\t 係り先: 3, 係り元: [1]\n",
            "#  3  形態素: ['模倣', 'し', 'た'],\t 係り先: 4, 係り元: [2]\n",
            "#  4  形態素: ['プログラム'],\t 係り先: 8, 係り元: [3]\n",
            "#  5  形態素: ['（', '人工', '無', '脳', '）', 'が'],\t 係り先: 8, 係り元: []\n",
            "#  6  形態素: ['しばしば'],\t 係り先: 8, 係り元: []\n",
            "#  7  形態素: ['引き合い', 'に'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['出さ', 'れる', 'が', '、'],\t 係り先: 27, 係り元: [4, 5, 6, 7]\n",
            "#  9  形態素: ['計算', '機', 'に'],\t 係り先: 13, 係り元: []\n",
            "# 10  形態素: ['人間', 'の'],\t 係り先: 11, 係り元: []\n",
            "# 11  形態素: ['専門', '家', 'の'],\t 係り先: 12, 係り元: [10]\n",
            "# 12  形態素: ['役割', 'を'],\t 係り先: 13, 係り元: [11]\n",
            "# 13  形態素: ['さ', 'せよ', 'う', 'という'],\t 係り先: 14, 係り元: [9, 12]\n",
            "# 14  形態素: ['「', 'エキスパート', 'システム', '」', 'と'],\t 係り先: 15, 係り元: [13]\n",
            "# 15  形態素: ['呼ば', 'れる'],\t 係り先: 16, 係り元: [14]\n",
            "# 16  形態素: ['研究', '・', '情報処理', 'システム', 'の'],\t 係り先: 17, 係り元: [15]\n",
            "# 17  形態素: ['実現', 'は', '、'],\t 係り先: 27, 係り元: [16]\n",
            "# 18  形態素: ['人間', 'が'],\t 係り先: 20, 係り元: []\n",
            "# 19  形態素: ['暗黙', 'に'],\t 係り先: 20, 係り元: []\n",
            "# 20  形態素: ['持つ'],\t 係り先: 21, 係り元: [18, 19]\n",
            "# 21  形態素: ['常識', 'の'],\t 係り先: 22, 係り元: [20]\n",
            "# 22  形態素: ['記述', 'が'],\t 係り先: 24, 係り元: [21]\n",
            "# 23  形態素: ['問題', 'と'],\t 係り先: 24, 係り元: []\n",
            "# 24  形態素: ['なり', '、'],\t 係り先: 27, 係り元: [22, 23]\n",
            "# 25  形態素: ['実用', 'へ', 'の'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['利用', 'が'],\t 係り先: 27, 係り元: [25]\n",
            "# 27  形態素: ['困難', '視', 'さ', 'れ', 'て', 'いる', '。'],\t 係り先: 42, 係り元: [8, 17, 24, 26]\n",
            "# 28  形態素: ['人工', '的', 'な'],\t 係り先: 29, 係り元: []\n",
            "# 29  形態素: ['知能', 'の'],\t 係り先: 30, 係り元: [28]\n",
            "# 30  形態素: ['実現', 'へ', 'の'],\t 係り先: 31, 係り元: [29]\n",
            "# 31  形態素: ['アプローチ', 'として', 'は', '、'],\t 係り先: 35, 係り元: [30]\n",
            "# 32  形態素: ['「', 'ファジィ', '理論', '」', 'や'],\t 係り先: 33, 係り元: []\n",
            "# 33  形態素: ['「', 'ニューラルネットワーク', '」', 'など', 'の', 'よう', 'な'],\t 係り先: 34, 係り元: [32]\n",
            "# 34  形態素: ['アプローチ', 'も'],\t 係り先: 35, 係り元: [33]\n",
            "# 35  形態素: ['知ら', 'れ', 'て', 'いる', 'が', '、'],\t 係り先: 42, 係り元: [31, 34]\n",
            "# 36  形態素: ['従来', 'の'],\t 係り先: 37, 係り元: []\n",
            "# 37  形態素: ['人工', '知能', 'で', 'ある'],\t 係り先: 38, 係り元: [36]\n",
            "# 38  形態素: ['(', 'Good', 'Old', 'Fashioned', 'AI', ')', 'と', 'の'],\t 係り先: 39, 係り元: [37]\n",
            "# 39  形態素: ['差', 'は'],\t 係り先: 42, 係り元: [38]\n",
            "# 40  形態素: ['記述', 'の'],\t 係り先: 41, 係り元: []\n",
            "# 41  形態素: ['記号', '的', '明示', '性', 'に'],\t 係り先: 42, 係り元: [40]\n",
            "# 42  形態素: ['ある', '。'],\t 係り先: 46, 係り元: [27, 35, 39, 41]\n",
            "# 43  形態素: ['その後'],\t 係り先: 46, 係り元: []\n",
            "# 44  形態素: ['「', 'サポートベクターマシン', '」', 'が'],\t 係り先: 46, 係り元: []\n",
            "# 45  形態素: ['注目', 'を'],\t 係り先: 46, 係り元: []\n",
            "# 46  形態素: ['集め', 'た', '。'],\t 係り先: 55, 係り元: [42, 43, 44, 45]\n",
            "# 47  形態素: ['また', '、'],\t 係り先: 55, 係り元: []\n",
            "# 48  形態素: ['自ら', 'の'],\t 係り先: 49, 係り元: []\n",
            "# 49  形態素: ['経験', 'を'],\t 係り先: 52, 係り元: [48]\n",
            "# 50  形態素: ['元', 'に'],\t 係り先: 52, 係り元: []\n",
            "# 51  形態素: ['学習', 'を'],\t 係り先: 52, 係り元: []\n",
            "# 52  形態素: ['行う'],\t 係り先: 53, 係り元: [49, 50, 51]\n",
            "# 53  形態素: ['強化', '学習', 'という'],\t 係り先: 54, 係り元: [52]\n",
            "# 54  形態素: ['手法', 'も'],\t 係り先: 55, 係り元: [53]\n",
            "# 55  形態素: ['ある', '。'],\t 係り先: 71, 係り元: [46, 47, 54]\n",
            "# 56  形態素: ['「', 'この'],\t 係り先: 57, 係り元: []\n",
            "# 57  形態素: ['宇宙', 'において', '、'],\t 係り先: 67, 係り元: [56]\n",
            "# 58  形態素: ['知性', 'と', 'は'],\t 係り先: 61, 係り元: []\n",
            "# 59  形態素: ['最も'],\t 係り先: 60, 係り元: []\n",
            "# 60  形態素: ['強力', 'な'],\t 係り先: 61, 係り元: [59]\n",
            "# 61  形態素: ['形質', 'で', 'ある'],\t 係り先: 62, 係り元: [58, 60]\n",
            "# 62  形態素: ['（', 'レイ', '・', 'カーツ', 'ワイル', '）', '」', 'という'],\t 係り先: 63, 係り元: [61]\n",
            "# 63  形態素: ['言葉', '通り', '、'],\t 係り先: 67, 係り元: [62]\n",
            "# 64  形態素: ['知性', 'を'],\t 係り先: 66, 係り元: []\n",
            "# 65  形態素: ['機械', '的', 'に'],\t 係り先: 66, 係り元: []\n",
            "# 66  形態素: ['表現', 'し'],\t 係り先: 67, 係り元: [64, 65]\n",
            "# 67  形態素: ['実装', 'する', 'という'],\t 係り先: 68, 係り元: [57, 63, 66]\n",
            "# 68  形態素: ['こと', 'は'],\t 係り先: 71, 係り元: [67]\n",
            "# 69  形態素: ['極めて'],\t 係り先: 70, 係り元: []\n",
            "# 70  形態素: ['重要', 'な'],\t 係り先: 71, 係り元: [69]\n",
            "# 71  形態素: ['作業', 'で', 'ある', '。'],\t 係り先: -1, 係り元: [55, 68, 70]\n",
            "--- Sentence 0005 ---\n",
            "#  0  形態素: ['2006', '年', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['ディープラーニング'],\t 係り先: 3, 係り元: [0]\n",
            "#  2  形態素: ['（', '深層', '学習', '）', 'の'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['登場', 'と'],\t 係り先: 7, 係り元: [1, 2]\n",
            "#  4  形態素: ['2010', '年代'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['以降', 'の'],\t 係り先: 6, 係り元: [4]\n",
            "#  6  形態素: ['ビッグ', 'データ', 'の'],\t 係り先: 7, 係り元: [5]\n",
            "#  7  形態素: ['登場', 'により', '、'],\t 係り先: 13, 係り元: [3, 6]\n",
            "#  8  形態素: ['一過', '性', 'の'],\t 係り先: 9, 係り元: []\n",
            "#  9  形態素: ['流行', 'を'],\t 係り先: 10, 係り元: [8]\n",
            "# 10  形態素: ['超え', 'て'],\t 係り先: 12, 係り元: [9]\n",
            "# 11  形態素: ['社会', 'に'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['浸透', 'し', 'て'],\t 係り先: 13, 係り元: [10, 11]\n",
            "# 13  形態素: ['行っ', 'た', '。'],\t 係り先: 36, 係り元: [7, 12]\n",
            "# 14  形態素: ['2016', '年', 'から'],\t 係り先: 15, 係り元: []\n",
            "# 15  形態素: ['2017', '年', 'にかけて', '、'],\t 係り先: 17, 係り元: [14]\n",
            "# 16  形態素: ['ディープラーニング', 'を'],\t 係り先: 17, 係り元: []\n",
            "# 17  形態素: ['導入', 'し', 'た'],\t 係り先: 18, 係り元: [15, 16]\n",
            "# 18  形態素: ['AI', 'が'],\t 係り先: 19, 係り元: [17]\n",
            "# 19  形態素: ['完全', '情報', 'ゲーム', 'で', 'ある'],\t 係り先: 20, 係り元: [18]\n",
            "# 20  形態素: ['囲碁', 'など', 'の'],\t 係り先: 21, 係り元: [19]\n",
            "# 21  形態素: ['トップ', '棋士', '、'],\t 係り先: 26, 係り元: [20]\n",
            "# 22  形態素: ['さらに'],\t 係り先: 23, 係り元: []\n",
            "# 23  形態素: ['不完全', '情報', 'ゲーム', 'で', 'ある'],\t 係り先: 24, 係り元: [22]\n",
            "# 24  形態素: ['ポーカー', 'の'],\t 係り先: 25, 係り元: [23]\n",
            "# 25  形態素: ['世界', 'トップクラス', 'の'],\t 係り先: 26, 係り元: [24]\n",
            "# 26  形態素: ['プレイヤー', 'も'],\t 係り先: 27, 係り元: [21, 25]\n",
            "# 27  形態素: ['破り', '、'],\t 係り先: 36, 係り元: [26]\n",
            "# 28  形態素: ['麻雀', 'で', 'は'],\t 係り先: 36, 係り元: []\n",
            "# 29  形態素: ['「', 'Microsoft', 'Suphx', '(', 'Super', 'Phoenix', ')」', 'が'],\t 係り先: 33, 係り元: []\n",
            "# 30  形態素: ['AI', 'として'],\t 係り先: 33, 係り元: []\n",
            "# 31  形態素: ['初めて'],\t 係り先: 33, 係り元: []\n",
            "# 32  形態素: ['十', '段', 'に'],\t 係り先: 33, 係り元: []\n",
            "# 33  形態素: ['到達', 'する', 'など', '、'],\t 係り先: 36, 係り元: [29, 30, 31, 32]\n",
            "# 34  形態素: ['時代', 'の'],\t 係り先: 35, 係り元: []\n",
            "# 35  形態素: ['最先端', '技術', 'と'],\t 係り先: 36, 係り元: [34]\n",
            "# 36  形態素: ['なっ', 'た', '。'],\t 係り先: -1, 係り元: [13, 27, 28, 33, 35]\n",
            "--- Sentence 0006 ---\n",
            "#  0  形態素: ['第', '２', '次', '人工', '知能', 'ブーム', 'で', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['人工', '知能', 'は'],\t 係り先: 3, 係り元: [0]\n",
            "#  2  形態素: ['機械', '学習', 'と'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['呼ば', 'れ', '、'],\t 係り先: 6, 係り元: [1, 2]\n",
            "#  4  形態素: ['以下', 'の', 'よう', 'な'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['もの', 'が'],\t 係り先: 6, 係り元: [4]\n",
            "#  6  形態素: ['ある', '。'],\t 係り先: -1, 係り元: [3, 5]\n",
            "--- Sentence 0007 ---\n",
            "#  0  形態素: ['一方', '、'],\t 係り先: 26, 係り元: []\n",
            "#  1  形態素: ['計算', '知能'],\t 係り先: 2, 係り元: []\n",
            "#  2  形態素: ['（', 'CI', '）', 'は'],\t 係り先: 12, 係り元: [1]\n",
            "#  3  形態素: ['開発', 'や'],\t 係り先: 4, 係り元: []\n",
            "#  4  形態素: ['学習', 'を'],\t 係り先: 5, 係り元: [3]\n",
            "#  5  形態素: ['繰り返す'],\t 係り先: 6, 係り元: [4]\n",
            "#  6  形態素: ['こと', 'を'],\t 係り先: 8, 係り元: [5]\n",
            "#  7  形態素: ['基本', 'と'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['し', 'て', 'いる'],\t 係り先: 9, 係り元: [6, 7]\n",
            "#  9  形態素: ['（', '例えば', '、'],\t 係り先: 12, 係り元: [8]\n",
            "# 10  形態素: ['パラメータ', '調整', '、'],\t 係り先: 12, 係り元: []\n",
            "# 11  形態素: ['コネクショニズム', 'の'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['システム', '）', '。'],\t 係り先: 21, 係り元: [2, 9, 10, 11]\n",
            "# 13  形態素: ['学習', 'は'],\t 係り先: 16, 係り元: []\n",
            "# 14  形態素: ['経験', 'に'],\t 係り先: 15, 係り元: []\n",
            "# 15  形態素: ['基づく'],\t 係り先: 16, 係り元: [14]\n",
            "# 16  形態素: ['手法', 'で', 'あり', '、'],\t 係り先: 21, 係り元: [13, 15]\n",
            "# 17  形態素: ['非', '記号', '的', 'AI', '、'],\t 係り先: 18, 係り元: []\n",
            "# 18  形態素: ['美しく', 'ない'],\t 係り先: 20, 係り元: [17]\n",
            "# 19  形態素: ['AI', '、'],\t 係り先: 20, 係り元: []\n",
            "# 20  形態素: ['ソフトコンピューティング', 'と'],\t 係り先: 21, 係り元: [18, 19]\n",
            "# 21  形態素: ['関係', 'し', 'て', 'いる', '。'],\t 係り先: 26, 係り元: [12, 16, 20]\n",
            "# 22  形態素: ['その'],\t 係り先: 23, 係り元: []\n",
            "# 23  形態素: ['手法', 'として', 'は', '、'],\t 係り先: 26, 係り元: [22]\n",
            "# 24  形態素: ['以下', 'の'],\t 係り先: 25, 係り元: []\n",
            "# 25  形態素: ['もの', 'が'],\t 係り先: 26, 係り元: [24]\n",
            "# 26  形態素: ['ある', '。'],\t 係り先: -1, 係り元: [0, 21, 23, 25]\n",
            "--- Sentence 0008 ---\n",
            "#  0  形態素: ['これら', 'を'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['統合', 'し', 'た'],\t 係り先: 2, 係り元: [0]\n",
            "#  2  形態素: ['知的', 'システム', 'を'],\t 係り先: 3, 係り元: [1]\n",
            "#  3  形態素: ['作る'],\t 係り先: 4, 係り元: [2]\n",
            "#  4  形態素: ['試み', 'も'],\t 係り先: 5, 係り元: [3]\n",
            "#  5  形態素: ['なさ', 'れ', 'て', 'いる', '。'],\t 係り先: 13, 係り元: [4]\n",
            "#  6  形態素: ['ACT', '-', 'R', 'で', 'は', '、'],\t 係り先: 13, 係り元: []\n",
            "#  7  形態素: ['エキスパート', 'の'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['推論', 'ルール', 'を', '、'],\t 係り先: 13, 係り元: [7]\n",
            "#  9  形態素: ['統計', '的', '学習', 'を'],\t 係り先: 13, 係り元: []\n",
            "# 10  形態素: ['元', 'に'],\t 係り先: 13, 係り元: []\n",
            "# 11  形態素: ['ニューラルネットワーク', 'や'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['生成', '規則', 'を通して'],\t 係り先: 13, 係り元: [11]\n",
            "# 13  形態素: ['生成', 'する', '。'],\t 係り先: -1, 係り元: [5, 6, 8, 9, 10, 12]\n",
            "--- Sentence 0009 ---\n",
            "#  0  形態素: ['第', '3', '次', '人工', '知能', 'ブーム', 'で', 'は', '、'],\t 係り先: 15, 係り元: []\n",
            "#  1  形態素: ['ディープラーニング', 'が'],\t 係り先: 2, 係り元: []\n",
            "#  2  形態素: ['画像', '認識', '、'],\t 係り先: 3, 係り元: [1]\n",
            "#  3  形態素: ['テキスト', '解析', '、'],\t 係り先: 4, 係り元: [2]\n",
            "#  4  形態素: ['音声', '認識', 'など'],\t 係り先: 6, 係り元: [3]\n",
            "#  5  形態素: ['様々', 'な'],\t 係り先: 6, 係り元: []\n",
            "#  6  形態素: ['領域', 'で'],\t 係り先: 9, 係り元: [4, 5]\n",
            "#  7  形態素: ['第', '2', '次', '人工', '知能', 'ブーム', 'の'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['人工', '知能', 'を'],\t 係り先: 9, 係り元: [7]\n",
            "#  9  形態素: ['上回る'],\t 係り先: 10, 係り元: [6, 8]\n",
            "# 10  形態素: ['精度', 'を'],\t 係り先: 11, 係り元: [9]\n",
            "# 11  形態素: ['出し', 'て', 'おり', '、'],\t 係り先: 15, 係り元: [10]\n",
            "# 12  形態素: ['ディープラーニング', 'の'],\t 係り先: 13, 係り元: []\n",
            "# 13  形態素: ['研究', 'が'],\t 係り先: 15, 係り元: [12]\n",
            "# 14  形態素: ['盛ん', 'に'],\t 係り先: 15, 係り元: []\n",
            "# 15  形態素: ['行わ', 'れ', 'て', 'いる', '。'],\t 係り先: 26, 係り元: [0, 11, 13, 14]\n",
            "# 16  形態素: ['最近', 'で', 'は', '、'],\t 係り先: 26, 係り元: []\n",
            "# 17  形態素: ['DQN', '、'],\t 係り先: 18, 係り元: []\n",
            "# 18  形態素: ['CNN', '、'],\t 係り先: 19, 係り元: [17]\n",
            "# 19  形態素: ['RNN', '、'],\t 係り先: 20, 係り元: [18]\n",
            "# 20  形態素: ['GAN', 'と'],\t 係り先: 22, 係り元: [19]\n",
            "# 21  形態素: ['様々', 'な'],\t 係り先: 22, 係り元: []\n",
            "# 22  形態素: ['ディープラーニング', 'の'],\t 係り先: 23, 係り元: [20, 21]\n",
            "# 23  形態素: ['派生', 'が'],\t 係り先: 24, 係り元: [22]\n",
            "# 24  形態素: ['で', 'て'],\t 係り先: 26, 係り元: [23]\n",
            "# 25  形態素: ['各', '分野', 'で'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['活躍', 'し', 'て', 'いる', '。'],\t 係り先: 42, 係り元: [15, 16, 24, 25]\n",
            "# 27  形態素: ['特に', '、'],\t 係り先: 42, 係り元: []\n",
            "# 28  形態素: ['GAN'],\t 係り先: 29, 係り元: []\n",
            "# 29  形態素: ['（', '敵対', '的', '生成', 'ネットワーク', '）', 'は', '、'],\t 係り先: 42, 係り元: [28]\n",
            "# 30  形態素: ['ディープラーニング', 'が'],\t 係り先: 35, 係り元: []\n",
            "# 31  形態素: ['認識', 'や'],\t 係り先: 32, 係り元: []\n",
            "# 32  形態素: ['予測', 'など', 'の'],\t 係り先: 33, 係り元: [31]\n",
            "# 33  形態素: ['分野', 'で'],\t 係り先: 35, 係り元: [32]\n",
            "# 34  形態素: ['成果', 'を'],\t 係り先: 35, 係り元: []\n",
            "# 35  形態素: ['だし', 'て', 'いる'],\t 係り先: 36, 係り元: [30, 33, 34]\n",
            "# 36  形態素: ['こと', 'に'],\t 係り先: 37, 係り元: [35]\n",
            "# 37  形態素: ['加え', 'て', '、'],\t 係り先: 42, 係り元: [36]\n",
            "# 38  形態素: ['画像', 'の'],\t 係り先: 39, 係り元: []\n",
            "# 39  形態素: ['生成', '技術', 'において'],\t 係り先: 42, 係り元: [38]\n",
            "# 40  形態素: ['大きな'],\t 係り先: 41, 係り元: []\n",
            "# 41  形態素: ['進化', 'を'],\t 係り先: 42, 係り元: [40]\n",
            "# 42  形態素: ['見せ', 'て', 'いる', '。'],\t 係り先: 55, 係り元: [26, 27, 29, 37, 39, 41]\n",
            "# 43  形態素: ['森', '正弥', 'は'],\t 係り先: 50, 係り元: []\n",
            "# 44  形態素: ['これら', 'の'],\t 係り先: 45, 係り元: []\n",
            "# 45  形態素: ['成果', 'を'],\t 係り先: 46, 係り元: [44]\n",
            "# 46  形態素: ['背景', 'に', '、'],\t 係り先: 50, 係り元: [45]\n",
            "# 47  形態素: ['従来', 'の'],\t 係り先: 48, 係り元: []\n",
            "# 48  形態素: ['人工', '知能', 'の'],\t 係り先: 49, 係り元: [47]\n",
            "# 49  形態素: ['応用', '分野', 'が'],\t 係り先: 50, 係り元: [48]\n",
            "# 50  形態素: ['広がっ', 'て', 'おり', '、'],\t 係り先: 55, 係り元: [43, 46, 49]\n",
            "# 51  形態素: ['Creative', 'AI', 'という'],\t 係り先: 52, 係り元: []\n",
            "# 52  形態素: ['コンテンツ', '生成', 'を'],\t 係り先: 53, 係り元: [51]\n",
            "# 53  形態素: ['行っ', 'て', 'いく'],\t 係り先: 54, 係り元: [52]\n",
            "# 54  形態素: ['応用', 'も'],\t 係り先: 55, 係り元: [53]\n",
            "# 55  形態素: ['始まっ', 'て', 'いる', 'と'],\t 係り先: 56, 係り元: [42, 50, 54]\n",
            "# 56  形態素: ['指摘', 'し', 'て', 'いる', '。'],\t 係り先: -1, 係り元: [55]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 42. 係り元と係り先の文節の表示\n",
        "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
        "\n",
        "> 41番が出来ていればforで探索してやるだけ！\n",
        "\n",
        "> 記号は指示があったからif内包で省いたものの省かなくても良い気がするな…"
      ],
      "metadata": {
        "id": "EkCDSwTMGCvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "\"\"\"\n",
        "# 確認\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:5]:\n",
        "    print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\"\"\"\n",
        "\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if c.dst == -1:\n",
        "      continue\n",
        "    kakari_moto_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs])\n",
        "    kakari_saki_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[c.dst].morphs])\n",
        "    print(f\"#{c.num:3g}  係り元 : {kakari_moto_text} \\t->\\t 係り先 : {kakari_saki_text}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G_OM9atUGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e1e6ec-f325-49a9-9cdf-03c99cc8772e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "--- Sentence 0001 ---\n",
            "#  0  係り元 : 人工知能 \t->\t 係り先 : 語\n",
            "#  1  係り元 : じんこうちのう \t->\t 係り先 : 語\n",
            "#  2  係り元 : AI \t->\t 係り先 : エーアイとは\n",
            "#  3  係り元 : エーアイとは \t->\t 係り先 : 語\n",
            "#  4  係り元 : 計算 \t->\t 係り先 : という\n",
            "#  5  係り元 : という \t->\t 係り先 : 道具を\n",
            "#  6  係り元 : 概念と \t->\t 係り先 : 道具を\n",
            "#  7  係り元 : コンピュータ \t->\t 係り先 : という\n",
            "#  8  係り元 : という \t->\t 係り先 : 道具を\n",
            "#  9  係り元 : 道具を \t->\t 係り先 : 用いて\n",
            "# 10  係り元 : 用いて \t->\t 係り先 : 研究する\n",
            "# 11  係り元 : 知能を \t->\t 係り先 : 研究する\n",
            "# 12  係り元 : 研究する \t->\t 係り先 : 計算機科学\n",
            "# 13  係り元 : 計算機科学 \t->\t 係り先 : の\n",
            "# 14  係り元 : の \t->\t 係り先 : 一分野を\n",
            "# 15  係り元 : 一分野を \t->\t 係り先 : 指す\n",
            "# 16  係り元 : 指す \t->\t 係り先 : 語\n",
            "# 17  係り元 : 語 \t->\t 係り先 : 研究分野とも\n",
            "# 18  係り元 : 言語の \t->\t 係り先 : 推論\n",
            "# 19  係り元 : 理解や \t->\t 係り先 : 推論\n",
            "# 20  係り元 : 推論 \t->\t 係り先 : 問題解決などの\n",
            "# 21  係り元 : 問題解決などの \t->\t 係り先 : 知的行動を\n",
            "# 22  係り元 : 知的行動を \t->\t 係り先 : 代わって\n",
            "# 23  係り元 : 人間に \t->\t 係り先 : 代わって\n",
            "# 24  係り元 : 代わって \t->\t 係り先 : 行わせる\n",
            "# 25  係り元 : コンピューターに \t->\t 係り先 : 行わせる\n",
            "# 26  係り元 : 行わせる \t->\t 係り先 : 技術または\n",
            "# 27  係り元 : 技術または \t->\t 係り先 : 研究分野とも\n",
            "# 28  係り元 : 計算機 \t->\t 係り先 : コンピュータによる\n",
            "# 29  係り元 : コンピュータによる \t->\t 係り先 : 情報処理システムの\n",
            "# 30  係り元 : 知的な \t->\t 係り先 : 情報処理システムの\n",
            "# 31  係り元 : 情報処理システムの \t->\t 係り先 : 実現に関する\n",
            "# 32  係り元 : 設計や \t->\t 係り先 : 実現に関する\n",
            "# 33  係り元 : 実現に関する \t->\t 係り先 : 研究分野とも\n",
            "# 34  係り元 : 研究分野とも \t->\t 係り先 : される\n",
            "--- Sentence 0002 ---\n",
            "#  0  係り元 : 日本大百科全書(ニッポニカ)』の \t->\t 係り先 : 解説で\n",
            "#  1  係り元 : 解説で \t->\t 係り先 : 述べている\n",
            "#  2  係り元 : 情報工学者通信工学者の \t->\t 係り先 : 佐藤理史は\n",
            "#  3  係り元 : 佐藤理史は \t->\t 係り先 : 述べている\n",
            "#  4  係り元 : 次のように \t->\t 係り先 : 述べている\n",
            "--- Sentence 0003 ---\n",
            "#  0  係り元 : 人間の \t->\t 係り先 : 知的能力を\n",
            "#  1  係り元 : 知的能力を \t->\t 係り先 : 実現する\n",
            "#  2  係り元 : コンピュータ上で \t->\t 係り先 : 実現する\n",
            "#  3  係り元 : 実現する \t->\t 係り先 : 技術ソフトウェアコンピュータシステム\n",
            "#  4  係り元 : 様々な \t->\t 係り先 : 技術ソフトウェアコンピュータシステム\n",
            "#  5  係り元 : 技術ソフトウェアコンピュータシステム \t->\t 係り先 : ある\n",
            "#  6  係り元 : 応用例は \t->\t 係り先 : ある\n",
            "#  7  係り元 : 自然言語処理 \t->\t 係り先 : 機械翻訳かな漢字変換構文解析等\n",
            "#  8  係り元 : 機械翻訳かな漢字変換構文解析等 \t->\t 係り先 : 専門家の\n",
            "#  9  係り元 : 専門家の \t->\t 係り先 : 推論判断を\n",
            "# 10  係り元 : 推論判断を \t->\t 係り先 : 模倣する\n",
            "# 11  係り元 : 模倣する \t->\t 係り先 : エキスパートシステム\n",
            "# 12  係り元 : エキスパートシステム \t->\t 係り先 : 画像認識等が\n",
            "# 13  係り元 : 画像データを \t->\t 係り先 : 解析して\n",
            "# 14  係り元 : 解析して \t->\t 係り先 : 検出抽出したりする\n",
            "# 15  係り元 : 特定の \t->\t 係り先 : パターンを\n",
            "# 16  係り元 : パターンを \t->\t 係り先 : 検出抽出したりする\n",
            "# 17  係り元 : 検出抽出したりする \t->\t 係り先 : 画像認識等が\n",
            "# 18  係り元 : 画像認識等が \t->\t 係り先 : ある\n",
            "# 19  係り元 : ある \t->\t 係り先 : 命名された\n",
            "# 20  係り元 : 1956年に \t->\t 係り先 : 命名された\n",
            "# 21  係り元 : ダートマス会議で \t->\t 係り先 : 命名された\n",
            "# 22  係り元 : ジョンマッカーシーにより \t->\t 係り先 : 命名された\n",
            "# 23  係り元 : 命名された \t->\t 係り先 : 使われている\n",
            "# 24  係り元 : 現在では \t->\t 係り先 : 使われている\n",
            "# 25  係り元 : 記号処理を \t->\t 係り先 : 用いた\n",
            "# 26  係り元 : 用いた \t->\t 係り先 : 知能の\n",
            "# 27  係り元 : 知能の \t->\t 係り先 : 記述を\n",
            "# 28  係り元 : 記述を \t->\t 係り先 : する\n",
            "# 29  係り元 : 主体と \t->\t 係り先 : する\n",
            "# 30  係り元 : する \t->\t 係り先 : 研究での\n",
            "# 31  係り元 : 情報処理や \t->\t 係り先 : 研究での\n",
            "# 32  係り元 : 研究での \t->\t 係り先 : アプローチという\n",
            "# 33  係り元 : アプローチという \t->\t 係り先 : 意味あいでも\n",
            "# 34  係り元 : 意味あいでも \t->\t 係り先 : 使われている\n",
            "# 35  係り元 : 使われている \t->\t 係り先 : ある\n",
            "# 36  係り元 : 家庭用電気機械器具の \t->\t 係り先 : 制御システムや\n",
            "# 37  係り元 : 制御システムや \t->\t 係り先 : 思考ルーチンも\n",
            "# 38  係り元 : ゲームソフトの \t->\t 係り先 : 思考ルーチンも\n",
            "# 39  係り元 : 思考ルーチンも \t->\t 係り先 : 呼ばれる\n",
            "# 40  係り元 : こう \t->\t 係り先 : 呼ばれる\n",
            "# 41  係り元 : 呼ばれる \t->\t 係り先 : ことも\n",
            "# 42  係り元 : ことも \t->\t 係り先 : ある\n",
            "--- Sentence 0004 ---\n",
            "#  0  係り元 : プログラミング言語による \t->\t 係り先 : という\n",
            "#  1  係り元 : という \t->\t 係り先 : カウンセラーを\n",
            "#  2  係り元 : カウンセラーを \t->\t 係り先 : 模倣した\n",
            "#  3  係り元 : 模倣した \t->\t 係り先 : プログラム\n",
            "#  4  係り元 : プログラム \t->\t 係り先 : 出されるが\n",
            "#  5  係り元 : 人工無脳が \t->\t 係り先 : 出されるが\n",
            "#  6  係り元 : しばしば \t->\t 係り先 : 出されるが\n",
            "#  7  係り元 : 引き合いに \t->\t 係り先 : 出されるが\n",
            "#  8  係り元 : 出されるが \t->\t 係り先 : 困難視されている\n",
            "#  9  係り元 : 計算機に \t->\t 係り先 : させようという\n",
            "# 10  係り元 : 人間の \t->\t 係り先 : 専門家の\n",
            "# 11  係り元 : 専門家の \t->\t 係り先 : 役割を\n",
            "# 12  係り元 : 役割を \t->\t 係り先 : させようという\n",
            "# 13  係り元 : させようという \t->\t 係り先 : エキスパートシステムと\n",
            "# 14  係り元 : エキスパートシステムと \t->\t 係り先 : 呼ばれる\n",
            "# 15  係り元 : 呼ばれる \t->\t 係り先 : 研究情報処理システムの\n",
            "# 16  係り元 : 研究情報処理システムの \t->\t 係り先 : 実現は\n",
            "# 17  係り元 : 実現は \t->\t 係り先 : 困難視されている\n",
            "# 18  係り元 : 人間が \t->\t 係り先 : 持つ\n",
            "# 19  係り元 : 暗黙に \t->\t 係り先 : 持つ\n",
            "# 20  係り元 : 持つ \t->\t 係り先 : 常識の\n",
            "# 21  係り元 : 常識の \t->\t 係り先 : 記述が\n",
            "# 22  係り元 : 記述が \t->\t 係り先 : なり\n",
            "# 23  係り元 : 問題と \t->\t 係り先 : なり\n",
            "# 24  係り元 : なり \t->\t 係り先 : 困難視されている\n",
            "# 25  係り元 : 実用への \t->\t 係り先 : 利用が\n",
            "# 26  係り元 : 利用が \t->\t 係り先 : 困難視されている\n",
            "# 27  係り元 : 困難視されている \t->\t 係り先 : ある\n",
            "# 28  係り元 : 人工的な \t->\t 係り先 : 知能の\n",
            "# 29  係り元 : 知能の \t->\t 係り先 : 実現への\n",
            "# 30  係り元 : 実現への \t->\t 係り先 : アプローチとしては\n",
            "# 31  係り元 : アプローチとしては \t->\t 係り先 : 知られているが\n",
            "# 32  係り元 : ファジィ理論や \t->\t 係り先 : ニューラルネットワークなどのような\n",
            "# 33  係り元 : ニューラルネットワークなどのような \t->\t 係り先 : アプローチも\n",
            "# 34  係り元 : アプローチも \t->\t 係り先 : 知られているが\n",
            "# 35  係り元 : 知られているが \t->\t 係り先 : ある\n",
            "# 36  係り元 : 従来の \t->\t 係り先 : 人工知能である\n",
            "# 37  係り元 : 人工知能である \t->\t 係り先 : (GoodOldFashionedAI)との\n",
            "# 38  係り元 : (GoodOldFashionedAI)との \t->\t 係り先 : 差は\n",
            "# 39  係り元 : 差は \t->\t 係り先 : ある\n",
            "# 40  係り元 : 記述の \t->\t 係り先 : 記号的明示性に\n",
            "# 41  係り元 : 記号的明示性に \t->\t 係り先 : ある\n",
            "# 42  係り元 : ある \t->\t 係り先 : 集めた\n",
            "# 43  係り元 : その後 \t->\t 係り先 : 集めた\n",
            "# 44  係り元 : サポートベクターマシンが \t->\t 係り先 : 集めた\n",
            "# 45  係り元 : 注目を \t->\t 係り先 : 集めた\n",
            "# 46  係り元 : 集めた \t->\t 係り先 : ある\n",
            "# 47  係り元 : また \t->\t 係り先 : ある\n",
            "# 48  係り元 : 自らの \t->\t 係り先 : 経験を\n",
            "# 49  係り元 : 経験を \t->\t 係り先 : 行う\n",
            "# 50  係り元 : 元に \t->\t 係り先 : 行う\n",
            "# 51  係り元 : 学習を \t->\t 係り先 : 行う\n",
            "# 52  係り元 : 行う \t->\t 係り先 : 強化学習という\n",
            "# 53  係り元 : 強化学習という \t->\t 係り先 : 手法も\n",
            "# 54  係り元 : 手法も \t->\t 係り先 : ある\n",
            "# 55  係り元 : ある \t->\t 係り先 : 作業である\n",
            "# 56  係り元 : この \t->\t 係り先 : 宇宙において\n",
            "# 57  係り元 : 宇宙において \t->\t 係り先 : 実装するという\n",
            "# 58  係り元 : 知性とは \t->\t 係り先 : 形質である\n",
            "# 59  係り元 : 最も \t->\t 係り先 : 強力な\n",
            "# 60  係り元 : 強力な \t->\t 係り先 : 形質である\n",
            "# 61  係り元 : 形質である \t->\t 係り先 : レイカーツワイルという\n",
            "# 62  係り元 : レイカーツワイルという \t->\t 係り先 : 言葉通り\n",
            "# 63  係り元 : 言葉通り \t->\t 係り先 : 実装するという\n",
            "# 64  係り元 : 知性を \t->\t 係り先 : 表現し\n",
            "# 65  係り元 : 機械的に \t->\t 係り先 : 表現し\n",
            "# 66  係り元 : 表現し \t->\t 係り先 : 実装するという\n",
            "# 67  係り元 : 実装するという \t->\t 係り先 : ことは\n",
            "# 68  係り元 : ことは \t->\t 係り先 : 作業である\n",
            "# 69  係り元 : 極めて \t->\t 係り先 : 重要な\n",
            "# 70  係り元 : 重要な \t->\t 係り先 : 作業である\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
        "\n",
        "> やるだけ"
      ],
      "metadata": {
        "id": "tki4hy54GCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:5]:\n",
        "    kakari_moto = c\n",
        "    kakari_saki = sentence[c.dst]\n",
        "    #print(\"名詞\" in [m.pos for m in kakari_moto.morphs])\n",
        "    #print(\"動詞\" in [m.pos for m in kakari_saki.morphs])\n",
        "    if (\"名詞\" in [m.pos for m in kakari_moto.morphs]) \\\n",
        "        and (\"動詞\" in [m.pos for m in kakari_saki.morphs]):\n",
        "      kakari_moto_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs])\n",
        "      kakari_saki_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[c.dst].morphs])\n",
        "      print(f\"#{c.num:3g}  係り元 : {kakari_moto_text} \\t->\\t 係り先 : {kakari_saki_text}\")\n"
      ],
      "metadata": {
        "id": "9o102UtqGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa99cf1-8c37-43fd-d8ac-6bd6df916ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "--- Sentence 0001 ---\n",
            "--- Sentence 0002 ---\n",
            "#  1  係り元 : 解説で \t->\t 係り先 : 述べている\n",
            "#  3  係り元 : 佐藤理史は \t->\t 係り先 : 述べている\n",
            "#  4  係り元 : 次のように \t->\t 係り先 : 述べている\n",
            "--- Sentence 0003 ---\n",
            "#  1  係り元 : 知的能力を \t->\t 係り先 : 実現する\n",
            "#  2  係り元 : コンピュータ上で \t->\t 係り先 : 実現する\n",
            "--- Sentence 0004 ---\n",
            "#  2  係り元 : カウンセラーを \t->\t 係り先 : 模倣した\n",
            "#  4  係り元 : プログラム \t->\t 係り先 : 出されるが\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44. 係り受け木の可視化\n",
        "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．\n",
        "\n",
        "> 旧問題文では次のようになっている\n",
        "\n",
        "    可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．\n",
        "    また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．\n",
        "\n",
        "以下、必要な準備"
      ],
      "metadata": {
        "id": "tZR-DWmxGCvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ライブラリの導入\n",
        "`GrapghViz` と `pydot` を導入する。`pydot`はpython3互換の`pydot-ng`というライブラリがあるのでそちらを使用。\n",
        "- `GraphViz` : DOT言語のグラフ情報から画像を作るツール。\n",
        "- `pydot-ng` : GraphVizを使うためのライブラリ。\n",
        "  - `pyparsing` : pydot-ngの依存で、DOT言語を解析するためのライブラリ。\n",
        "\n",
        "> - 全部pipでインストール出来る\n",
        "> - なんならインストールしようと思ったらGoogle Colabolatory標準でインストールされてた……。\n",
        "\n",
        "\n",
        "#### 係り受け木 is 何？？？\n",
        "単語の係り受け関係を有向グラフとして表現したもの…多分！\n",
        "\n",
        "> 例 : [滝沢カレンの理解不能な文章を言語解析してみた。| Qiita](https://qiita.com/naoyu822/items/9d7a83879c161573f63c)\n",
        "\n"
      ],
      "metadata": {
        "id": "DntCsTHXuSnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DOT言語の表現ステップ\n",
        "基本的には以下の3ステップで行う\n",
        "\n",
        "> 参考 : [【Python】Graphviz の使い方とキーワードマップを描くコード例【Windows】 | シラベルノート](https://srbrnote.work/archives/4205)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_gcQUPXiCFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ステップ\n",
        "1. キーワードリストを用意\n",
        "  - List形式で`[A, B]`という要素を与えると`A-B`にノードとエッジのあるグラフを表現している\n",
        "\n",
        "2. graphvizでDOT言語データを作成\n",
        "  - コレに関しては参考リンクのものをほぼパクってます…\n",
        "\n",
        "\n",
        "    # DOT言語ファイルのファイル名を決める\n",
        "    dot_filename = 'keyword_map'\n",
        "    # DOT言語ファイルと画像の保存フォルダを決める\n",
        "    datas_dir = r'***\\datas'\n",
        "    # ダイグラフ Digraph のインスタンスを作成\n",
        "    dot = graphviz.Digraph(\n",
        "        # name=None,\n",
        "        comment='Graphvizでキーワードマップを作図する',\n",
        "        filename=dot_filename, # DOT言語ファイルのファイル名 (これがグラフ画像のファイル名にも使われる)\n",
        "        directory=datas_dir, # DOT言語ファイルと画像を保存するフォルダ\n",
        "        format='png', # グラフの保存形式\n",
        "        engine='dot',\n",
        "        # encoding='utf-8',\n",
        "        # graph_attr=(('fontname', 'MS Gothic'), ),\n",
        "        # node_attr=(('fontname', 'MS Gothic'), ('shape', 'box'), ('color', 'blue'), ('style', 'rounded')),\n",
        "        # edge_attr=(('fontname', 'MS Gothic'), ('penwidth', '1.5'), ('color', 'gray')),\n",
        "        # body=None,\n",
        "        # strict=False,\n",
        "        )\n",
        "    # フォントの名前 (英語名を使えばOK)\n",
        "    fontname = 'MS Gothic'\n",
        "    # グラフ フォントを指定する\n",
        "    dot.attr('graph', fontname=fontname)\n",
        "    # ノード フォント、枠線の形、枠線の色、枠線のスタイルを指定する\n",
        "    dot.attr('node', fontname=fontname, shape='box', color='blue', style='rounded')\n",
        "    # エッジ フォント、矢印の太さ、矢印の色を指定する\n",
        "    dot.attr('edge', fontname=fontname, penwidth='1.5', color='gray')\n",
        "    # キーワードを追加する\n",
        "    for ks in keyword_list:\n",
        "        # キーワードの数が2個未満のときは、ノードとして追加。\n",
        "        if len(ks) < 2:\n",
        "            for k in ks:\n",
        "                dot.node(k)\n",
        "        else:\n",
        "            # そうでなければ、エッジに追加。\n",
        "            for i in range(len(ks) - 1):\n",
        "                dot.edge(ks[i], ks[i+1])\n",
        "\n",
        "3. DOT言語ファイルをグラフ画像にする\n",
        "\n",
        "\n",
        "    # 先の graphviz.Digraph() で filename と directory を指定したので、ここは引数なしでOK。\n",
        "    # もし、graphviz.Digraph() で filename と directory を指定していなければ、ここで指定すればOK。\n",
        "    dot.render(\n",
        "        # filename=None,\n",
        "        # directory=None,\n",
        "        # view=False,\n",
        "        # cleanup=False,\n",
        "        # format=None,\n",
        "        # renderer=None,\n",
        "        # formatter=None,\n",
        "        # quiet=False,\n",
        "        # quiet_view=False,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "rmrfytcAh9fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### グラフの有向化\n",
        "`directed=True` オプションを付与することで有向グラフ化できる。方向は[A,B]のとき、A->B方向に付けられる\n",
        "\n",
        "    graph = pydot.graph_from_edges(edges, directed=True)\n",
        "\n",
        "\n",
        "#### dotファイルをインライン表示する\n",
        "以下のコマンドでインライン表示出来る\n",
        "\n",
        "    import pydot_ng\n",
        "    from IPython.display import Image\n",
        "\n",
        "    graph = <グラフの作成>\n",
        "    Image(graph.create_png())\n",
        "\n",
        "> [参考 : 決定木とかグラフ構造（dotファイル）をJupyter上で表示する | 粉末@それは風のように (日記)](https://funmatu.wordpress.com/2017/04/25/%E6%B1%BA%E5%AE%9A%E6%9C%A8%E3%81%A8%E3%81%8B%E3%82%B0%E3%83%A9%E3%83%95%E6%A7%8B%E9%80%A0%EF%BC%88dot%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%EF%BC%89%E3%82%92jupyter%E4%B8%8A%E3%81%A7%E8%A1%A8%E7%A4%BA/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DE2s5d33h1oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydot-ng\n",
        "!pip install graphviz\n",
        "!pip show pydot-ng\n",
        "!pip show graphviz\n",
        "!dot -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GpDjkZjvXPi",
        "outputId": "be2f33b4-cadc-4fbc-a550-f3613b5b6688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydot-ng in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pydot-ng) (3.0.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Name: pydot-ng\n",
            "Version: 2.0.0\n",
            "Summary: Python interface to Graphviz's Dot\n",
            "Home-page: https://github.com/pydot/pydot-ng\n",
            "Author: Ero Carrera\n",
            "Author-email: ero@dkbza.org\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: pyparsing\n",
            "Required-by: \n",
            "Name: graphviz\n",
            "Version: 0.10.1\n",
            "Summary: Simple Python interface for Graphviz\n",
            "Home-page: https://github.com/xflr6/graphviz\n",
            "Author: Sebastian Bank\n",
            "Author-email: sebastian.bank@uni-leipzig.de\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "dot - graphviz version 2.40.1 (20161225.0304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "import pydot_ng as pydot\n",
        "# グラフ表示用\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "edges=[(1,2), (1,3), (1,4), (3,4)]\n",
        "g=pydot.graph_from_edges(edges)\n",
        "#g.write_jpeg('graph_from_edges_dot.jpg', prog='dot') \n",
        "Image(g.create_png())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "sT44T9YtAKXY",
        "outputId": "beb5f5c0-b695-46a4-b71d-ddeb26227187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44 回答\n",
        "\n",
        "- 他の方々の回答同様に、係り受けの組をstrとしてListを作り、`pydot.graph_from_edges()`に投げる形をとっているのだが、なぜかグラフ作成で\n",
        "  > `Warning: node 2, port AI unrecognized`\n",
        "\n",
        "  という警告が出て描画されない。\n",
        "  - ちなみに他のプログラムを流用したら同じ形式にもかかわらず警告は出なかった\n",
        "\n",
        "  > $[SentenceList[ChunkList[係り受けの組(Str)]]]$\n",
        "<br>という感じの構造\n",
        "\n",
        "- あとこれは散々既出のようだが日本語が表示されないらしい。\n",
        "\n",
        "> 少し先見たら木構造のグラフ作成するのはこの問題だけみたいなのでもうコレでいいということにします……\n",
        "  「グラフを作ること」で躓くのは問題の本質ではないので…"
      ],
      "metadata": {
        "id": "1Q1TeJ6M_ZY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "import pydot_ng as pydot\n",
        "import graphviz\n",
        "\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "\n",
        "C_edges = []\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(all_text[1:2]):\n",
        "  #print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if c.dst == -1:\n",
        "      continue\n",
        "    kakari_moto_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs])\n",
        "    kakari_saki_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[c.dst].morphs])\n",
        "    #print(f\"#{c.num:3g}  係り元 : {kakari_moto_text} \\t->\\t 係り先 : {kakari_saki_text}\")\n",
        "    C_edges.append([\n",
        "               f\"{c.num}:{kakari_moto_text}\",\n",
        "               f\"{sentence[c.dst].num}:{kakari_saki_text}\"\n",
        "              ])\n",
        "  print(C_edges)\n",
        "  print(type(C_edges))\n",
        "  n = pydot.Node('node')\n",
        "  n.fontname = 'IPAGothic'\n",
        "  C_Graph = pydot.graph_from_edges(C_edges, directed=True)\n",
        "  C_Graph.add_node(n)\n",
        "  C_Graph.write_png(\"C_G.png\")\n",
        "  Image(C_Graph.create_png())\n",
        "  \"\"\"\n",
        "  display_png(Image('./C_G.png'))\n",
        "  \"\"\"\n",
        "  C_edges = []\n"
      ],
      "metadata": {
        "id": "3NJui4rdGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "c6c5a0da-3852-40bc-9b23-74dbc1d481a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['0:人工知能', '17:語'], ['1:じんこうちのう', '17:語'], ['2:AI', '3:エーアイとは'], ['3:エーアイとは', '17:語'], ['4:計算', '5:という'], ['5:という', '9:道具を'], ['6:概念と', '9:道具を'], ['7:コンピュータ', '8:という'], ['8:という', '9:道具を'], ['9:道具を', '10:用いて'], ['10:用いて', '12:研究する'], ['11:知能を', '12:研究する'], ['12:研究する', '13:計算機科学'], ['13:計算機科学', '14:の'], ['14:の', '15:一分野を'], ['15:一分野を', '16:指す'], ['16:指す', '17:語'], ['17:語', '34:研究分野とも'], ['18:言語の', '20:推論'], ['19:理解や', '20:推論'], ['20:推論', '21:問題解決などの'], ['21:問題解決などの', '22:知的行動を'], ['22:知的行動を', '24:代わって'], ['23:人間に', '24:代わって'], ['24:代わって', '26:行わせる'], ['25:コンピューターに', '26:行わせる'], ['26:行わせる', '27:技術または'], ['27:技術または', '34:研究分野とも'], ['28:計算機', '29:コンピュータによる'], ['29:コンピュータによる', '31:情報処理システムの'], ['30:知的な', '31:情報処理システムの'], ['31:情報処理システムの', '33:実現に関する'], ['32:設計や', '33:実現に関する'], ['33:実現に関する', '34:研究分野とも'], ['34:研究分野とも', '35:される']]\n",
            "<class 'list'>\n",
            "Warning: node 2, port AI unrecognized\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-9ea07b03a1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mC_Graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0mC_Graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C_G.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_Graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m   \"\"\"\n\u001b[1;32m     99\u001b[0m   \u001b[0mdisplay_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./C_G.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 45. 動詞の格パターンの抽出\n",
        "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
        "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
        "- 述語に係る助詞を格とする\n",
        "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "    作り出す\tで は を\n",
        "\n",
        "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
        "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
        "\n",
        "> 実際やることは43番と同じなので、やるだけ\n",
        "\n",
        "> いい加減に覚えたいメモ\n",
        "\n",
        "#### pandasで特定の要素を抽出したい(それ以外を消したい)時\n",
        "- `df[df[\"column\"]==hoge]` で抽出できる\n",
        "  - `df[\"column\"]==hoge` でcolumn列の値がhogeの場所のみTrueとなるdfができるのでそれでマスクしている\n",
        "\n",
        "#### pandasで要素の頻度を見たい時\n",
        "- `df.value_counts(\"column\")` でcolumn列の要素を集計できる"
      ],
      "metadata": {
        "id": "KsTUgQ7qGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Doushi_Kaku = []\n",
        "\n",
        "for i,sentence in enumerate(all_text[:]):\n",
        "  #print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if (\"動詞\" in [m.pos for m in c.morphs]):  # 文節に動詞を含む場合のみ\n",
        "      kakari_moto_num = c.srcs\n",
        "      for c_joshi in [sentence[c_num] for c_num in kakari_moto_num]:\n",
        "        kakari_moto_joshi = \" \".join([m.surface for m in c_joshi.morphs if m.pos==\"助詞\"])\n",
        "      doushi_text = [m.base for m in c.morphs if m.pos==\"動詞\"][0]\n",
        "      #print(f\"#{c.num:3g} \\t 係り先 : {doushi_text} \\t 係り元 : {kakari_moto_joshi}\")\n",
        "      output_Doushi_Kaku.append(f\"{doushi_text}\\t{kakari_moto_joshi}\")\n",
        "\n",
        "with open(\"output_Doushi_Kaku.txt\",mode=\"w\") as out:\n",
        "  for line in output_Doushi_Kaku:\n",
        "    out.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "grwlaBb9GCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認用(UNIXコマンドじゃないけど許して～～～)\n",
        "import pandas as pd\n",
        "with open(\"output_Doushi_Kaku.txt\",mode=\"r\") as f:\n",
        "  df = pd.read_table(f, names=[\"動詞\",\"助詞\"])\n",
        "\n",
        "print(df)\n",
        "\n",
        "for V in [\"行う\",\"なる\",\"与える\"]:\n",
        "  df_count = df[df[\"動詞\"]==V]\n",
        "  print(f\"--- 動詞「{V}」にかかる助詞の出現頻度 ---\")\n",
        "  print(df_count.value_counts(\"助詞\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iKLikjHMYEv",
        "outputId": "866ed6fd-9c2f-4764-bfd5-f5110d8acf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      動詞     助詞\n",
            "0    用いる      を\n",
            "1     する      を\n",
            "2     指す      を\n",
            "3    代わる      に\n",
            "4     行う      に\n",
            "..   ...    ...\n",
            "703   思う  に と は\n",
            "704   つく      が\n",
            "705   する    か と\n",
            "706   つく      は\n",
            "707  答える    ね と\n",
            "\n",
            "[708 rows x 2 columns]\n",
            "--- 動詞「行う」にかかる助詞の出現頻度 ---\n",
            "助詞\n",
            "を        16\n",
            "に         4\n",
            "が         2\n",
            "から        1\n",
            "は         1\n",
            "をめぐって     1\n",
            "dtype: int64\n",
            "--- 動詞「なる」にかかる助詞の出現頻度 ---\n",
            "助詞\n",
            "に      11\n",
            "と       9\n",
            "も       2\n",
            "と は     1\n",
            "に は     1\n",
            "は       1\n",
            "dtype: int64\n",
            "--- 動詞「与える」にかかる助詞の出現頻度 ---\n",
            "助詞\n",
            "など に    1\n",
            "に       1\n",
            "を       1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 46. 動詞の格フレーム情報の抽出\n",
        "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
        "\n",
        "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
        "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "    作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n",
        "\n",
        "> さっき出来てなかったけど、動詞だけで係っている助詞がないパターンを処理するのを忘れていたのでifを追加した。\n",
        "\n",
        "> なんかすぐできたかと思ったけど、他の方の回答と見比べると、\n",
        "\n",
        "    他の方 >> 代わる   に を 知的行動を 人間に\n",
        "    自分   >> # 24 係り先:代わる 係り元助詞:に 係り元文節:人間に\n",
        "    他の方 >> 述べる   で に の は 解説で 佐藤理史は 次のように\n",
        "    自分   >> #  5 係り先:述べる 係り元助詞:の に 係り元文節:次のように\n",
        "\n",
        "といった具合で少し拾えてないところがある事に気づいた。ちょっと調整する必要がありそう。\n",
        "\n",
        "> chunkオブジェクトがイテラブルじゃないのでforで回してリストを作ってから結合するというすごく手間なことをしてしまった。"
      ],
      "metadata": {
        "id": "ZY2_LUvmGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "    \n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Doushi_Kaku = []\n",
        "\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  #print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if (\"動詞\" in [m.pos for m in c.morphs]):  # 文節に動詞を含む場合のみ\n",
        "      kakari_moto_num = c.srcs\n",
        "      kakari_moto_joshi_L = []\n",
        "      kakari_moto_chunk_L = []\n",
        "      for c_joshi in [sentence[c_n] for c_n in kakari_moto_num]:\n",
        "        kakari_moto_joshi_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos==\"助詞\"]))\n",
        "        kakari_moto_chunk_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos!=\"記号\"]))\n",
        "      #print(kakari_moto_joshi_L)\n",
        "      #print(kakari_moto_chunk_L)\n",
        "      kakari_moto_joshi = \" \".join(kakari_moto_joshi_L)\n",
        "      kakari_moto_chunk = \" \".join(kakari_moto_chunk_L)\n",
        "      if kakari_moto_joshi:\n",
        "        doushi_text = [m.base for m in c.morphs if m.pos==\"動詞\"][0]\n",
        "        print(f\"#{c.num:3g} \\t 係り先 : {doushi_text} \\t 係り元助詞 : {kakari_moto_joshi} \\t 係り元文節 : {kakari_moto_chunk}\")\n",
        "        output_Doushi_Kaku.append(f\"{doushi_text}\\t{kakari_moto_joshi}\\t{kakari_moto_chunk}\")\n",
        "\n",
        "\n",
        "with open(\"output_Doushi_Kou.txt\",mode=\"w\") as out:\n",
        "  for line in output_Doushi_Kaku:\n",
        "    out.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "e7oStrybGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd1947f-4ba9-4b0c-c5d9-c7aad13e98a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 10 \t 係り先 : 用いる \t 係り元助詞 : を \t 係り元文節 : 道具を\n",
            "# 12 \t 係り先 : する \t 係り元助詞 : て を \t 係り元文節 : 用いて 知能を\n",
            "# 16 \t 係り先 : 指す \t 係り元助詞 : を \t 係り元文節 : 一分野を\n",
            "# 24 \t 係り先 : 代わる \t 係り元助詞 : を に \t 係り元文節 : 知的行動を 人間に\n",
            "# 26 \t 係り先 : 行う \t 係り元助詞 : て に \t 係り元文節 : 代わって コンピューターに\n",
            "# 35 \t 係り先 : する \t 係り元助詞 : とも \t 係り元文節 : 研究分野とも\n",
            "#  5 \t 係り先 : 述べる \t 係り元助詞 : で は のに \t 係り元文節 : 解説で 佐藤理史は 次のように\n",
            "#  3 \t 係り先 : する \t 係り元助詞 : を で \t 係り元文節 : 知的能力を コンピュータ上で\n",
            "# 11 \t 係り先 : する \t 係り元助詞 : を \t 係り元文節 : 推論判断を\n",
            "# 14 \t 係り先 : する \t 係り元助詞 : を \t 係り元文節 : 画像データを\n",
            "# 17 \t 係り先 : する \t 係り元助詞 : て を \t 係り元文節 : 解析して パターンを\n",
            "# 19 \t 係り先 : ある \t 係り元助詞 :  は が \t 係り元文節 : 技術ソフトウェアコンピュータシステム 応用例は 画像認識等が\n",
            "# 23 \t 係り先 : する \t 係り元助詞 :  に で により \t 係り元文節 : ある 1956年に ダートマス会議で ジョンマッカーシーにより\n",
            "# 26 \t 係り先 : 用いる \t 係り元助詞 : を \t 係り元文節 : 記号処理を\n",
            "# 30 \t 係り先 : する \t 係り元助詞 : を と \t 係り元文節 : 記述を 主体と\n",
            "# 35 \t 係り先 : 使う \t 係り元助詞 :  では でも \t 係り元文節 : 命名された 現在では 意味あいでも\n",
            "# 41 \t 係り先 : 呼ぶ \t 係り元助詞 : も  \t 係り元文節 : 思考ルーチンも こう\n",
            "# 43 \t 係り先 : ある \t 係り元助詞 : て も \t 係り元文節 : 使われている ことも\n",
            "#  3 \t 係り先 : する \t 係り元助詞 : を \t 係り元文節 : カウンセラーを\n",
            "#  8 \t 係り先 : 出す \t 係り元助詞 :  が  に \t 係り元文節 : プログラム 人工無脳が しばしば 引き合いに\n",
            "# 13 \t 係り先 : する \t 係り元助詞 : に を \t 係り元文節 : 計算機に 役割を\n",
            "# 15 \t 係り先 : 呼ぶ \t 係り元助詞 : と \t 係り元文節 : エキスパートシステムと\n",
            "# 20 \t 係り先 : 持つ \t 係り元助詞 : が に \t 係り元文節 : 人間が 暗黙に\n",
            "# 24 \t 係り先 : なる \t 係り元助詞 : が と \t 係り元文節 : 記述が 問題と\n",
            "# 27 \t 係り先 : する \t 係り元助詞 : が は  が \t 係り元文節 : 出されるが 実現は なり 利用が\n",
            "# 35 \t 係り先 : 知る \t 係り元助詞 : としては も \t 係り元文節 : アプローチとしては アプローチも\n",
            "# 42 \t 係り先 : ある \t 係り元助詞 : て てが は に \t 係り元文節 : 困難視されている 知られているが 差は 記号的明示性に\n",
            "# 46 \t 係り先 : 集める \t 係り元助詞 :   が を \t 係り元文節 : ある その後 サポートベクターマシンが 注目を\n",
            "# 52 \t 係り先 : 行う \t 係り元助詞 : を に を \t 係り元文節 : 経験を 元に 学習を\n",
            "# 55 \t 係り先 : ある \t 係り元助詞 :   も \t 係り元文節 : 集めた また 手法も\n",
            "# 66 \t 係り先 : する \t 係り元助詞 : を に \t 係り元文節 : 知性を 機械的に\n",
            "# 67 \t 係り先 : する \t 係り元助詞 : において   \t 係り元文節 : 宇宙において 言葉通り 表現し\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認用(UNIXコマンドじゃないけど許して～～～)\n",
        "import pandas as pd\n",
        "with open(\"output_Doushi_Kou.txt\",mode=\"r\") as f:\n",
        "  df = pd.read_table(f, names=[\"動詞\",\"助詞\",\"文節\"])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VTMMQ4-USKOF",
        "outputId": "7d88cf3b-4137-4f9a-eb26-b2171eff3fa6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     動詞        助詞                               文節\n",
              "0   用いる         を                              道具を\n",
              "1    する       て を                          用いて 知能を\n",
              "2    指す         を                             一分野を\n",
              "3   代わる       を に                        知的行動を 人間に\n",
              "4    行う       て に                    代わって コンピューターに\n",
              "5    する        とも                           研究分野とも\n",
              "6   述べる    で は のに                  解説で 佐藤理史は 次のように\n",
              "7    する       を で                   知的能力を コンピュータ上で\n",
              "8    する         を                            推論判断を\n",
              "9    する         を                           画像データを\n",
              "10   する       て を                       解析して パターンを\n",
              "11   ある       は が   技術ソフトウェアコンピュータシステム 応用例は 画像認識等が\n",
              "12   する   に で により  ある 1956年に ダートマス会議で ジョンマッカーシーにより\n",
              "13  用いる         を                            記号処理を\n",
              "14   する       を と                          記述を 主体と\n",
              "15   使う     では でも                命名された 現在では 意味あいでも\n",
              "16   呼ぶ        も                        思考ルーチンも こう\n",
              "17   ある       て も                       使われている ことも\n",
              "18   する         を                          カウンセラーを\n",
              "19   出す      が  に           プログラム 人工無脳が しばしば 引き合いに\n",
              "20   する       に を                         計算機に 役割を\n",
              "21   呼ぶ         と                      エキスパートシステムと\n",
              "22   持つ       が に                          人間が 暗黙に\n",
              "23   なる       が と                          記述が 問題と\n",
              "24   する    が は  が                 出されるが 実現は なり 利用が\n",
              "25   知る    としては も                 アプローチとしては アプローチも\n",
              "26   ある  て てが は に      困難視されている 知られているが 差は 記号的明示性に\n",
              "27  集める       が を          ある その後 サポートベクターマシンが 注目を\n",
              "28   行う     を に を                       経験を 元に 学習を\n",
              "29   ある         も                       集めた また 手法も\n",
              "30   する       を に                         知性を 機械的に\n",
              "31   する    において                    宇宙において 言葉通り 表現し"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea3c7546-d513-404c-b207-913e31b19b9b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>動詞</th>\n",
              "      <th>助詞</th>\n",
              "      <th>文節</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>用いる</td>\n",
              "      <td>を</td>\n",
              "      <td>道具を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>する</td>\n",
              "      <td>て を</td>\n",
              "      <td>用いて 知能を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>指す</td>\n",
              "      <td>を</td>\n",
              "      <td>一分野を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>代わる</td>\n",
              "      <td>を に</td>\n",
              "      <td>知的行動を 人間に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>行う</td>\n",
              "      <td>て に</td>\n",
              "      <td>代わって コンピューターに</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>する</td>\n",
              "      <td>とも</td>\n",
              "      <td>研究分野とも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>述べる</td>\n",
              "      <td>で は のに</td>\n",
              "      <td>解説で 佐藤理史は 次のように</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>する</td>\n",
              "      <td>を で</td>\n",
              "      <td>知的能力を コンピュータ上で</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>する</td>\n",
              "      <td>を</td>\n",
              "      <td>推論判断を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>する</td>\n",
              "      <td>を</td>\n",
              "      <td>画像データを</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>する</td>\n",
              "      <td>て を</td>\n",
              "      <td>解析して パターンを</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ある</td>\n",
              "      <td>は が</td>\n",
              "      <td>技術ソフトウェアコンピュータシステム 応用例は 画像認識等が</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>する</td>\n",
              "      <td>に で により</td>\n",
              "      <td>ある 1956年に ダートマス会議で ジョンマッカーシーにより</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>用いる</td>\n",
              "      <td>を</td>\n",
              "      <td>記号処理を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>する</td>\n",
              "      <td>を と</td>\n",
              "      <td>記述を 主体と</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>使う</td>\n",
              "      <td>では でも</td>\n",
              "      <td>命名された 現在では 意味あいでも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>呼ぶ</td>\n",
              "      <td>も</td>\n",
              "      <td>思考ルーチンも こう</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ある</td>\n",
              "      <td>て も</td>\n",
              "      <td>使われている ことも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>する</td>\n",
              "      <td>を</td>\n",
              "      <td>カウンセラーを</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>出す</td>\n",
              "      <td>が  に</td>\n",
              "      <td>プログラム 人工無脳が しばしば 引き合いに</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>する</td>\n",
              "      <td>に を</td>\n",
              "      <td>計算機に 役割を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>呼ぶ</td>\n",
              "      <td>と</td>\n",
              "      <td>エキスパートシステムと</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>持つ</td>\n",
              "      <td>が に</td>\n",
              "      <td>人間が 暗黙に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>なる</td>\n",
              "      <td>が と</td>\n",
              "      <td>記述が 問題と</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>する</td>\n",
              "      <td>が は  が</td>\n",
              "      <td>出されるが 実現は なり 利用が</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>知る</td>\n",
              "      <td>としては も</td>\n",
              "      <td>アプローチとしては アプローチも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ある</td>\n",
              "      <td>て てが は に</td>\n",
              "      <td>困難視されている 知られているが 差は 記号的明示性に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>集める</td>\n",
              "      <td>が を</td>\n",
              "      <td>ある その後 サポートベクターマシンが 注目を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>行う</td>\n",
              "      <td>を に を</td>\n",
              "      <td>経験を 元に 学習を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ある</td>\n",
              "      <td>も</td>\n",
              "      <td>集めた また 手法も</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>する</td>\n",
              "      <td>を に</td>\n",
              "      <td>知性を 機械的に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>する</td>\n",
              "      <td>において</td>\n",
              "      <td>宇宙において 言葉通り 表現し</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea3c7546-d513-404c-b207-913e31b19b9b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea3c7546-d513-404c-b207-913e31b19b9b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea3c7546-d513-404c-b207-913e31b19b9b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 47. 機能動詞構文のマイニング\n",
        "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
        "\n",
        "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
        "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
        "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
        "\n",
        "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
        "\n",
        "    学習を行う\tに を\t元に 経験を\n",
        "\n",
        "> 過去にやった「連接」の探索と同じようにしようとしても、chunk/morphオブジェクトはイテラブルではないので詰む。\n",
        "  他の方の回答を見ると、「動詞の係り元chunkから助詞を探す」という風にしていた。かしこ～\n",
        "\n",
        "> 動詞→係り元取得→「サ変+を」の取得までは出来たけど、(「サ変+を」→係り先→動詞探索にしても)動詞から助詞を含む係り元を探して格納する時に「サ変+を」が入ってる文節を省くのが出来ねえ…単純に`not \"サ変接続\" in pos1`で省いて良いものか？\n",
        "\n",
        "少なくとも、上記の例では「行う」に「学習を(サ変+を)」が係っていて、それ以外の「元に/経験を」を出力しているが…\n",
        "\n",
        "    # 53 \t 係り先 : コンテンツ生成を行う \t 係り元助詞 : を \t 係り元文節 : コンテンツ生成を\n",
        "みたいな例もあるし「サ変+を」の文節を消していいとも思えん…\n",
        "でも二重になってるのも気に入らんからわからんわ～とりあえず消したものを回答としておきます。\n",
        "\n",
        "\n",
        "> `df.value_counts`に何も引数を与えずに呼び出すと全部の列を見て一致するものをユニークとしたDataFrameを返す。ので、不要な列を予めDropしておいてコレを叩けば良いっぽい。"
      ],
      "metadata": {
        "id": "0dFw4MJIGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "    \n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Kinou_Doushi = []\n",
        "\n",
        "for i,sentence in enumerate(all_text[:]):\n",
        "  for c in sentence:\n",
        "    kakari_moto_joshi_L = []\n",
        "    kakari_moto_chunk_L = []\n",
        "    kakari_moto_text = \"\"\n",
        "    c_sahen_wo = None\n",
        "    if (\"動詞\" in [m.pos for m in c.morphs]):  # 文節に動詞を含む場合のみ\n",
        "      for c_src in c.srcs:\n",
        "        if(\n",
        "            (\"サ変接続\" in [m.pos1 for m in sentence[c_src].morphs])\n",
        "            and (\"を\" in [m.surface for m in sentence[c_src].morphs])\n",
        "            ):\n",
        "          kakari_moto_text = (\n",
        "              \"\".join([m.surface for m in sentence[c_src].morphs if m.pos!=\"記号\"])  # 「サ変名詞+を」を含む係り元\n",
        "                + [m.base for m in c.morphs if m.pos==\"動詞\"][0]  # 最左の動詞のみ抜き出す\n",
        "              )\n",
        "          c_sahen_wo = c_src\n",
        "    if kakari_moto_text:\n",
        "      print(f\"--- Sentence {i:04g} ---\")\n",
        "      for c_joshi in [sentence[c_n] for c_n in c.srcs if not c_n==c_sahen_wo]:  # 「サ変+を」の文節と同一ではないものを対象に\n",
        "        kakari_moto_joshi_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos==\"助詞\"]))\n",
        "        kakari_moto_chunk_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos!=\"記号\"]))\n",
        "      kakari_moto_joshi = \" \".join(kakari_moto_joshi_L)\n",
        "      kakari_moto_chunk = \" \".join(kakari_moto_chunk_L)\n",
        "      print(f\"#{c.num:3g} \\t 係り先 : {kakari_moto_text} \\t 係り元助詞 : {kakari_moto_joshi} \\t 係り元文節 : {kakari_moto_chunk}\")\n",
        "      output_Kinou_Doushi.append(f\"{kakari_moto_text}\\t{kakari_moto_joshi}\\t{kakari_moto_chunk}\")\n",
        "\n",
        "\n",
        "with open(\"output_Kinou_Doushi.txt\",mode=\"w\") as out:\n",
        "  for line in output_Kinou_Doushi:\n",
        "    out.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "EgJZi8auGCvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d494bb-69d8-426e-f002-35eefe30d77d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0001 ---\n",
            "# 24 \t 係り先 : 知的行動を代わる \t 係り元助詞 : に \t 係り元文節 : 人間に\n",
            "--- Sentence 0003 ---\n",
            "# 11 \t 係り先 : 推論判断をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0003 ---\n",
            "# 26 \t 係り先 : 記号処理を用いる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0003 ---\n",
            "# 30 \t 係り先 : 記述をする \t 係り元助詞 : と \t 係り元文節 : 主体と\n",
            "--- Sentence 0004 ---\n",
            "# 46 \t 係り先 : 注目を集める \t 係り元助詞 :   が \t 係り元文節 : ある その後 サポートベクターマシンが\n",
            "--- Sentence 0004 ---\n",
            "# 52 \t 係り先 : 学習を行う \t 係り元助詞 : を に \t 係り元文節 : 経験を 元に\n",
            "--- Sentence 0005 ---\n",
            "# 10 \t 係り先 : 流行を超える \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0007 ---\n",
            "#  5 \t 係り先 : 学習を繰り返す \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0008 ---\n",
            "# 13 \t 係り先 : 統計的学習をする \t 係り元助詞 : て では を に を通して \t 係り元文節 : なされている ACT-Rでは 推論ルールを 元に 生成規則を通して\n",
            "--- Sentence 0009 ---\n",
            "# 42 \t 係り先 : 進化を見せる \t 係り元助詞 : て  は て において \t 係り元文節 : 活躍している 特に 敵対的生成ネットワークは 加えて 生成技術において\n",
            "--- Sentence 0009 ---\n",
            "# 53 \t 係り先 : コンテンツ生成を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0011 ---\n",
            "# 14 \t 係り先 : 機械式計算機をする \t 係り元助詞 :  は  \t 係り元文節 : 機械論 ブレーズ・パスカルは 1642年\n",
            "--- Sentence 0011 ---\n",
            "# 20 \t 係り先 : 開発を行う \t 係り元助詞 :  は \t 係り元文節 : 製作した エイダ・ラブレスは\n",
            "--- Sentence 0013 ---\n",
            "# 15 \t 係り先 : プログラミング言語をする \t 係り元助詞 :  は \t 係り元文節 : 作り出した 彼はまた\n",
            "--- Sentence 0013 ---\n",
            "# 18 \t 係り先 : テストをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0013 ---\n",
            "# 27 \t 係り先 : 来談者中心療法を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0015 ---\n",
            "# 19 \t 係り先 : プログラミング言語をする \t 係り元助詞 :   は \t 係り元文節 : 示した 示し アランカルメラウアーは\n",
            "--- Sentence 0019 ---\n",
            "# 16 \t 係り先 : バックギャモン専用コンピュータTDギャモンをする \t 係り元助詞 : に は \t 係り元文節 : 1992年に IBMは\n",
            "--- Sentence 0019 ---\n",
            "# 45 \t 係り先 : 投資全額を上回る \t 係り元助詞 :  が \t 係り元文節 : 使い コストが\n",
            "--- Sentence 0020 ---\n",
            "# 11 \t 係り先 : 意味付けをする \t 係り元助詞 : から に対して \t 係り元文節 : ここから 非構造化データに対して\n",
            "--- Sentence 0020 ---\n",
            "# 13 \t 係り先 : 処理を行う \t 係り元助詞 :  \t 係り元文節 : 適用し\n",
            "--- Sentence 0020 ---\n",
            "# 22 \t 係り先 : 知的処理を行う \t 係り元助詞 : に により に \t 係り元文節 : 同年に ティム・バーナーズリーにより Webに\n",
            "--- Sentence 0020 ---\n",
            "# 30 \t 係り先 : 意味をする \t 係り元助詞 : に \t 係り元文節 : データに\n",
            "--- Sentence 0020 ---\n",
            "# 33 \t 係り先 : 知的処理を行う \t 係り元助詞 : て に \t 係り元文節 : 付加して コンピュータに\n",
            "--- Sentence 0023 ---\n",
            "#  8 \t 係り先 : 研究を進める \t 係り元助詞 : て \t 係り元文節 : 費やして\n",
            "--- Sentence 0023 ---\n",
            "# 38 \t 係り先 : 命令をする \t 係り元助詞 :  で \t 係り元文節 : 直接 機構で\n",
            "--- Sentence 0024 ---\n",
            "# 48 \t 係り先 : 運転をする \t 係り元助詞 :  に \t 係り元文節 : 増やし 元に\n",
            "--- Sentence 0025 ---\n",
            "#  6 \t 係り先 : 特許をする \t 係り元助詞 : までに が \t 係り元文節 : 2018年までに 日本が\n",
            "--- Sentence 0026 ---\n",
            "#  9 \t 係り先 : 研究をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0026 ---\n",
            "# 25 \t 係り先 : 運転をする \t 係り元助詞 : て に \t 係り元文節 : 基づいて 柔軟に\n",
            "--- Sentence 0026 ---\n",
            "# 46 \t 係り先 : 注目を集める \t 係り元助詞 :  から は  \t 係り元文節 : 世界初であった ことから ファジィは 関わらず\n",
            "--- Sentence 0026 ---\n",
            "# 64 \t 係り先 : ニューロファジィ制御をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0026 ---\n",
            "# 80 \t 係り先 : 成功を受ける \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0026 ---\n",
            "# 84 \t 係り先 : 知的制御を用いる \t 係り元助詞 : て も \t 係り元文節 : 受けて 他社も\n",
            "--- Sentence 0027 ---\n",
            "# 37 \t 係り先 : 開発工数を抑える \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0027 ---\n",
            "# 57 \t 係り先 : 制御をする \t 係り元助詞 : から  \t 係り元文節 : 少なさから 多少\n",
            "--- Sentence 0028 ---\n",
            "# 13 \t 係り先 : 知的制御をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0028 ---\n",
            "# 46 \t 係り先 : 表現するをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0029 ---\n",
            "# 11 \t 係り先 : 進歩を担う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0032 ---\n",
            "# 23 \t 係り先 : 精度改善を果たす \t 係り元助詞 : に で が \t 係り元文節 : 2012年に 画像処理コンテストで チームが\n",
            "--- Sentence 0033 ---\n",
            "# 15 \t 係り先 : 専用プログラムを使う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0035 ---\n",
            "#  4 \t 係り先 : 研究を続ける \t 係り元助詞 : が て \t 係り元文節 : ジェフホーキンスが 向けて\n",
            "--- Sentence 0037 ---\n",
            "# 22 \t 係り先 : 行動型システムを用いる \t 係り元助詞 : て は は \t 係り元文節 : 登場している これは ものではなく\n",
            "--- Sentence 0039 ---\n",
            "#  6 \t 係り先 : 関連性を導き出す \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0039 ---\n",
            "# 20 \t 係り先 : ワンショット学習をする \t 係り元助詞 : が \t 係り元文節 : データが\n",
            "--- Sentence 0039 ---\n",
            "# 27 \t 係り先 : 認識能力を持つ \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0039 ---\n",
            "# 33 \t 係り先 : 記号接地問題(シンボルグラウンディング問題)をする \t 係り元助詞 :  には \t 係り元文節 : 開発 8月には\n",
            "--- Sentence 0040 ---\n",
            "# 25 \t 係り先 : 注目を集める \t 係り元助詞 : に \t 係り元文節 : 急速に\n",
            "--- Sentence 0040 ---\n",
            "# 33 \t 係り先 : 普及を受ける \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0040 ---\n",
            "# 62 \t 係り先 : 機械学習を組み合わせる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0043 ---\n",
            "# 21 \t 係り先 : 投資を行う \t 係り元助詞 : に で \t 係り元文節 : 全世界的に 民間企業主導で\n",
            "--- Sentence 0047 ---\n",
            "# 11 \t 係り先 : 探索を行う \t 係り元助詞 :  で \t 係り元文節 : 実装し 無報酬で\n",
            "--- Sentence 0048 ---\n",
            "#  9 \t 係り先 : 推論をする \t 係り元助詞 : て \t 係り元文節 : 経て\n",
            "--- Sentence 0051 ---\n",
            "# 25 \t 係り先 : 共同研究を始める \t 係り元助詞 : は とも \t 係り元文節 : Googleは マックスプランク研究所とも\n",
            "--- Sentence 0051 ---\n",
            "# 31 \t 係り先 : 研究を行う \t 係り元助詞 :  て \t 係り元文節 : いう 始めており\n",
            "--- Sentence 0052 ---\n",
            "# 11 \t 係り先 : 研究開発をする \t 係り元助詞 : では  で \t 係り元文節 : 中国では 立ち上げ 官民一体で\n",
            "--- Sentence 0052 ---\n",
            "# 34 \t 係り先 : 実験をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0052 ---\n",
            "# 41 \t 係り先 : 研究開発をする \t 係り元助詞 : で \t 係り元文節 : 日本で\n",
            "--- Sentence 0055 ---\n",
            "#  4 \t 係り先 : 投資をする \t 係り元助詞 : は までに \t 係り元文節 : 韓国は 2022年までに\n",
            "--- Sentence 0058 ---\n",
            "#  1 \t 係り先 : 深層学習をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0059 ---\n",
            "#  1 \t 係り先 : 脳シミュレーションを行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0060 ---\n",
            "#  9 \t 係り先 : 反乱を起こす \t 係り元助詞 : て に対して \t 係り元文節 : 於いて 人間に対して\n",
            "--- Sentence 0061 ---\n",
            "#  6 \t 係り先 : 弾圧を併せ持つ \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0061 ---\n",
            "# 67 \t 係り先 : 監視を行う \t 係り元助詞 :  まで に \t 係り元文節 : 推し進められ 歩行者まで 人工知能に\n",
            "--- Sentence 0061 ---\n",
            "# 90 \t 係り先 : 法的手続きを経る \t 係り元助詞 : を \t 係り元文節 : ウイグル族を\n",
            "--- Sentence 0061 ---\n",
            "#125 \t 係り先 : 監視社会化を恐れる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0061 ---\n",
            "#131 \t 係り先 : 監視カメラをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0061 ---\n",
            "#140 \t 係り先 : AI監視技術をする \t 係り元助詞 : は に \t 係り元文節 : 中国は 世界各国に\n",
            "--- Sentence 0062 ---\n",
            "# 26 \t 係り先 : 差別を認める \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0063 ---\n",
            "# 17 \t 係り先 : 研究をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0064 ---\n",
            "# 38 \t 係り先 : 展開を変える \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0064 ---\n",
            "# 40 \t 係り先 : 戦争をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0065 ---\n",
            "# 14 \t 係り先 : ファジィ制御をする \t 係り元助詞 : は \t 係り元文節 : AIプログラムは\n",
            "--- Sentence 0065 ---\n",
            "# 28 \t 係り先 : 判断を介す \t 係り元助詞 : から \t 係り元文節 : 観点から\n",
            "--- Sentence 0065 ---\n",
            "# 32 \t 係り先 : 開発禁止令を出す \t 係り元助詞 : に \t 係り元文節 : 2012年に\n",
            "--- Sentence 0067 ---\n",
            "# 65 \t 係り先 : 禁止を求める \t 係り元助詞 :  には が \t 係り元文節 : 記された 4月には ヒューマン・ライツ・ウォッチが\n",
            "--- Sentence 0067 ---\n",
            "# 79 \t 係り先 : 運用をめぐる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "# 11 \t 係り先 : 開発競争を行う \t 係り元助詞 : は をめぐって \t 係り元文節 : 米国中国ロシアは 軍事利用をめぐって\n",
            "--- Sentence 0068 ---\n",
            "# 25 \t 係り先 : 記録をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "# 39 \t 係り先 : 自律無人艇を使う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "# 42 \t 係り先 : 試験を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "#112 \t 係り先 : メイヴン計画を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "#140 \t 係り先 : 追及を受ける \t 係り元助詞 :  て では とともに と で \t 係り元文節 : なった 暴露されており 公聴会では とともに 拒否すると 整合性で\n",
            "--- Sentence 0068 ---\n",
            "#159 \t 係り先 : 共同研究をする \t 係り元助詞 : が \t 係り元文節 : Microsoftが\n",
            "--- Sentence 0068 ---\n",
            "#171 \t 係り先 : 監視国家をする \t 係り元助詞 : が によって \t 係り元文節 : 中国が AIによって\n",
            "--- Sentence 0068 ---\n",
            "#177 \t 係り先 : 無人攻撃機をする \t 係り元助詞 : で に \t 係り元文節 : 中東で 大量に\n",
            "--- Sentence 0070 ---\n",
            "# 15 \t 係り先 : 反科学反マイノリティ・地球温暖化懐疑論等をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0070 ---\n",
            "# 20 \t 係り先 : 解任をする \t 係り元助詞 :   て は \t 係り元文節 : 発表した しかし 含まれており Google社員らは\n",
            "--- Sentence 0070 ---\n",
            "# 31 \t 係り先 : 解散をする \t 係り元助詞 :   は が で \t 係り元文節 : 要請した 4月4日 Googleは 倫理委員会が 理由で\n",
            "--- Sentence 0071 ---\n",
            "# 16 \t 係り先 : 霊的存在を見いだす \t 係り元助詞 : に \t 係り元文節 : ものに\n",
            "--- Sentence 0072 ---\n",
            "#  9 \t 係り先 : 道)」をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0072 ---\n",
            "# 17 \t 係り先 : 実現をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0073 ---\n",
            "# 20 \t 係り先 : 話をする \t 係り元助詞 : は  \t 係り元文節 : 哲学者は みんな\n",
            "--- Sentence 0076 ---\n",
            "# 29 \t 係り先 : 意思疎通を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0078 ---\n",
            "# 48 \t 係り先 : 勘違いをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0082 ---\n",
            "#  6 \t 係り先 : 議論を行う \t 係り元助詞 : まで   \t 係り元文節 : これまで けっこう 長時間\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認用(UNIXコマンドじゃないけど許して～～～)\n",
        "import pandas as pd\n",
        "with open(\"output_Kinou_Doushi.txt\",mode=\"r\") as f:\n",
        "  df = pd.read_table(f, names=[\"動詞\",\"助詞\",\"文節\"])\n",
        "\n",
        "print(df)\n",
        "print(\"----- -----\")\n",
        "\n",
        "print(\"頻出述語(サ変+を+動詞)\")\n",
        "print(df.value_counts(\"動詞\"))\n",
        "\n",
        "print(\"----- -----\")\n",
        "\n",
        "print(\"頻出述語パターン\")\n",
        "print(df.drop(columns=\"文節\").value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buW2NidokQS9",
        "outputId": "7a4b3e76-78f8-4a5d-eccf-cb7a49ceb196"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          動詞    助詞                   文節\n",
            "0   知的行動を代わる     に                  人間に\n",
            "1    推論判断をする   NaN                  NaN\n",
            "2   記号処理を用いる   NaN                  NaN\n",
            "3      記述をする     と                  主体と\n",
            "4     注目を集める     が  ある その後 サポートベクターマシンが\n",
            "..       ...   ...                  ...\n",
            "91     実現をする   NaN                  NaN\n",
            "92      話をする    は              哲学者は みんな\n",
            "93   意思疎通を行う   NaN                  NaN\n",
            "94    勘違いをする   NaN                  NaN\n",
            "95     議論を行う  まで          これまで けっこう 長時間\n",
            "\n",
            "[96 rows x 3 columns]\n",
            "----- -----\n",
            "頻出述語(サ変+を+動詞)\n",
            "動詞\n",
            "注目を集める          3\n",
            "プログラミング言語をする    2\n",
            "知的処理を行う         2\n",
            "運転をする           2\n",
            "研究開発をする         2\n",
            "               ..\n",
            "意思疎通を行う         1\n",
            "意味付けをする         1\n",
            "意味をする           1\n",
            "弾圧を併せ持つ         1\n",
            "霊的存在を見いだす       1\n",
            "Length: 89, dtype: int64\n",
            "----- -----\n",
            "頻出述語パターン\n",
            "動詞                         助詞            \n",
            "AI監視技術をする                  は に               1\n",
            "バックギャモン専用コンピュータTDギャモンをする   に は               1\n",
            "知的処理を行う                    に により に           1\n",
            "知的制御を用いる                   て も               1\n",
            "知的行動を代わる                   に                 1\n",
            "研究を続ける                     が て               1\n",
            "研究を行う                       て                1\n",
            "研究を進める                     て                 1\n",
            "研究開発をする                    で                 1\n",
            "                           では  で             1\n",
            "禁止を求める                      には が             1\n",
            "精度改善を果たす                   に で が             1\n",
            "統計的学習をする                   て では を に を通して     1\n",
            "行動型システムを用いる                て は は             1\n",
            "解任をする                        て は             1\n",
            "解散をする                        は が で           1\n",
            "記号接地問題(シンボルグラウンディング問題)をする   には               1\n",
            "記述をする                      と                 1\n",
            "話をする                       は                 1\n",
            "議論を行う                      まで                1\n",
            "追及を受ける                      て では とともに と で    1\n",
            "進化を見せる                     て  は て において       1\n",
            "運転をする                       に                1\n",
            "                           て に               1\n",
            "開発を行う                       は                1\n",
            "開発禁止令を出す                   に                 1\n",
            "開発競争を行う                    は をめぐって           1\n",
            "知的処理を行う                    て に               1\n",
            "監視国家をする                    が によって            1\n",
            "監視を行う                       まで に             1\n",
            "意味をする                      に                 1\n",
            "ファジィ制御をする                  は                 1\n",
            "プログラミング言語をする                 は               1\n",
            "                            は                1\n",
            "ワンショット学習をする                が                 1\n",
            "共同研究をする                    が                 1\n",
            "共同研究を始める                   は とも              1\n",
            "判断を介す                      から                1\n",
            "制御をする                      から                1\n",
            "反乱を起こす                     て に対して            1\n",
            "命令をする                       で                1\n",
            "学習を行う                      を に               1\n",
            "意味付けをする                    から に対して           1\n",
            "特許をする                      までに が             1\n",
            "投資をする                      は までに             1\n",
            "投資を行う                      に で               1\n",
            "投資全額を上回る                    が                1\n",
            "探索を行う                       で                1\n",
            "推論をする                      て                 1\n",
            "機械式計算機をする                   は                1\n",
            "法的手続きを経る                   を                 1\n",
            "注目を集める                       が               1\n",
            "                            から は             1\n",
            "                           に                 1\n",
            "無人攻撃機をする                   で に               1\n",
            "霊的存在を見いだす                  に                 1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 48. 名詞から根へのパスの抽出\n",
        "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
        "- 各文節は（表層形の）形態素列で表現する\n",
        "- パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    ジョンマッカーシーは -> 作り出した\n",
        "    AIに関する -> 最初の -> 会議で -> 作り出した\n",
        "    最初の -> 会議で -> 作り出した\n",
        "    会議で -> 作り出した\n",
        "    人工知能という -> 用語を -> 作り出した\n",
        "    用語を -> 作り出した\n",
        "\n",
        "KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    ジョンマッカーシーは -> 作り出した\n",
        "    ＡＩに -> 関する -> 会議で -> 作り出した\n",
        "    会議で -> 作り出した\n",
        "    人工知能と -> いう -> 用語を -> 作り出した\n",
        "    用語を -> 作り出した\n",
        "\n",
        "> 係り受け木を作るより遥かにラク。順に辿って文節を詰んで結合して吐くだけ。\n",
        "  しかも正順なので1:1に対応されてる！！！逆順だと1:多対応なので……\n",
        "  今までのと比べて格段に楽な本当にやるだけの問題だ…順番がおかしいだろ！"
      ],
      "metadata": {
        "id": "ASM7F7ZMGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "    \n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Kinou_Doushi = []\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(all_text[:]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    text = []\n",
        "    kakari_next = c.dst\n",
        "    if c.dst == -1:\n",
        "      continue\n",
        "    text.append(\"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs]))\n",
        "    while kakari_next != -1:\n",
        "      text.append(\"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[kakari_next].morphs]))\n",
        "      kakari_next = sentence[kakari_next].dst\n",
        "\n",
        "    kakari_text_chain = \"\\t->\".join(text)\n",
        "    print(f\"#{c.num:3g} 開始文節 : {kakari_text_chain}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XfoDNc3IGCvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c1a70f-c416-4ac9-cc96-fe1d205403ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "--- Sentence 0001 ---\n",
            "#  0 開始文節 : 人工知能\t->語\t->研究分野とも\t->される\n",
            "#  1 開始文節 : じんこうちのう\t->語\t->研究分野とも\t->される\n",
            "#  2 開始文節 : AI\t->エーアイとは\t->語\t->研究分野とも\t->される\n",
            "#  3 開始文節 : エーアイとは\t->語\t->研究分野とも\t->される\n",
            "#  4 開始文節 : 計算\t->という\t->道具を\t->用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "#  5 開始文節 : という\t->道具を\t->用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "#  6 開始文節 : 概念と\t->道具を\t->用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "#  7 開始文節 : コンピュータ\t->という\t->道具を\t->用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "#  8 開始文節 : という\t->道具を\t->用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "#  9 開始文節 : 道具を\t->用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 10 開始文節 : 用いて\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 11 開始文節 : 知能を\t->研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 12 開始文節 : 研究する\t->計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 13 開始文節 : 計算機科学\t->の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 14 開始文節 : の\t->一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 15 開始文節 : 一分野を\t->指す\t->語\t->研究分野とも\t->される\n",
            "# 16 開始文節 : 指す\t->語\t->研究分野とも\t->される\n",
            "# 17 開始文節 : 語\t->研究分野とも\t->される\n",
            "# 18 開始文節 : 言語の\t->推論\t->問題解決などの\t->知的行動を\t->代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 19 開始文節 : 理解や\t->推論\t->問題解決などの\t->知的行動を\t->代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 20 開始文節 : 推論\t->問題解決などの\t->知的行動を\t->代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 21 開始文節 : 問題解決などの\t->知的行動を\t->代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 22 開始文節 : 知的行動を\t->代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 23 開始文節 : 人間に\t->代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 24 開始文節 : 代わって\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 25 開始文節 : コンピューターに\t->行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 26 開始文節 : 行わせる\t->技術または\t->研究分野とも\t->される\n",
            "# 27 開始文節 : 技術または\t->研究分野とも\t->される\n",
            "# 28 開始文節 : 計算機\t->コンピュータによる\t->情報処理システムの\t->実現に関する\t->研究分野とも\t->される\n",
            "# 29 開始文節 : コンピュータによる\t->情報処理システムの\t->実現に関する\t->研究分野とも\t->される\n",
            "# 30 開始文節 : 知的な\t->情報処理システムの\t->実現に関する\t->研究分野とも\t->される\n",
            "# 31 開始文節 : 情報処理システムの\t->実現に関する\t->研究分野とも\t->される\n",
            "# 32 開始文節 : 設計や\t->実現に関する\t->研究分野とも\t->される\n",
            "# 33 開始文節 : 実現に関する\t->研究分野とも\t->される\n",
            "# 34 開始文節 : 研究分野とも\t->される\n",
            "--- Sentence 0002 ---\n",
            "#  0 開始文節 : 日本大百科全書(ニッポニカ)』の\t->解説で\t->述べている\n",
            "#  1 開始文節 : 解説で\t->述べている\n",
            "#  2 開始文節 : 情報工学者通信工学者の\t->佐藤理史は\t->述べている\n",
            "#  3 開始文節 : 佐藤理史は\t->述べている\n",
            "#  4 開始文節 : 次のように\t->述べている\n",
            "--- Sentence 0003 ---\n",
            "#  0 開始文節 : 人間の\t->知的能力を\t->実現する\t->技術ソフトウェアコンピュータシステム\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  1 開始文節 : 知的能力を\t->実現する\t->技術ソフトウェアコンピュータシステム\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  2 開始文節 : コンピュータ上で\t->実現する\t->技術ソフトウェアコンピュータシステム\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  3 開始文節 : 実現する\t->技術ソフトウェアコンピュータシステム\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  4 開始文節 : 様々な\t->技術ソフトウェアコンピュータシステム\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  5 開始文節 : 技術ソフトウェアコンピュータシステム\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  6 開始文節 : 応用例は\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  7 開始文節 : 自然言語処理\t->機械翻訳かな漢字変換構文解析等\t->専門家の\t->推論判断を\t->模倣する\t->エキスパートシステム\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  8 開始文節 : 機械翻訳かな漢字変換構文解析等\t->専門家の\t->推論判断を\t->模倣する\t->エキスパートシステム\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "#  9 開始文節 : 専門家の\t->推論判断を\t->模倣する\t->エキスパートシステム\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 10 開始文節 : 推論判断を\t->模倣する\t->エキスパートシステム\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 11 開始文節 : 模倣する\t->エキスパートシステム\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 12 開始文節 : エキスパートシステム\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 13 開始文節 : 画像データを\t->解析して\t->検出抽出したりする\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 14 開始文節 : 解析して\t->検出抽出したりする\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 15 開始文節 : 特定の\t->パターンを\t->検出抽出したりする\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 16 開始文節 : パターンを\t->検出抽出したりする\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 17 開始文節 : 検出抽出したりする\t->画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 18 開始文節 : 画像認識等が\t->ある\t->命名された\t->使われている\t->ある\n",
            "# 19 開始文節 : ある\t->命名された\t->使われている\t->ある\n",
            "# 20 開始文節 : 1956年に\t->命名された\t->使われている\t->ある\n",
            "# 21 開始文節 : ダートマス会議で\t->命名された\t->使われている\t->ある\n",
            "# 22 開始文節 : ジョンマッカーシーにより\t->命名された\t->使われている\t->ある\n",
            "# 23 開始文節 : 命名された\t->使われている\t->ある\n",
            "# 24 開始文節 : 現在では\t->使われている\t->ある\n",
            "# 25 開始文節 : 記号処理を\t->用いた\t->知能の\t->記述を\t->する\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 26 開始文節 : 用いた\t->知能の\t->記述を\t->する\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 27 開始文節 : 知能の\t->記述を\t->する\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 28 開始文節 : 記述を\t->する\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 29 開始文節 : 主体と\t->する\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 30 開始文節 : する\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 31 開始文節 : 情報処理や\t->研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 32 開始文節 : 研究での\t->アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 33 開始文節 : アプローチという\t->意味あいでも\t->使われている\t->ある\n",
            "# 34 開始文節 : 意味あいでも\t->使われている\t->ある\n",
            "# 35 開始文節 : 使われている\t->ある\n",
            "# 36 開始文節 : 家庭用電気機械器具の\t->制御システムや\t->思考ルーチンも\t->呼ばれる\t->ことも\t->ある\n",
            "# 37 開始文節 : 制御システムや\t->思考ルーチンも\t->呼ばれる\t->ことも\t->ある\n",
            "# 38 開始文節 : ゲームソフトの\t->思考ルーチンも\t->呼ばれる\t->ことも\t->ある\n",
            "# 39 開始文節 : 思考ルーチンも\t->呼ばれる\t->ことも\t->ある\n",
            "# 40 開始文節 : こう\t->呼ばれる\t->ことも\t->ある\n",
            "# 41 開始文節 : 呼ばれる\t->ことも\t->ある\n",
            "# 42 開始文節 : ことも\t->ある\n",
            "--- Sentence 0004 ---\n",
            "#  0 開始文節 : プログラミング言語による\t->という\t->カウンセラーを\t->模倣した\t->プログラム\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  1 開始文節 : という\t->カウンセラーを\t->模倣した\t->プログラム\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  2 開始文節 : カウンセラーを\t->模倣した\t->プログラム\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  3 開始文節 : 模倣した\t->プログラム\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  4 開始文節 : プログラム\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  5 開始文節 : 人工無脳が\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  6 開始文節 : しばしば\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  7 開始文節 : 引き合いに\t->出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  8 開始文節 : 出されるが\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "#  9 開始文節 : 計算機に\t->させようという\t->エキスパートシステムと\t->呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 10 開始文節 : 人間の\t->専門家の\t->役割を\t->させようという\t->エキスパートシステムと\t->呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 11 開始文節 : 専門家の\t->役割を\t->させようという\t->エキスパートシステムと\t->呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 12 開始文節 : 役割を\t->させようという\t->エキスパートシステムと\t->呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 13 開始文節 : させようという\t->エキスパートシステムと\t->呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 14 開始文節 : エキスパートシステムと\t->呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 15 開始文節 : 呼ばれる\t->研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 16 開始文節 : 研究情報処理システムの\t->実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 17 開始文節 : 実現は\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 18 開始文節 : 人間が\t->持つ\t->常識の\t->記述が\t->なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 19 開始文節 : 暗黙に\t->持つ\t->常識の\t->記述が\t->なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 20 開始文節 : 持つ\t->常識の\t->記述が\t->なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 21 開始文節 : 常識の\t->記述が\t->なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 22 開始文節 : 記述が\t->なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 23 開始文節 : 問題と\t->なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 24 開始文節 : なり\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 25 開始文節 : 実用への\t->利用が\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 26 開始文節 : 利用が\t->困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 27 開始文節 : 困難視されている\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 28 開始文節 : 人工的な\t->知能の\t->実現への\t->アプローチとしては\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 29 開始文節 : 知能の\t->実現への\t->アプローチとしては\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 30 開始文節 : 実現への\t->アプローチとしては\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 31 開始文節 : アプローチとしては\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 32 開始文節 : ファジィ理論や\t->ニューラルネットワークなどのような\t->アプローチも\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 33 開始文節 : ニューラルネットワークなどのような\t->アプローチも\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 34 開始文節 : アプローチも\t->知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 35 開始文節 : 知られているが\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 36 開始文節 : 従来の\t->人工知能である\t->(GoodOldFashionedAI)との\t->差は\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 37 開始文節 : 人工知能である\t->(GoodOldFashionedAI)との\t->差は\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 38 開始文節 : (GoodOldFashionedAI)との\t->差は\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 39 開始文節 : 差は\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 40 開始文節 : 記述の\t->記号的明示性に\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 41 開始文節 : 記号的明示性に\t->ある\t->集めた\t->ある\t->作業である\n",
            "# 42 開始文節 : ある\t->集めた\t->ある\t->作業である\n",
            "# 43 開始文節 : その後\t->集めた\t->ある\t->作業である\n",
            "# 44 開始文節 : サポートベクターマシンが\t->集めた\t->ある\t->作業である\n",
            "# 45 開始文節 : 注目を\t->集めた\t->ある\t->作業である\n",
            "# 46 開始文節 : 集めた\t->ある\t->作業である\n",
            "# 47 開始文節 : また\t->ある\t->作業である\n",
            "# 48 開始文節 : 自らの\t->経験を\t->行う\t->強化学習という\t->手法も\t->ある\t->作業である\n",
            "# 49 開始文節 : 経験を\t->行う\t->強化学習という\t->手法も\t->ある\t->作業である\n",
            "# 50 開始文節 : 元に\t->行う\t->強化学習という\t->手法も\t->ある\t->作業である\n",
            "# 51 開始文節 : 学習を\t->行う\t->強化学習という\t->手法も\t->ある\t->作業である\n",
            "# 52 開始文節 : 行う\t->強化学習という\t->手法も\t->ある\t->作業である\n",
            "# 53 開始文節 : 強化学習という\t->手法も\t->ある\t->作業である\n",
            "# 54 開始文節 : 手法も\t->ある\t->作業である\n",
            "# 55 開始文節 : ある\t->作業である\n",
            "# 56 開始文節 : この\t->宇宙において\t->実装するという\t->ことは\t->作業である\n",
            "# 57 開始文節 : 宇宙において\t->実装するという\t->ことは\t->作業である\n",
            "# 58 開始文節 : 知性とは\t->形質である\t->レイカーツワイルという\t->言葉通り\t->実装するという\t->ことは\t->作業である\n",
            "# 59 開始文節 : 最も\t->強力な\t->形質である\t->レイカーツワイルという\t->言葉通り\t->実装するという\t->ことは\t->作業である\n",
            "# 60 開始文節 : 強力な\t->形質である\t->レイカーツワイルという\t->言葉通り\t->実装するという\t->ことは\t->作業である\n",
            "# 61 開始文節 : 形質である\t->レイカーツワイルという\t->言葉通り\t->実装するという\t->ことは\t->作業である\n",
            "# 62 開始文節 : レイカーツワイルという\t->言葉通り\t->実装するという\t->ことは\t->作業である\n",
            "# 63 開始文節 : 言葉通り\t->実装するという\t->ことは\t->作業である\n",
            "# 64 開始文節 : 知性を\t->表現し\t->実装するという\t->ことは\t->作業である\n",
            "# 65 開始文節 : 機械的に\t->表現し\t->実装するという\t->ことは\t->作業である\n",
            "# 66 開始文節 : 表現し\t->実装するという\t->ことは\t->作業である\n",
            "# 67 開始文節 : 実装するという\t->ことは\t->作業である\n",
            "# 68 開始文節 : ことは\t->作業である\n",
            "# 69 開始文節 : 極めて\t->重要な\t->作業である\n",
            "# 70 開始文節 : 重要な\t->作業である\n",
            "--- Sentence 0005 ---\n",
            "#  0 開始文節 : 2006年の\t->ディープラーニング\t->登場と\t->登場により\t->行った\t->なった\n",
            "#  1 開始文節 : ディープラーニング\t->登場と\t->登場により\t->行った\t->なった\n",
            "#  2 開始文節 : 深層学習の\t->登場と\t->登場により\t->行った\t->なった\n",
            "#  3 開始文節 : 登場と\t->登場により\t->行った\t->なった\n",
            "#  4 開始文節 : 2010年代\t->以降の\t->ビッグデータの\t->登場により\t->行った\t->なった\n",
            "#  5 開始文節 : 以降の\t->ビッグデータの\t->登場により\t->行った\t->なった\n",
            "#  6 開始文節 : ビッグデータの\t->登場により\t->行った\t->なった\n",
            "#  7 開始文節 : 登場により\t->行った\t->なった\n",
            "#  8 開始文節 : 一過性の\t->流行を\t->超えて\t->浸透して\t->行った\t->なった\n",
            "#  9 開始文節 : 流行を\t->超えて\t->浸透して\t->行った\t->なった\n",
            "# 10 開始文節 : 超えて\t->浸透して\t->行った\t->なった\n",
            "# 11 開始文節 : 社会に\t->浸透して\t->行った\t->なった\n",
            "# 12 開始文節 : 浸透して\t->行った\t->なった\n",
            "# 13 開始文節 : 行った\t->なった\n",
            "# 14 開始文節 : 2016年から\t->2017年にかけて\t->導入した\t->AIが\t->完全情報ゲームである\t->囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 15 開始文節 : 2017年にかけて\t->導入した\t->AIが\t->完全情報ゲームである\t->囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 16 開始文節 : ディープラーニングを\t->導入した\t->AIが\t->完全情報ゲームである\t->囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 17 開始文節 : 導入した\t->AIが\t->完全情報ゲームである\t->囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 18 開始文節 : AIが\t->完全情報ゲームである\t->囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 19 開始文節 : 完全情報ゲームである\t->囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 20 開始文節 : 囲碁などの\t->トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 21 開始文節 : トップ棋士\t->プレイヤーも\t->破り\t->なった\n",
            "# 22 開始文節 : さらに\t->不完全情報ゲームである\t->ポーカーの\t->世界トップクラスの\t->プレイヤーも\t->破り\t->なった\n",
            "# 23 開始文節 : 不完全情報ゲームである\t->ポーカーの\t->世界トップクラスの\t->プレイヤーも\t->破り\t->なった\n",
            "# 24 開始文節 : ポーカーの\t->世界トップクラスの\t->プレイヤーも\t->破り\t->なった\n",
            "# 25 開始文節 : 世界トップクラスの\t->プレイヤーも\t->破り\t->なった\n",
            "# 26 開始文節 : プレイヤーも\t->破り\t->なった\n",
            "# 27 開始文節 : 破り\t->なった\n",
            "# 28 開始文節 : 麻雀では\t->なった\n",
            "# 29 開始文節 : MicrosoftSuphx(SuperPhoenix)」が\t->到達するなど\t->なった\n",
            "# 30 開始文節 : AIとして\t->到達するなど\t->なった\n",
            "# 31 開始文節 : 初めて\t->到達するなど\t->なった\n",
            "# 32 開始文節 : 十段に\t->到達するなど\t->なった\n",
            "# 33 開始文節 : 到達するなど\t->なった\n",
            "# 34 開始文節 : 時代の\t->最先端技術と\t->なった\n",
            "# 35 開始文節 : 最先端技術と\t->なった\n",
            "--- Sentence 0006 ---\n",
            "#  0 開始文節 : 第２次人工知能ブームでの\t->人工知能は\t->呼ばれ\t->ある\n",
            "#  1 開始文節 : 人工知能は\t->呼ばれ\t->ある\n",
            "#  2 開始文節 : 機械学習と\t->呼ばれ\t->ある\n",
            "#  3 開始文節 : 呼ばれ\t->ある\n",
            "#  4 開始文節 : 以下のような\t->ものが\t->ある\n",
            "#  5 開始文節 : ものが\t->ある\n",
            "--- Sentence 0007 ---\n",
            "#  0 開始文節 : 一方\t->ある\n",
            "#  1 開始文節 : 計算知能\t->CIは\t->システム\t->関係している\t->ある\n",
            "#  2 開始文節 : CIは\t->システム\t->関係している\t->ある\n",
            "#  3 開始文節 : 開発や\t->学習を\t->繰り返す\t->ことを\t->している\t->例えば\t->システム\t->関係している\t->ある\n",
            "#  4 開始文節 : 学習を\t->繰り返す\t->ことを\t->している\t->例えば\t->システム\t->関係している\t->ある\n",
            "#  5 開始文節 : 繰り返す\t->ことを\t->している\t->例えば\t->システム\t->関係している\t->ある\n",
            "#  6 開始文節 : ことを\t->している\t->例えば\t->システム\t->関係している\t->ある\n",
            "#  7 開始文節 : 基本と\t->している\t->例えば\t->システム\t->関係している\t->ある\n",
            "#  8 開始文節 : している\t->例えば\t->システム\t->関係している\t->ある\n",
            "#  9 開始文節 : 例えば\t->システム\t->関係している\t->ある\n",
            "# 10 開始文節 : パラメータ調整\t->システム\t->関係している\t->ある\n",
            "# 11 開始文節 : コネクショニズムの\t->システム\t->関係している\t->ある\n",
            "# 12 開始文節 : システム\t->関係している\t->ある\n",
            "# 13 開始文節 : 学習は\t->手法であり\t->関係している\t->ある\n",
            "# 14 開始文節 : 経験に\t->基づく\t->手法であり\t->関係している\t->ある\n",
            "# 15 開始文節 : 基づく\t->手法であり\t->関係している\t->ある\n",
            "# 16 開始文節 : 手法であり\t->関係している\t->ある\n",
            "# 17 開始文節 : 非記号的AI\t->美しくない\t->ソフトコンピューティングと\t->関係している\t->ある\n",
            "# 18 開始文節 : 美しくない\t->ソフトコンピューティングと\t->関係している\t->ある\n",
            "# 19 開始文節 : AI\t->ソフトコンピューティングと\t->関係している\t->ある\n",
            "# 20 開始文節 : ソフトコンピューティングと\t->関係している\t->ある\n",
            "# 21 開始文節 : 関係している\t->ある\n",
            "# 22 開始文節 : その\t->手法としては\t->ある\n",
            "# 23 開始文節 : 手法としては\t->ある\n",
            "# 24 開始文節 : 以下の\t->ものが\t->ある\n",
            "# 25 開始文節 : ものが\t->ある\n",
            "--- Sentence 0008 ---\n",
            "#  0 開始文節 : これらを\t->統合した\t->知的システムを\t->作る\t->試みも\t->なされている\t->生成する\n",
            "#  1 開始文節 : 統合した\t->知的システムを\t->作る\t->試みも\t->なされている\t->生成する\n",
            "#  2 開始文節 : 知的システムを\t->作る\t->試みも\t->なされている\t->生成する\n",
            "#  3 開始文節 : 作る\t->試みも\t->なされている\t->生成する\n",
            "#  4 開始文節 : 試みも\t->なされている\t->生成する\n",
            "#  5 開始文節 : なされている\t->生成する\n",
            "#  6 開始文節 : ACT-Rでは\t->生成する\n",
            "#  7 開始文節 : エキスパートの\t->推論ルールを\t->生成する\n",
            "#  8 開始文節 : 推論ルールを\t->生成する\n",
            "#  9 開始文節 : 統計的学習を\t->生成する\n",
            "# 10 開始文節 : 元に\t->生成する\n",
            "# 11 開始文節 : ニューラルネットワークや\t->生成規則を通して\t->生成する\n",
            "# 12 開始文節 : 生成規則を通して\t->生成する\n",
            "--- Sentence 0009 ---\n",
            "#  0 開始文節 : 第3次人工知能ブームでは\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  1 開始文節 : ディープラーニングが\t->画像認識\t->テキスト解析\t->音声認識など\t->領域で\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  2 開始文節 : 画像認識\t->テキスト解析\t->音声認識など\t->領域で\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  3 開始文節 : テキスト解析\t->音声認識など\t->領域で\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  4 開始文節 : 音声認識など\t->領域で\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  5 開始文節 : 様々な\t->領域で\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  6 開始文節 : 領域で\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  7 開始文節 : 第2次人工知能ブームの\t->人工知能を\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  8 開始文節 : 人工知能を\t->上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "#  9 開始文節 : 上回る\t->精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 10 開始文節 : 精度を\t->出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 11 開始文節 : 出しており\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 12 開始文節 : ディープラーニングの\t->研究が\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 13 開始文節 : 研究が\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 14 開始文節 : 盛んに\t->行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 15 開始文節 : 行われている\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 16 開始文節 : 最近では\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 17 開始文節 : DQN\t->CNN\t->RNN\t->GANと\t->ディープラーニングの\t->派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 18 開始文節 : CNN\t->RNN\t->GANと\t->ディープラーニングの\t->派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 19 開始文節 : RNN\t->GANと\t->ディープラーニングの\t->派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 20 開始文節 : GANと\t->ディープラーニングの\t->派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 21 開始文節 : 様々な\t->ディープラーニングの\t->派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 22 開始文節 : ディープラーニングの\t->派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 23 開始文節 : 派生が\t->でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 24 開始文節 : でて\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 25 開始文節 : 各分野で\t->活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 26 開始文節 : 活躍している\t->見せている\t->始まっていると\t->指摘している\n",
            "# 27 開始文節 : 特に\t->見せている\t->始まっていると\t->指摘している\n",
            "# 28 開始文節 : GAN\t->敵対的生成ネットワークは\t->見せている\t->始まっていると\t->指摘している\n",
            "# 29 開始文節 : 敵対的生成ネットワークは\t->見せている\t->始まっていると\t->指摘している\n",
            "# 30 開始文節 : ディープラーニングが\t->だしている\t->ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 31 開始文節 : 認識や\t->予測などの\t->分野で\t->だしている\t->ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 32 開始文節 : 予測などの\t->分野で\t->だしている\t->ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 33 開始文節 : 分野で\t->だしている\t->ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 34 開始文節 : 成果を\t->だしている\t->ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 35 開始文節 : だしている\t->ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 36 開始文節 : ことに\t->加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 37 開始文節 : 加えて\t->見せている\t->始まっていると\t->指摘している\n",
            "# 38 開始文節 : 画像の\t->生成技術において\t->見せている\t->始まっていると\t->指摘している\n",
            "# 39 開始文節 : 生成技術において\t->見せている\t->始まっていると\t->指摘している\n",
            "# 40 開始文節 : 大きな\t->進化を\t->見せている\t->始まっていると\t->指摘している\n",
            "# 41 開始文節 : 進化を\t->見せている\t->始まっていると\t->指摘している\n",
            "# 42 開始文節 : 見せている\t->始まっていると\t->指摘している\n",
            "# 43 開始文節 : 森正弥は\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 44 開始文節 : これらの\t->成果を\t->背景に\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 45 開始文節 : 成果を\t->背景に\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 46 開始文節 : 背景に\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 47 開始文節 : 従来の\t->人工知能の\t->応用分野が\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 48 開始文節 : 人工知能の\t->応用分野が\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 49 開始文節 : 応用分野が\t->広がっており\t->始まっていると\t->指摘している\n",
            "# 50 開始文節 : 広がっており\t->始まっていると\t->指摘している\n",
            "# 51 開始文節 : CreativeAIという\t->コンテンツ生成を\t->行っていく\t->応用も\t->始まっていると\t->指摘している\n",
            "# 52 開始文節 : コンテンツ生成を\t->行っていく\t->応用も\t->始まっていると\t->指摘している\n",
            "# 53 開始文節 : 行っていく\t->応用も\t->始まっていると\t->指摘している\n",
            "# 54 開始文節 : 応用も\t->始まっていると\t->指摘している\n",
            "# 55 開始文節 : 始まっていると\t->指摘している\n",
            "--- Sentence 0010 ---\n",
            "#  0 開始文節 : AIの\t->構築が\t->長い間\t->試みられてきているが\t->なってきた\n",
            "#  1 開始文節 : 構築が\t->長い間\t->試みられてきているが\t->なってきた\n",
            "#  2 開始文節 : 長い間\t->試みられてきているが\t->なってきた\n",
            "#  3 開始文節 : 試みられてきているが\t->なってきた\n",
            "#  4 開始文節 : シンボルグラウンディング問題と\t->フレーム問題の\t->解決が\t->なってきた\n",
            "#  5 開始文節 : フレーム問題の\t->解決が\t->なってきた\n",
            "#  6 開始文節 : 解決が\t->なってきた\n",
            "#  7 開始文節 : 大きな\t->壁と\t->なってきた\n",
            "#  8 開始文節 : 壁と\t->なってきた\n",
            "--- Sentence 0011 ---\n",
            "#  0 開始文節 : 17世紀\t->初め\t->機械論\t->製作した\t->行った\n",
            "#  1 開始文節 : 初め\t->機械論\t->製作した\t->行った\n",
            "#  2 開始文節 : ルネ・デカルトは\t->機械論\t->製作した\t->行った\n",
            "#  3 開始文節 : 動物の\t->身体が\t->提唱した\t->機械論\t->製作した\t->行った\n",
            "#  4 開始文節 : 身体が\t->提唱した\t->機械論\t->製作した\t->行った\n",
            "#  5 開始文節 : ただの\t->機械であると\t->提唱した\t->機械論\t->製作した\t->行った\n",
            "#  6 開始文節 : 複雑な\t->機械であると\t->提唱した\t->機械論\t->製作した\t->行った\n",
            "#  7 開始文節 : 機械であると\t->提唱した\t->機械論\t->製作した\t->行った\n",
            "#  8 開始文節 : 提唱した\t->機械論\t->製作した\t->行った\n",
            "#  9 開始文節 : 機械論\t->製作した\t->行った\n",
            "# 10 開始文節 : ブレーズ・パスカルは\t->製作した\t->行った\n",
            "# 11 開始文節 : 1642年\t->製作した\t->行った\n",
            "# 12 開始文節 : 最初の\t->機械式計算機を\t->製作した\t->行った\n",
            "# 13 開始文節 : 機械式計算機を\t->製作した\t->行った\n",
            "# 14 開始文節 : 製作した\t->行った\n",
            "# 15 開始文節 : チャールズバベッジと\t->エイダ・ラブレスは\t->行った\n",
            "# 16 開始文節 : エイダ・ラブレスは\t->行った\n",
            "# 17 開始文節 : プログラム可能な\t->機械式計算機の\t->開発を\t->行った\n",
            "# 18 開始文節 : 機械式計算機の\t->開発を\t->行った\n",
            "# 19 開始文節 : 開発を\t->行った\n",
            "--- Sentence 0012 ---\n",
            "#  0 開始文節 : バートランド・ラッセルと\t->アルフレッドノースホワイトヘッドは\t->もたらした\t->築いた\n",
            "#  1 開始文節 : アルフレッドノースホワイトヘッドは\t->もたらした\t->築いた\n",
            "#  2 開始文節 : 数学原理を\t->出版し\t->もたらした\t->築いた\n",
            "#  3 開始文節 : 出版し\t->もたらした\t->築いた\n",
            "#  4 開始文節 : 形式論理に\t->もたらした\t->築いた\n",
            "#  5 開始文節 : 革命を\t->もたらした\t->築いた\n",
            "#  6 開始文節 : もたらした\t->築いた\n",
            "#  7 開始文節 : ウォーレン・マカロックと\t->ウォルター・ピッツは\t->発表し\t->築いた\n",
            "#  8 開始文節 : ウォルター・ピッツは\t->発表し\t->築いた\n",
            "#  9 開始文節 : 神経活動に\t->内在する\t->アイデアの\t->論理計算と\t->題する\t->論文を\t->発表し\t->築いた\n",
            "# 10 開始文節 : 内在する\t->アイデアの\t->論理計算と\t->題する\t->論文を\t->発表し\t->築いた\n",
            "# 11 開始文節 : アイデアの\t->論理計算と\t->題する\t->論文を\t->発表し\t->築いた\n",
            "# 12 開始文節 : 論理計算と\t->題する\t->論文を\t->発表し\t->築いた\n",
            "# 13 開始文節 : 題する\t->論文を\t->発表し\t->築いた\n",
            "# 14 開始文節 : 論文を\t->発表し\t->築いた\n",
            "# 15 開始文節 : 1943年に\t->発表し\t->築いた\n",
            "# 16 開始文節 : 発表し\t->築いた\n",
            "# 17 開始文節 : ニューラルネットワークの\t->基礎を\t->築いた\n",
            "# 18 開始文節 : 基礎を\t->築いた\n",
            "--- Sentence 0013 ---\n",
            "#  0 開始文節 : 1950年代に\t->なると\t->出始めた\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  1 開始文節 : なると\t->出始めた\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  2 開始文節 : AIに関して\t->出始めた\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  3 開始文節 : 活発な\t->成果が\t->出始めた\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  4 開始文節 : 成果が\t->出始めた\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  5 開始文節 : 出始めた\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  6 開始文節 : ジョンマッカーシーは\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  7 開始文節 : AIに関する\t->最初の\t->会議で\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  8 開始文節 : 最初の\t->会議で\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "#  9 開始文節 : 会議で\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 10 開始文節 : 人工知能という\t->用語を\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 11 開始文節 : 用語を\t->作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 12 開始文節 : 作り出した\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 13 開始文節 : 彼はまた\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 14 開始文節 : プログラミング言語を\t->開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 15 開始文節 : 開発した\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 16 開始文節 : 知的ふるまいに関する\t->テストを\t->可能にする\t->方法として\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 17 開始文節 : テストを\t->可能にする\t->方法として\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 18 開始文節 : 可能にする\t->方法として\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 19 開始文節 : 方法として\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 20 開始文節 : アランチューリングは\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 21 開始文節 : チューリングテストを\t->導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 22 開始文節 : 導入した\t->構築した\t->おしゃべりロボットである\n",
            "# 23 開始文節 : ジョセフ・ワイゼンバウムはを\t->構築した\t->おしゃべりロボットである\n",
            "# 24 開始文節 : 構築した\t->おしゃべりロボットである\n",
            "# 25 開始文節 : これは\t->おしゃべりロボットである\n",
            "# 26 開始文節 : 来談者中心療法を\t->行う\t->おしゃべりロボットである\n",
            "# 27 開始文節 : 行う\t->おしゃべりロボットである\n",
            "--- Sentence 0014 ---\n",
            "#  0 開始文節 : 1956年に\t->行われた\t->提案書において\t->使用され\t->創立された\n",
            "#  1 開始文節 : 行われた\t->提案書において\t->使用され\t->創立された\n",
            "#  2 開始文節 : ダートマス会議開催の\t->提案書において\t->使用され\t->創立された\n",
            "#  3 開始文節 : 提案書において\t->使用され\t->創立された\n",
            "#  4 開始文節 : 人類史上\t->用語として\t->使用され\t->創立された\n",
            "#  5 開始文節 : 用語として\t->使用され\t->創立された\n",
            "#  6 開始文節 : 初めて\t->使用され\t->創立された\n",
            "#  7 開始文節 : 使用され\t->創立された\n",
            "#  8 開始文節 : 新たな\t->分野として\t->創立された\n",
            "#  9 開始文節 : 分野として\t->創立された\n",
            "--- Sentence 0015 ---\n",
            "#  0 開始文節 : 1960年代と\t->1970年代の\t->間に\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  1 開始文節 : 1970年代の\t->間に\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  2 開始文節 : 間に\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  3 開始文節 : ジョエルモーゼスは\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  4 開始文節 : プログラム中で\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  5 開始文節 : 積分問題での\t->記号的推論の\t->パワーを\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  6 開始文節 : 記号的推論の\t->パワーを\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  7 開始文節 : パワーを\t->示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  8 開始文節 : 示した\t->開発した\t->示した\t->ある\t->障害物の\n",
            "#  9 開始文節 : マービン・ミンスキーと\t->シーモア・パパートは\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 10 開始文節 : シーモア・パパートは\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 11 開始文節 : パーセプトロンを\t->出版して\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 12 開始文節 : 出版して\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 13 開始文節 : 単純な\t->ニューラルネットの\t->限界を\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 14 開始文節 : ニューラルネットの\t->限界を\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 15 開始文節 : 限界を\t->示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 16 開始文節 : 示し\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 17 開始文節 : アランカルメラウアーは\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 18 開始文節 : プログラミング言語を\t->開発した\t->示した\t->ある\t->障害物の\n",
            "# 19 開始文節 : 開発した\t->示した\t->ある\t->障害物の\n",
            "# 20 開始文節 : テッド・ショートリッフェは\t->構築し\t->示した\t->ある\t->障害物の\n",
            "# 21 開始文節 : 医学的診断と\t->療法における\t->ルールベースシステムを\t->構築し\t->示した\t->ある\t->障害物の\n",
            "# 22 開始文節 : 療法における\t->ルールベースシステムを\t->構築し\t->示した\t->ある\t->障害物の\n",
            "# 23 開始文節 : ルールベースシステムを\t->構築し\t->示した\t->ある\t->障害物の\n",
            "# 24 開始文節 : 構築し\t->示した\t->ある\t->障害物の\n",
            "# 25 開始文節 : 知識表現と\t->推論の\t->パワーを\t->示した\t->ある\t->障害物の\n",
            "# 26 開始文節 : 推論の\t->パワーを\t->示した\t->ある\t->障害物の\n",
            "# 27 開始文節 : パワーを\t->示した\t->ある\t->障害物の\n",
            "# 28 開始文節 : 示した\t->ある\t->障害物の\n",
            "# 29 開始文節 : これは\t->ある\t->障害物の\n",
            "# 30 開始文節 : 最初の\t->エキスパートシステムと\t->呼ばれる\t->ことも\t->ある\t->障害物の\n",
            "# 31 開始文節 : エキスパートシステムと\t->呼ばれる\t->ことも\t->ある\t->障害物の\n",
            "# 32 開始文節 : 呼ばれる\t->ことも\t->ある\t->障害物の\n",
            "# 33 開始文節 : ことも\t->ある\t->障害物の\n",
            "# 34 開始文節 : ある\t->障害物の\n",
            "# 35 開始文節 : ハンス・モラベックは\t->障害物の\n",
            "--- Sentence 0016 ---\n",
            "#  0 開始文節 : ある\t->コースを\t->走行する\t->最初の\t->コンピューター制御の\t->乗り物を\t->開発した\n",
            "#  1 開始文節 : コースを\t->走行する\t->最初の\t->コンピューター制御の\t->乗り物を\t->開発した\n",
            "#  2 開始文節 : 自律的に\t->走行する\t->最初の\t->コンピューター制御の\t->乗り物を\t->開発した\n",
            "#  3 開始文節 : 走行する\t->最初の\t->コンピューター制御の\t->乗り物を\t->開発した\n",
            "#  4 開始文節 : 最初の\t->コンピューター制御の\t->乗り物を\t->開発した\n",
            "#  5 開始文節 : コンピューター制御の\t->乗り物を\t->開発した\n",
            "#  6 開始文節 : 乗り物を\t->開発した\n",
            "--- Sentence 0017 ---\n",
            "#  0 開始文節 : 1980年代に\t->使われるようになった\n",
            "#  1 開始文節 : ニューラルネットワークは\t->使われるようになった\n",
            "#  2 開始文節 : バックプロパゲーションアルゴリズムによって\t->使われるようになった\n",
            "#  3 開始文節 : 広く\t->使われるようになった\n",
            "--- Sentence 0018 ---\n",
            "#  0 開始文節 : また\t->提唱した\n",
            "#  1 開始文節 : この\t->時代に\t->提唱した\n",
            "#  2 開始文節 : 時代に\t->提唱した\n",
            "#  3 開始文節 : ロドニー・ブルックスが\t->提唱した\n",
            "#  4 開始文節 : 知能には\t->提唱した\n",
            "#  5 開始文節 : 身体が\t->必須との\t->学説\t->身体性を\t->提唱した\n",
            "#  6 開始文節 : 必須との\t->学説\t->身体性を\t->提唱した\n",
            "#  7 開始文節 : 学説\t->身体性を\t->提唱した\n",
            "#  8 開始文節 : 身体性を\t->提唱した\n",
            "--- Sentence 0019 ---\n",
            "#  0 開始文節 : 1990年代は\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  1 開始文節 : AIの\t->多くの\t->分野で\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  2 開始文節 : 多くの\t->分野で\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  3 開始文節 : 分野で\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  4 開始文節 : 様々な\t->アプリケーションが\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  5 開始文節 : アプリケーションが\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  6 開始文節 : 成果を\t->上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  7 開始文節 : 上げた\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  8 開始文節 : 特に\t->目覚ましく\t->敗れた\t->明らかにした\t->指摘された\n",
            "#  9 開始文節 : ボードゲームでは\t->目覚ましく\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 10 開始文節 : 目覚ましく\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 11 開始文節 : 1992年に\t->開発し\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 12 開始文節 : IBMは\t->開発し\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 13 開始文節 : 世界チャンピオンに\t->匹敵する\t->バックギャモン専用コンピュータTDギャモンを\t->開発し\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 14 開始文節 : 匹敵する\t->バックギャモン専用コンピュータTDギャモンを\t->開発し\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 15 開始文節 : バックギャモン専用コンピュータTDギャモンを\t->開発し\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 16 開始文節 : 開発し\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 17 開始文節 : IBMの\t->チェス専用コンピュータ・ディープ・ブルーは\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 18 開始文節 : チェス専用コンピュータ・ディープ・ブルーは\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 19 開始文節 : 1997年\t->5月に\t->打ち負かし\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 20 開始文節 : 5月に\t->打ち負かし\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 21 開始文節 : ガルリ・カスパロフを\t->打ち負かし\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 22 開始文節 : 打ち負かし\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 23 開始文節 : 同年\t->8月には\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 24 開始文節 : 8月には\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 25 開始文節 : オセロで\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 26 開始文節 : 日本電気の\t->オセロ専用コンピュータ・ロジステロに\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 27 開始文節 : オセロ専用コンピュータ・ロジステロに\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 28 開始文節 : 世界チャンピオンの\t->村上健が\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 29 開始文節 : 村上健が\t->敗れた\t->明らかにした\t->指摘された\n",
            "# 30 開始文節 : 敗れた\t->明らかにした\t->指摘された\n",
            "# 31 開始文節 : 国防高等研究計画局は\t->明らかにした\t->指摘された\n",
            "# 32 開始文節 : 最初の\t->湾岸戦争において\t->スケジューリングするのに\t->使い\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 33 開始文節 : 湾岸戦争において\t->スケジューリングするのに\t->使い\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 34 開始文節 : ユニットを\t->スケジューリングするのに\t->使い\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 35 開始文節 : スケジューリングするのに\t->使い\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 36 開始文節 : AIを\t->使い\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 37 開始文節 : 使い\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 38 開始文節 : これによって\t->省かれた\t->コストが\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 39 開始文節 : 省かれた\t->コストが\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 40 開始文節 : コストが\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 41 開始文節 : 1950年代以来の\t->AI研究への\t->政府の\t->投資全額を\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 42 開始文節 : AI研究への\t->政府の\t->投資全額を\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 43 開始文節 : 政府の\t->投資全額を\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 44 開始文節 : 投資全額を\t->上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 45 開始文節 : 上回った\t->ことを\t->明らかにした\t->指摘された\n",
            "# 46 開始文節 : ことを\t->明らかにした\t->指摘された\n",
            "# 47 開始文節 : 明らかにした\t->指摘された\n",
            "# 48 開始文節 : 日本では\t->発生したが\t->指摘された\n",
            "# 49 開始文節 : 甘利俊一日本学士院会員らが\t->啓蒙し\t->発生したが\t->指摘された\n",
            "# 50 開始文節 : 精力的に\t->啓蒙し\t->発生したが\t->指摘された\n",
            "# 51 開始文節 : 啓蒙し\t->発生したが\t->指摘された\n",
            "# 52 開始文節 : 優秀な\t->成果も\t->発生したが\t->指摘された\n",
            "# 53 開始文節 : 成果も\t->発生したが\t->指摘された\n",
            "# 54 開始文節 : 発生したが\t->指摘された\n",
            "# 55 開始文節 : 論理の\t->ブラックボックス性が\t->指摘された\n",
            "# 56 開始文節 : ブラックボックス性が\t->指摘された\n",
            "--- Sentence 0020 ---\n",
            "#  0 開始文節 : 1998年には\t->提唱されたが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  1 開始文節 : 非構造化データ形式の\t->国際規格である\t->XMLが\t->提唱されたが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  2 開始文節 : 国際規格である\t->XMLが\t->提唱されたが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  3 開始文節 : XMLが\t->提唱されたが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  4 開始文節 : 提唱されたが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  5 開始文節 : ここから\t->適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  6 開始文節 : Web上の\t->非構造化データに対して\t->適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  7 開始文節 : 非構造化データに対して\t->適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  8 開始文節 : アプリケーション別に\t->適した\t->意味付けを\t->適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "#  9 開始文節 : 適した\t->意味付けを\t->適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 10 開始文節 : 意味付けを\t->適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 11 開始文節 : 適用し\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 12 開始文節 : 処理を\t->行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 13 開始文節 : 行わせる\t->試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 14 開始文節 : 試みが\t->開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 15 開始文節 : 開始された\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 16 開始文節 : 同年に\t->行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 17 開始文節 : W\t->3Cの\t->ティム・バーナーズリーにより\t->行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 18 開始文節 : 3Cの\t->ティム・バーナーズリーにより\t->行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 19 開始文節 : ティム・バーナーズリーにより\t->行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 20 開始文節 : Webに\t->行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 21 開始文節 : 知的処理を\t->行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 22 開始文節 : 行わせる\t->セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 23 開始文節 : セマンティック・ウェブが\t->提唱された\t->ものである\t->分かる\t->していない\n",
            "# 24 開始文節 : 提唱された\t->ものである\t->分かる\t->していない\n",
            "# 25 開始文節 : この\t->技術は\t->ものである\t->分かる\t->していない\n",
            "# 26 開始文節 : 技術は\t->ものである\t->分かる\t->していない\n",
            "# 27 開始文節 : Web上の\t->データに\t->付加して\t->行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 28 開始文節 : データに\t->付加して\t->行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 29 開始文節 : 意味を\t->付加して\t->行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 30 開始文節 : 付加して\t->行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 31 開始文節 : コンピュータに\t->行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 32 開始文節 : 知的処理を\t->行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 33 開始文節 : 行わせる\t->方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 34 開始文節 : 方法を\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 35 開始文節 : 国際的に\t->規格化する\t->ものである\t->分かる\t->していない\n",
            "# 36 開始文節 : 規格化する\t->ものである\t->分かる\t->していない\n",
            "# 37 開始文節 : ものである\t->分かる\t->していない\n",
            "# 38 開始文節 : この\t->規格には\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 39 開始文節 : 規格には\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 40 開始文節 : 知識工学における\t->オントロジーを\t->表現する\t->データ形式の\t->OWLも\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 41 開始文節 : オントロジーを\t->表現する\t->データ形式の\t->OWLも\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 42 開始文節 : 表現する\t->データ形式の\t->OWLも\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 43 開始文節 : データ形式の\t->OWLも\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 44 開始文節 : OWLも\t->含まれている\t->ことから\t->分かる\t->していない\n",
            "# 45 開始文節 : 含まれている\t->ことから\t->分かる\t->していない\n",
            "# 46 開始文節 : ことから\t->分かる\t->していない\n",
            "# 47 開始文節 : かつて\t->流行した\t->エキスパートシステムの\t->亜種である\t->ことが\t->分かる\t->していない\n",
            "# 48 開始文節 : 流行した\t->エキスパートシステムの\t->亜種である\t->ことが\t->分かる\t->していない\n",
            "# 49 開始文節 : エキスパートシステムの\t->亜種である\t->ことが\t->分かる\t->していない\n",
            "# 50 開始文節 : 亜種である\t->ことが\t->分かる\t->していない\n",
            "# 51 開始文節 : ことが\t->分かる\t->していない\n",
            "# 52 開始文節 : 分かる\t->していない\n",
            "# 53 開始文節 : 2000年代\t->前半に\t->完了しているが\t->していない\n",
            "# 54 開始文節 : 前半に\t->完了しているが\t->していない\n",
            "# 55 開始文節 : 規格化が\t->完了しているが\t->していない\n",
            "# 56 開始文節 : 完了しているが\t->していない\n",
            "# 57 開始文節 : Web開発者にとっては\t->していない\n",
            "# 58 開始文節 : 開発工数に\t->見合うだけの\t->メリットが\t->見出せなかった\t->ことから\t->していない\n",
            "# 59 開始文節 : 見合うだけの\t->メリットが\t->見出せなかった\t->ことから\t->していない\n",
            "# 60 開始文節 : メリットが\t->見出せなかった\t->ことから\t->していない\n",
            "# 61 開始文節 : 見出せなかった\t->ことから\t->していない\n",
            "# 62 開始文節 : ことから\t->していない\n",
            "# 63 開始文節 : 現在も\t->していない\n",
            "# 64 開始文節 : 普及は\t->していない\n",
            "--- Sentence 0021 ---\n",
            "#  0 開始文節 : 日本においては\t->流行した\t->終焉した\n",
            "#  1 開始文節 : エキスパートシステムの\t->流行の\t->後に\t->流行した\t->終焉した\n",
            "#  2 開始文節 : 流行の\t->後に\t->流行した\t->終焉した\n",
            "#  3 開始文節 : 後に\t->流行した\t->終焉した\n",
            "#  4 開始文節 : ニューロファジィが\t->流行した\t->終焉した\n",
            "#  5 開始文節 : 流行した\t->終焉した\n",
            "#  6 開始文節 : しかし\t->終焉した\n",
            "#  7 開始文節 : 研究が\t->進むにつれて\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "#  8 開始文節 : 進むにつれて\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "#  9 開始文節 : 計算リソースや\t->データ量の\t->不足\t->シンボルグラウンディング問題\t->フレーム問題に\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 10 開始文節 : データ量の\t->不足\t->シンボルグラウンディング問題\t->フレーム問題に\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 11 開始文節 : 不足\t->シンボルグラウンディング問題\t->フレーム問題に\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 12 開始文節 : シンボルグラウンディング問題\t->フレーム問題に\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 13 開始文節 : フレーム問題に\t->直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 14 開始文節 : 直面し\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 15 開始文節 : 産業の\t->在り方を\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 16 開始文節 : 在り方を\t->激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 17 開始文節 : 激変させるような\t->AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 18 開始文節 : AIに\t->至る\t->ことは\t->無く\t->終焉した\n",
            "# 19 開始文節 : 至る\t->ことは\t->無く\t->終焉した\n",
            "# 20 開始文節 : ことは\t->無く\t->終焉した\n",
            "# 21 開始文節 : 無く\t->終焉した\n",
            "# 22 開始文節 : ブームは\t->終焉した\n",
            "--- Sentence 0022 ---\n",
            "#  0 開始文節 : 1980年代に\t->入って\t->立ち上がった\t->挙げられる\n",
            "#  1 開始文節 : 入って\t->立ち上がった\t->挙げられる\n",
            "#  2 開始文節 : 大企業の\t->研究所を\t->中心に\t->立ち上がった\t->挙げられる\n",
            "#  3 開始文節 : 研究所を\t->中心に\t->立ち上がった\t->挙げられる\n",
            "#  4 開始文節 : 中心に\t->立ち上がった\t->挙げられる\n",
            "#  5 開始文節 : 知識工学に\t->基づく\t->エキスパートシステムが\t->提案されるようになり\t->立ち上がった\t->挙げられる\n",
            "#  6 開始文節 : 基づく\t->エキスパートシステムが\t->提案されるようになり\t->立ち上がった\t->挙げられる\n",
            "#  7 開始文節 : エキスパートシステムが\t->提案されるようになり\t->立ち上がった\t->挙げられる\n",
            "#  8 開始文節 : 多数\t->提案されるようになり\t->立ち上がった\t->挙げられる\n",
            "#  9 開始文節 : 提案されるようになり\t->立ち上がった\t->挙げられる\n",
            "# 10 開始文節 : エキスパートシステムを\t->する\t->AIベンチャーも\t->立ち上がった\t->挙げられる\n",
            "# 11 開始文節 : 専門と\t->する\t->AIベンチャーも\t->立ち上がった\t->挙げられる\n",
            "# 12 開始文節 : する\t->AIベンチャーも\t->立ち上がった\t->挙げられる\n",
            "# 13 開始文節 : AIベンチャーも\t->立ち上がった\t->挙げられる\n",
            "# 14 開始文節 : 次々と\t->立ち上がった\t->挙げられる\n",
            "# 15 開始文節 : 立ち上がった\t->挙げられる\n",
            "# 16 開始文節 : その\t->流行から\t->生まれた\t->究極の\t->プロジェクトとして\t->挙げられる\n",
            "# 17 開始文節 : 流行から\t->生まれた\t->究極の\t->プロジェクトとして\t->挙げられる\n",
            "# 18 開始文節 : 生まれた\t->究極の\t->プロジェクトとして\t->挙げられる\n",
            "# 19 開始文節 : 究極の\t->プロジェクトとして\t->挙げられる\n",
            "# 20 開始文節 : プロジェクトとして\t->挙げられる\n",
            "# 21 開始文節 : 第五世代コンピュータが\t->挙げられる\n",
            "--- Sentence 0023 ---\n",
            "#  0 開始文節 : 1982年から\t->1992年まで\t->至らなかった\t->見つからなかった\n",
            "#  1 開始文節 : 1992年まで\t->至らなかった\t->見つからなかった\n",
            "#  2 開始文節 : 日本は\t->至らなかった\t->見つからなかった\n",
            "#  3 開始文節 : 国家プロジェクトとして\t->費やして\t->進めるも\t->至らなかった\t->見つからなかった\n",
            "#  4 開始文節 : 570億円を\t->費やして\t->進めるも\t->至らなかった\t->見つからなかった\n",
            "#  5 開始文節 : 費やして\t->進めるも\t->至らなかった\t->見つからなかった\n",
            "#  6 開始文節 : 第五世代コンピュータの\t->研究を\t->進めるも\t->至らなかった\t->見つからなかった\n",
            "#  7 開始文節 : 研究を\t->進めるも\t->至らなかった\t->見つからなかった\n",
            "#  8 開始文節 : 進めるも\t->至らなかった\t->見つからなかった\n",
            "#  9 開始文節 : 採用した\t->知識工学的手法では\t->必要で\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 10 開始文節 : 知識工学的手法では\t->必要で\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 11 開始文節 : 膨大な\t->ルールの\t->手入力が\t->必要で\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 12 開始文節 : ルールの\t->手入力が\t->必要で\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 13 開始文節 : 手入力が\t->必要で\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 14 開始文節 : 必要で\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 15 開始文節 : 専門家間で\t->異なる\t->場合には\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 16 開始文節 : 専門知識の\t->解釈が\t->異なる\t->場合には\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 17 開始文節 : 解釈が\t->異なる\t->場合には\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 18 開始文節 : 異なる\t->場合には\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 19 開始文節 : 場合には\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 20 開始文節 : 統一した\t->ルール化が\t->行えない\t->等の\t->問題も\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 21 開始文節 : ルール化が\t->行えない\t->等の\t->問題も\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 22 開始文節 : 行えない\t->等の\t->問題も\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 23 開始文節 : 等の\t->問題も\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 24 開始文節 : 問題も\t->あり\t->至らなかった\t->見つからなかった\n",
            "# 25 開始文節 : あり\t->至らなかった\t->見つからなかった\n",
            "# 26 開始文節 : 実用的な\t->エキスパートシステムの\t->実現には\t->至らなかった\t->見つからなかった\n",
            "# 27 開始文節 : エキスパートシステムの\t->実現には\t->至らなかった\t->見つからなかった\n",
            "# 28 開始文節 : 実現には\t->至らなかった\t->見つからなかった\n",
            "# 29 開始文節 : 至らなかった\t->見つからなかった\n",
            "# 30 開始文節 : 実現した\t->成果物は\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 31 開始文節 : 成果物は\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 32 開始文節 : Prologの\t->命令を\t->解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 33 開始文節 : 命令を\t->解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 34 開始文節 : 直接\t->解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 35 開始文節 : CPUの\t->ハードウェアの\t->機構で\t->解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 36 開始文節 : ハードウェアの\t->機構で\t->解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 37 開始文節 : 機構で\t->解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 38 開始文節 : 解釈して\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 39 開始文節 : 高速に\t->実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 40 開始文節 : 実行する\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 41 開始文節 : 並列型の\t->Prolog専用機であるが\t->見つからなかった\n",
            "# 42 開始文節 : Prolog専用機であるが\t->見つからなかった\n",
            "# 43 開始文節 : 商業的な\t->意味で\t->見つからなかった\n",
            "# 44 開始文節 : 意味で\t->見つからなかった\n",
            "# 45 開始文節 : 応用先が\t->見つからなかった\n",
            "# 46 開始文節 : 全く\t->見つからなかった\n",
            "--- Sentence 0024 ---\n",
            "#  0 開始文節 : 1980年代\t->後半から\t->中頃にかけて\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  1 開始文節 : 後半から\t->中頃にかけて\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  2 開始文節 : 1990年代\t->中頃にかけて\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  3 開始文節 : 中頃にかけて\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  4 開始文節 : 従来から\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  5 開始文節 : 電子制御の\t->手法として\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  6 開始文節 : 手法として\t->用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  7 開始文節 : 用いられてきた\t->ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  8 開始文節 : ON/OFF制御\t->PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "#  9 開始文節 : PID制御\t->現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "# 10 開始文節 : 現代制御の\t->問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "# 11 開始文節 : 問題を\t->克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "# 12 開始文節 : 克服する\t->ため\t->迎えた\t->発売され始めた\n",
            "# 13 開始文節 : ため\t->迎えた\t->発売され始めた\n",
            "# 14 開始文節 : 知的制御が\t->研究され\t->迎えた\t->発売され始めた\n",
            "# 15 開始文節 : 盛んに\t->研究され\t->迎えた\t->発売され始めた\n",
            "# 16 開始文節 : 研究され\t->迎えた\t->発売され始めた\n",
            "# 17 開始文節 : 知識工学的な\t->ルールを\t->用いる\t->ファジィ制御\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 18 開始文節 : ルールを\t->用いる\t->ファジィ制御\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 19 開始文節 : 用いる\t->ファジィ制御\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 20 開始文節 : ファジィ制御\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 21 開始文節 : データの\t->特徴を\t->学習して\t->分類する\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 22 開始文節 : 特徴を\t->学習して\t->分類する\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 23 開始文節 : 学習して\t->分類する\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 24 開始文節 : 分類する\t->ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 25 開始文節 : ニューラルネットワーク\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 26 開始文節 : その\t->2つを\t->融合した\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 27 開始文節 : 2つを\t->融合した\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 28 開始文節 : 融合した\t->ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 29 開始文節 : ニューロファジィという\t->手法が\t->迎えた\t->発売され始めた\n",
            "# 30 開始文節 : 手法が\t->迎えた\t->発売され始めた\n",
            "# 31 開始文節 : 日本を\t->中心に\t->迎えた\t->発売され始めた\n",
            "# 32 開始文節 : 中心に\t->迎えた\t->発売され始めた\n",
            "# 33 開始文節 : ブームを\t->迎えた\t->発売され始めた\n",
            "# 34 開始文節 : 迎えた\t->発売され始めた\n",
            "# 35 開始文節 : バブル期の\t->高級路線に\t->合わせて\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 36 開始文節 : 高級路線に\t->合わせて\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 37 開始文節 : 合わせて\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 38 開始文節 : 白物家電製品でも\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 39 開始文節 : センサの\t->個数と\t->種類を\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 40 開始文節 : 個数と\t->種類を\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 41 開始文節 : 種類を\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 42 開始文節 : 大幅に\t->増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 43 開始文節 : 増やし\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 44 開始文節 : 多様な\t->データを\t->元に\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 45 開始文節 : データを\t->元に\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 46 開始文節 : 元に\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 47 開始文節 : 運転を\t->最適化する\t->モデルが\t->発売され始めた\n",
            "# 48 開始文節 : 最適化する\t->モデルが\t->発売され始めた\n",
            "# 49 開始文節 : モデルが\t->発売され始めた\n",
            "# 50 開始文節 : 多数\t->発売され始めた\n",
            "--- Sentence 0025 ---\n",
            "#  0 開始文節 : ファジィについては\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  1 開始文節 : 2018年までに\t->取得している\t->事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  2 開始文節 : 日本が\t->取得している\t->事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  3 開始文節 : 世界の\t->1/5の\t->特許を\t->取得している\t->事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  4 開始文節 : 1/5の\t->特許を\t->取得している\t->事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  5 開始文節 : 特許を\t->取得している\t->事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  6 開始文節 : 取得している\t->事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  7 開始文節 : 事から\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  8 開始文節 : 日本で\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "#  9 開始文節 : 特に\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 10 開始文節 : 大きな\t->ブームと\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 11 開始文節 : ブームと\t->なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 12 開始文節 : なっていた\t->ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 13 開始文節 : ことが\t->分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 14 開始文節 : 分かっている\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 15 開始文節 : 現在の\t->白物家電では\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 16 開始文節 : 白物家電では\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 17 開始文節 : この\t->当時より\t->発展した\t->制御技術が\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 18 開始文節 : 当時より\t->発展した\t->制御技術が\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 19 開始文節 : 更に\t->発展した\t->制御技術が\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 20 開始文節 : 発展した\t->制御技術が\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 21 開始文節 : 制御技術が\t->用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 22 開始文節 : 用いられているが\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 23 開始文節 : 既に\t->なり\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 24 開始文節 : 当たり前の\t->ものに\t->なり\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 25 開始文節 : ものに\t->なり\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 26 開始文節 : なり\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 27 開始文節 : 利用者には\t->意識されなくなっている\t->行われなかった\t->言える\n",
            "# 28 開始文節 : 意識されなくなっている\t->行われなかった\t->言える\n",
            "# 29 開始文節 : ニューロファジィが\t->なった\t->1990年代には\t->行われなかった\t->言える\n",
            "# 30 開始文節 : ブームに\t->なった\t->1990年代には\t->行われなかった\t->言える\n",
            "# 31 開始文節 : なった\t->1990年代には\t->行われなかった\t->言える\n",
            "# 32 開始文節 : 1990年代には\t->行われなかった\t->言える\n",
            "# 33 開始文節 : 未だ\t->行われなかった\t->言える\n",
            "# 34 開始文節 : ビッグデータという\t->概念は\t->行われなかった\t->言える\n",
            "# 35 開始文節 : 概念は\t->行われなかった\t->言える\n",
            "# 36 開始文節 : 無く\t->提唱された\t->産業応用は\t->行われなかった\t->言える\n",
            "# 37 開始文節 : ブロードバンド接続普及後の\t->2010年に\t->提唱された\t->産業応用は\t->行われなかった\t->言える\n",
            "# 38 開始文節 : 2010年に\t->提唱された\t->産業応用は\t->行われなかった\t->言える\n",
            "# 39 開始文節 : 初めて\t->提唱された\t->産業応用は\t->行われなかった\t->言える\n",
            "# 40 開始文節 : 提唱された\t->産業応用は\t->行われなかった\t->言える\n",
            "# 41 開始文節 : データマイニングとしての\t->産業応用は\t->行われなかった\t->言える\n",
            "# 42 開始文節 : 産業応用は\t->行われなかった\t->言える\n",
            "# 43 開始文節 : 行われなかった\t->言える\n",
            "# 44 開始文節 : しかし\t->言える\n",
            "# 45 開始文節 : ニューラルネットワークが\t->流行した\t->事例としては\t->事例であり\t->社会現象と\t->言える\n",
            "# 46 開始文節 : 一般人も\t->巻き込んで\t->流行した\t->事例としては\t->事例であり\t->社会現象と\t->言える\n",
            "# 47 開始文節 : 巻き込んで\t->流行した\t->事例としては\t->事例であり\t->社会現象と\t->言える\n",
            "# 48 開始文節 : 流行した\t->事例としては\t->事例であり\t->社会現象と\t->言える\n",
            "# 49 開始文節 : 事例としては\t->事例であり\t->社会現象と\t->言える\n",
            "# 50 開始文節 : 初めての\t->事例であり\t->社会現象と\t->言える\n",
            "# 51 開始文節 : 事例であり\t->社会現象と\t->言える\n",
            "# 52 開始文節 : 2010年代の\t->ディープラーニングブームの\t->前史とも\t->言える\t->社会現象と\t->言える\n",
            "# 53 開始文節 : ディープラーニングブームの\t->前史とも\t->言える\t->社会現象と\t->言える\n",
            "# 54 開始文節 : 前史とも\t->言える\t->社会現象と\t->言える\n",
            "# 55 開始文節 : 言える\t->社会現象と\t->言える\n",
            "# 56 開始文節 : 社会現象と\t->言える\n",
            "--- Sentence 0026 ---\n",
            "#  0 開始文節 : 松下電器が\t->持つような\t->曖昧さを\t->活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  1 開始文節 : 1985年頃から\t->持つような\t->曖昧さを\t->活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  2 開始文節 : 人間が\t->持つような\t->曖昧さを\t->活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  3 開始文節 : 持つような\t->曖昧さを\t->活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  4 開始文節 : 曖昧さを\t->活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  5 開始文節 : 制御に\t->活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  6 開始文節 : 活かす\t->ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  7 開始文節 : ファジィ制御についての\t->研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  8 開始文節 : 研究を\t->開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "#  9 開始文節 : 開始し\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 10 開始文節 : 1990年2月\t->1日に\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 11 開始文節 : 1日に\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 12 開始文節 : ファジィ洗濯機第1号である\t->愛妻号Dayファジィの\t->発売に\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 13 開始文節 : 愛妻号Dayファジィの\t->発売に\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 14 開始文節 : 発売に\t->漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 15 開始文節 : 漕ぎ着けた\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 16 開始文節 : 愛妻号Dayファジィは\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 17 開始文節 : 従来よりも\t->収集した\t->データに\t->基づいて\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 18 開始文節 : 多数の\t->センサーで\t->収集した\t->データに\t->基づいて\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 19 開始文節 : センサーで\t->収集した\t->データに\t->基づいて\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 20 開始文節 : 収集した\t->データに\t->基づいて\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 21 開始文節 : データに\t->基づいて\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 22 開始文節 : 基づいて\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 23 開始文節 : 柔軟に\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 24 開始文節 : 運転を\t->最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 25 開始文節 : 最適化する\t->洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 26 開始文節 : 洗濯機で\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 27 開始文節 : 同種の\t->洗濯機としては\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 28 開始文節 : 洗濯機としては\t->世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 29 開始文節 : 世界初であった\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 30 開始文節 : ファジィ制御という\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 31 開始文節 : 当時\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 32 開始文節 : 最先端の\t->技術の\t->導入が\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 33 開始文節 : 技術の\t->導入が\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 34 開始文節 : 導入が\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 35 開始文節 : バブル期の\t->高級路線にも\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 36 開始文節 : 高級路線にも\t->マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 37 開始文節 : マッチした\t->ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 38 開始文節 : ことから\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 39 開始文節 : ファジィは\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 40 開始文節 : 裏方の\t->制御技術であるにも\t->関わらず\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 41 開始文節 : 制御技術であるにも\t->関わらず\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 42 開始文節 : 関わらず\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 43 開始文節 : 世間の\t->注目を\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 44 開始文節 : 大きな\t->注目を\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 45 開始文節 : 注目を\t->集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 46 開始文節 : 集めた\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 47 開始文節 : その\t->流行の\t->度合いは\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 48 開始文節 : 流行の\t->度合いは\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 49 開始文節 : 度合いは\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 50 開始文節 : 1990年の\t->新語流行語大賞における\t->新語部門の\t->金賞で\t->選ばれる\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 51 開始文節 : 新語流行語大賞における\t->新語部門の\t->金賞で\t->選ばれる\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 52 開始文節 : 新語部門の\t->金賞で\t->選ばれる\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 53 開始文節 : 金賞で\t->選ばれる\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 54 開始文節 : ファジィが\t->選ばれる\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 55 開始文節 : 選ばれる\t->程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 56 開始文節 : 程であった\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 57 開始文節 : その後に\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 58 開始文節 : 松下電器は\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 59 開始文節 : ファジィルールの\t->チューニングを\t->自動化した\t->ニューロファジィ制御を\t->開発し\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 60 開始文節 : 煩雑な\t->チューニングを\t->自動化した\t->ニューロファジィ制御を\t->開発し\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 61 開始文節 : チューニングを\t->自動化した\t->ニューロファジィ制御を\t->開発し\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 62 開始文節 : 自動化した\t->ニューロファジィ制御を\t->開発し\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 63 開始文節 : ニューロファジィ制御を\t->開発し\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 64 開始文節 : 開発し\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 65 開始文節 : 従来の\t->ファジィ理論の\t->限界を\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 66 開始文節 : ファジィ理論の\t->限界を\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 67 開始文節 : 限界を\t->突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 68 開始文節 : 突破して\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 69 開始文節 : 学会で\t->評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 70 開始文節 : 評価されるだけでなく\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 71 開始文節 : 白物家電への\t->応用にも\t->成功して\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 72 開始文節 : 応用にも\t->成功して\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 73 開始文節 : 成功して\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 74 開始文節 : 更なる\t->ブームを\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 75 開始文節 : ブームを\t->巻き起こした\t->発売した\t->付与されていた\n",
            "# 76 開始文節 : 巻き起こした\t->発売した\t->付与されていた\n",
            "# 77 開始文節 : 松下電器の\t->試みの\t->成功を\t->受けて\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 78 開始文節 : 試みの\t->成功を\t->受けて\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 79 開始文節 : 成功を\t->受けて\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 80 開始文節 : 受けて\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 81 開始文節 : 他社も\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 82 開始文節 : 同様の\t->知的制御を\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 83 開始文節 : 知的制御を\t->用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 84 開始文節 : 用いる\t->製品を\t->発売した\t->付与されていた\n",
            "# 85 開始文節 : 製品を\t->発売した\t->付与されていた\n",
            "# 86 開始文節 : 多数\t->発売した\t->付与されていた\n",
            "# 87 開始文節 : 発売した\t->付与されていた\n",
            "# 88 開始文節 : 1990年代\t->中頃までは\t->用いられており\t->付与されていた\n",
            "# 89 開始文節 : 中頃までは\t->用いられており\t->付与されていた\n",
            "# 90 開始文節 : メーカー各社による\t->一般向けの\t->白物家電の\t->売り文句として\t->用いられており\t->付与されていた\n",
            "# 91 開始文節 : 一般向けの\t->白物家電の\t->売り文句として\t->用いられており\t->付与されていた\n",
            "# 92 開始文節 : 白物家電の\t->売り文句として\t->用いられており\t->付与されていた\n",
            "# 93 開始文節 : 売り文句として\t->用いられており\t->付与されていた\n",
            "# 94 開始文節 : 知的制御技術の\t->名称が\t->用いられており\t->付与されていた\n",
            "# 95 開始文節 : 名称が\t->用いられており\t->付与されていた\n",
            "# 96 開始文節 : 大々的に\t->用いられており\t->付与されていた\n",
            "# 97 開始文節 : 用いられており\t->付与されていた\n",
            "# 98 開始文節 : 洗濯機の\t->製品名では\t->付与されていた\n",
            "# 99 開始文節 : 製品名では\t->付与されていた\n",
            "#100 開始文節 : 愛妻号DAYファジィ\t->掃除機の\t->分類としては\t->付与されていた\n",
            "#101 開始文節 : 掃除機の\t->分類としては\t->付与されていた\n",
            "#102 開始文節 : 分類としては\t->付与されていた\n",
            "#103 開始文節 : ニューロ・ファジィ掃除機\t->エアコンの\t->運転モードでは\t->付与されていた\n",
            "#104 開始文節 : エアコンの\t->運転モードでは\t->付与されていた\n",
            "#105 開始文節 : 運転モードでは\t->付与されていた\n",
            "#106 開始文節 : ニューロ自動などの\t->名称が\t->付与されていた\n",
            "#107 開始文節 : 名称が\t->付与されていた\n",
            "--- Sentence 0027 ---\n",
            "#  0 開始文節 : ニューロ\t->ファジィ\t->ニューロファジィという\t->手法は\t->あった\t->迎えた\t->あった\n",
            "#  1 開始文節 : ファジィ\t->ニューロファジィという\t->手法は\t->あった\t->迎えた\t->あった\n",
            "#  2 開始文節 : ニューロファジィという\t->手法は\t->あった\t->迎えた\t->あった\n",
            "#  3 開始文節 : 手法は\t->あった\t->迎えた\t->あった\n",
            "#  4 開始文節 : 従来の\t->オン・オフ制御や\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "#  5 開始文節 : 単純な\t->オン・オフ制御や\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "#  6 開始文節 : オン・オフ制御や\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "#  7 開始文節 : 対象を\t->モデル化する\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "#  8 開始文節 : 数式で\t->モデル化する\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "#  9 開始文節 : 客観的に\t->モデル化する\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 10 開始文節 : モデル化する\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 11 開始文節 : この\t->作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 12 開始文節 : 作業は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 13 開始文節 : 対象が\t->持つ\t->場合は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 14 開始文節 : 複雑な\t->機構を\t->持つ\t->場合は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 15 開始文節 : 機構を\t->持つ\t->場合は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 16 開始文節 : 持つ\t->場合は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 17 開始文節 : 場合は\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 18 開始文節 : 極めて\t->難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 19 開始文節 : 難しくなる\t->必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 20 開始文節 : 必要が\t->ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 21 開始文節 : ある\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 22 開始文節 : PID制御や\t->現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 23 開始文節 : 現代制御等と\t->比較して\t->あった\t->迎えた\t->あった\n",
            "# 24 開始文節 : 比較して\t->あった\t->迎えた\t->あった\n",
            "# 25 開始文節 : 人間の\t->経験則や\t->特徴が\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 26 開始文節 : 主観的な\t->経験則や\t->特徴が\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 27 開始文節 : 経験則や\t->特徴が\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 28 開始文節 : 計測した\t->データの\t->特徴が\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 29 開始文節 : データの\t->特徴が\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 30 開始文節 : 特徴が\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 31 開始文節 : 利用可能と\t->なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 32 開始文節 : なる\t->ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 33 開始文節 : ファジィ\t->ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 34 開始文節 : ニューロ\t->ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 35 開始文節 : ニューロファジィは\t->あった\t->迎えた\t->あった\n",
            "# 36 開始文節 : 開発工数を\t->抑えながら\t->できるという\t->利点が\t->あった\t->迎えた\t->あった\n",
            "# 37 開始文節 : 抑えながら\t->できるという\t->利点が\t->あった\t->迎えた\t->あった\n",
            "# 38 開始文節 : 環境適応時の\t->柔軟性を\t->できるという\t->利点が\t->あった\t->迎えた\t->あった\n",
            "# 39 開始文節 : 柔軟性を\t->できるという\t->利点が\t->あった\t->迎えた\t->あった\n",
            "# 40 開始文節 : 高く\t->できるという\t->利点が\t->あった\t->迎えた\t->あった\n",
            "# 41 開始文節 : できるという\t->利点が\t->あった\t->迎えた\t->あった\n",
            "# 42 開始文節 : 利点が\t->あった\t->迎えた\t->あった\n",
            "# 43 開始文節 : あった\t->迎えた\t->あった\n",
            "# 44 開始文節 : しかし\t->迎えた\t->あった\n",
            "# 45 開始文節 : 開発者らの\t->努力にも\t->関わらず\t->迎えた\t->あった\n",
            "# 46 開始文節 : 努力にも\t->関わらず\t->迎えた\t->あった\n",
            "# 47 開始文節 : 関わらず\t->迎えた\t->あった\n",
            "# 48 開始文節 : 計算能力や\t->データ量の\t->少なさから\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 49 開始文節 : 収集可能な\t->データ量の\t->少なさから\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 50 開始文節 : データ量の\t->少なさから\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 51 開始文節 : 少なさから\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 52 開始文節 : 既存の\t->工作機械や\t->制御を\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 53 開始文節 : 工作機械や\t->制御を\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 54 開始文節 : 家電製品の\t->制御を\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 55 開始文節 : 制御を\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 56 開始文節 : 多少\t->改善する\t->程度で\t->迎えた\t->あった\n",
            "# 57 開始文節 : 改善する\t->程度で\t->迎えた\t->あった\n",
            "# 58 開始文節 : 程度で\t->迎えた\t->あった\n",
            "# 59 開始文節 : 限界を\t->迎えた\t->あった\n",
            "# 60 開始文節 : 迎えた\t->あった\n",
            "# 61 開始文節 : 理論的にも\t->組み合わせであり\t->あった\n",
            "# 62 開始文節 : ファジィ集合と\t->深層学習が\t->組み合わせであり\t->あった\n",
            "# 63 開始文節 : 深層学習が\t->組み合わせであり\t->あった\n",
            "# 64 開始文節 : 不可能な\t->ニューラルネットワークの\t->組み合わせであり\t->あった\n",
            "# 65 開始文節 : ニューラルネットワークの\t->組み合わせであり\t->あった\n",
            "# 66 開始文節 : 組み合わせであり\t->あった\n",
            "# 67 開始文節 : 計算リソースや\t->データが\t->与えられたとしても\t->あった\n",
            "# 68 開始文節 : データが\t->与えられたとしても\t->あった\n",
            "# 69 開始文節 : 潤沢に\t->与えられたとしても\t->あった\n",
            "# 70 開始文節 : 与えられたとしても\t->あった\n",
            "# 71 開始文節 : 認識精度の\t->向上には\t->あった\n",
            "# 72 開始文節 : 向上には\t->あった\n",
            "# 73 開始文節 : 限界が\t->あった\n",
            "--- Sentence 0028 ---\n",
            "#  0 開始文節 : 以降\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  1 開始文節 : 計算機の\t->能力限界から\t->進まず\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  2 開始文節 : 能力限界から\t->進まず\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  3 開始文節 : 理論の\t->改善は\t->進まず\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  4 開始文節 : 改善は\t->進まず\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  5 開始文節 : 遅々として\t->進まず\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  6 開始文節 : 進まず\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  7 開始文節 : 目立った\t->進展は\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  8 開始文節 : 進展は\t->無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "#  9 開始文節 : 無くなり\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 10 開始文節 : 1990年代\t->末には\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 11 開始文節 : 末には\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 12 開始文節 : 知的制御を\t->搭載する\t->白物家電が\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 13 開始文節 : 搭載する\t->白物家電が\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 14 開始文節 : 白物家電が\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 15 開始文節 : 大多数に\t->なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 16 開始文節 : なった\t->ことで\t->去った\t->実証されている\t->発展した\n",
            "# 17 開始文節 : ことで\t->去った\t->実証されている\t->発展した\n",
            "# 18 開始文節 : 売り\t->去った\t->実証されている\t->発展した\n",
            "# 19 開始文節 : 文句としての\t->ブームは\t->去った\t->実証されている\t->発展した\n",
            "# 20 開始文節 : ブームは\t->去った\t->実証されている\t->発展した\n",
            "# 21 開始文節 : 去った\t->実証されている\t->発展した\n",
            "# 22 開始文節 : ブーム後は\t->意識されなくなったが\t->実証されている\t->発展した\n",
            "# 23 開始文節 : 一般には\t->意識されなくなったが\t->実証されている\t->発展した\n",
            "# 24 開始文節 : 意識されなくなったが\t->実証されている\t->発展した\n",
            "# 25 開始文節 : 現在では\t->実証されている\t->発展した\n",
            "# 26 開始文節 : 裏方の\t->技術として\t->実証されている\t->発展した\n",
            "# 27 開始文節 : 技術として\t->実証されている\t->発展した\n",
            "# 28 開始文節 : 家電製品のみならず\t->使われ\t->実証されている\t->発展した\n",
            "# 29 開始文節 : 雨水の\t->排水\t->管理システムなどの\t->社会インフラにも\t->使われ\t->実証されている\t->発展した\n",
            "# 30 開始文節 : 排水\t->管理システムなどの\t->社会インフラにも\t->使われ\t->実証されている\t->発展した\n",
            "# 31 開始文節 : 駐車場\t->管理システムなどの\t->社会インフラにも\t->使われ\t->実証されている\t->発展した\n",
            "# 32 開始文節 : ビルの\t->管理システムなどの\t->社会インフラにも\t->使われ\t->実証されている\t->発展した\n",
            "# 33 開始文節 : 管理システムなどの\t->社会インフラにも\t->使われ\t->実証されている\t->発展した\n",
            "# 34 開始文節 : 社会インフラにも\t->使われ\t->実証されている\t->発展した\n",
            "# 35 開始文節 : 使われ\t->実証されている\t->発展した\n",
            "# 36 開始文節 : 十分に\t->実証されている\t->発展した\n",
            "# 37 開始文節 : 性能と\t->安定性が\t->実証されている\t->発展した\n",
            "# 38 開始文節 : 安定性が\t->実証されている\t->発展した\n",
            "# 39 開始文節 : 実証されている\t->発展した\n",
            "# 40 開始文節 : 2003年頃には\t->発展した\n",
            "# 41 開始文節 : 人間が\t->設計した\t->オントロジー\t->表現するを\t->利活用する\t->ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 42 開始文節 : 設計した\t->オントロジー\t->表現するを\t->利活用する\t->ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 43 開始文節 : オントロジー\t->表現するを\t->利活用する\t->ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 44 開始文節 : ファジィルールとして\t->表現するを\t->利活用する\t->ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 45 開始文節 : 表現するを\t->利活用する\t->ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 46 開始文節 : 利活用する\t->ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 47 開始文節 : ネットワークインテリジェンスという\t->分野に\t->発展した\n",
            "# 48 開始文節 : 分野に\t->発展した\n",
            "--- Sentence 0029 ---\n",
            "#  0 開始文節 : 2005年\t->発表した\n",
            "#  1 開始文節 : レイカーツワイルは\t->発表した\n",
            "#  2 開始文節 : 著作で\t->発表した\n",
            "#  3 開始文節 : 圧倒的な\t->人工知能が\t->超越し\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "#  4 開始文節 : 人工知能が\t->超越し\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "#  5 開始文節 : 知識知能の\t->点で\t->超越し\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "#  6 開始文節 : 点で\t->超越し\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "#  7 開始文節 : 人間を\t->超越し\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "#  8 開始文節 : 超越し\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "#  9 開始文節 : 科学技術の\t->進歩を\t->担い\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 10 開始文節 : 進歩を\t->担い\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 11 開始文節 : 担い\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 12 開始文節 : 世界を\t->変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 13 開始文節 : 変革する\t->技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 14 開始文節 : 技術的特異点\t->シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 15 開始文節 : シンギュラリティが\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 16 開始文節 : 2045年にも\t->訪れると\t->する\t->説を\t->発表した\n",
            "# 17 開始文節 : 訪れると\t->する\t->説を\t->発表した\n",
            "# 18 開始文節 : する\t->説を\t->発表した\n",
            "# 19 開始文節 : 説を\t->発表した\n",
            "--- Sentence 0030 ---\n",
            "#  0 開始文節 : 2006年に\t->提案された\t->起源\n",
            "#  1 開始文節 : ジェフリーヒントンらの\t->研究チームにより\t->提案された\t->起源\n",
            "#  2 開始文節 : 研究チームにより\t->提案された\t->起源\n",
            "#  3 開始文節 : オートエンコーダによる\t->ニューラルネットワークの\t->深層化手法が\t->提案された\t->起源\n",
            "#  4 開始文節 : ニューラルネットワークの\t->深層化手法が\t->提案された\t->起源\n",
            "#  5 開始文節 : 深層化手法が\t->提案された\t->起源\n",
            "#  6 開始文節 : 提案された\t->起源\n",
            "#  7 開始文節 : 現在の\t->ディープラーニングの\t->起源\n",
            "#  8 開始文節 : ディープラーニングの\t->起源\n",
            "#  9 開始文節 : 直接的な\t->起源\n",
            "--- Sentence 0031 ---\n",
            "#  0 開始文節 : 2010年代に\t->入り\t->整備された\t->ことで\t->前進し始めた\n",
            "#  1 開始文節 : 入り\t->整備された\t->ことで\t->前進し始めた\n",
            "#  2 開始文節 : 膨大な\t->データを\t->扱う\t->研究開発の\t->ための\t->環境が\t->整備された\t->ことで\t->前進し始めた\n",
            "#  3 開始文節 : データを\t->扱う\t->研究開発の\t->ための\t->環境が\t->整備された\t->ことで\t->前進し始めた\n",
            "#  4 開始文節 : 扱う\t->研究開発の\t->ための\t->環境が\t->整備された\t->ことで\t->前進し始めた\n",
            "#  5 開始文節 : 研究開発の\t->ための\t->環境が\t->整備された\t->ことで\t->前進し始めた\n",
            "#  6 開始文節 : ための\t->環境が\t->整備された\t->ことで\t->前進し始めた\n",
            "#  7 開始文節 : 環境が\t->整備された\t->ことで\t->前進し始めた\n",
            "#  8 開始文節 : 整備された\t->ことで\t->前進し始めた\n",
            "#  9 開始文節 : ことで\t->前進し始めた\n",
            "# 10 開始文節 : AI関連の\t->研究が\t->前進し始めた\n",
            "# 11 開始文節 : 研究が\t->前進し始めた\n",
            "# 12 開始文節 : 再び\t->前進し始めた\n",
            "# 13 開始文節 : 大きく\t->前進し始めた\n",
            "--- Sentence 0032 ---\n",
            "#  0 開始文節 : 2010年に\t->提唱された\t->始まった\n",
            "#  1 開始文節 : 英国エコノミスト誌で\t->提唱された\t->始まった\n",
            "#  2 開始文節 : ビッグデータという\t->用語が\t->提唱された\t->始まった\n",
            "#  3 開始文節 : 用語が\t->提唱された\t->始まった\n",
            "#  4 開始文節 : 提唱された\t->始まった\n",
            "#  5 開始文節 : 同年に\t->始まった\n",
            "#  6 開始文節 : 質問応答システムの\t->ワトソンが\t->始まった\n",
            "#  7 開始文節 : ワトソンが\t->始まった\n",
            "#  8 開始文節 : クイズ番組\t->なった\t->始まった\n",
            "#  9 開始文節 : ジェパディ!」の\t->練習戦で\t->勝利し\t->なった\t->始まった\n",
            "# 10 開始文節 : 練習戦で\t->勝利し\t->なった\t->始まった\n",
            "# 11 開始文節 : 人間に\t->勝利し\t->なった\t->始まった\n",
            "# 12 開始文節 : 勝利し\t->なった\t->始まった\n",
            "# 13 開始文節 : 大きな\t->ニュースと\t->なった\t->始まった\n",
            "# 14 開始文節 : ニュースと\t->なった\t->始まった\n",
            "# 15 開始文節 : なった\t->始まった\n",
            "# 16 開始文節 : 2012年に\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 17 開始文節 : 画像処理コンテストで\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 18 開始文節 : ジェフリー・ヒントン氏の\t->チームが\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 19 開始文節 : チームが\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 20 開始文節 : 従来手法からの\t->精度改善を\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 21 開始文節 : 大幅な\t->精度改善を\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 22 開始文節 : 精度改善を\t->果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 23 開始文節 : 果たした\t->上で\t->優勝した\t->ことで\t->始まった\n",
            "# 24 開始文節 : 上で\t->優勝した\t->ことで\t->始まった\n",
            "# 25 開始文節 : 優勝した\t->ことで\t->始まった\n",
            "# 26 開始文節 : ことで\t->始まった\n",
            "# 27 開始文節 : 第三次AIブームが\t->始まった\n",
            "--- Sentence 0033 ---\n",
            "#  0 開始文節 : 2013年には\t->発表した\t->解読した\t->水準だった\n",
            "#  1 開始文節 : 国立情報学研究所や\t->富士通研究所の\t->研究チームが\t->開発した\t->東ロボくんで\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  2 開始文節 : 富士通研究所の\t->研究チームが\t->開発した\t->東ロボくんで\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  3 開始文節 : 研究チームが\t->開発した\t->東ロボくんで\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  4 開始文節 : 開発した\t->東ロボくんで\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  5 開始文節 : 東ロボくんで\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  6 開始文節 : 東京大学入試の\t->模擬試験に\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  7 開始文節 : 模擬試験に\t->挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  8 開始文節 : 挑んだと\t->発表した\t->解読した\t->水準だった\n",
            "#  9 開始文節 : 発表した\t->解読した\t->水準だった\n",
            "# 10 開始文節 : 数式の\t->計算や\t->解析にあたる\t->専用プログラムを\t->使い\t->解読した\t->水準だった\n",
            "# 11 開始文節 : 計算や\t->解析にあたる\t->専用プログラムを\t->使い\t->解読した\t->水準だった\n",
            "# 12 開始文節 : 単語の\t->解析にあたる\t->専用プログラムを\t->使い\t->解読した\t->水準だった\n",
            "# 13 開始文節 : 解析にあたる\t->専用プログラムを\t->使い\t->解読した\t->水準だった\n",
            "# 14 開始文節 : 専用プログラムを\t->使い\t->解読した\t->水準だった\n",
            "# 15 開始文節 : 使い\t->解読した\t->水準だった\n",
            "# 16 開始文節 : 実際に\t->臨んだ\t->大学入試センター試験と\t->問題を\t->解読した\t->水準だった\n",
            "# 17 開始文節 : 受験生が\t->臨んだ\t->大学入試センター試験と\t->問題を\t->解読した\t->水準だった\n",
            "# 18 開始文節 : 臨んだ\t->大学入試センター試験と\t->問題を\t->解読した\t->水準だった\n",
            "# 19 開始文節 : 大学入試センター試験と\t->問題を\t->解読した\t->水準だった\n",
            "# 20 開始文節 : 東大の\t->2次試験の\t->問題を\t->解読した\t->水準だった\n",
            "# 21 開始文節 : 2次試験の\t->問題を\t->解読した\t->水準だった\n",
            "# 22 開始文節 : 問題を\t->解読した\t->水準だった\n",
            "# 23 開始文節 : 解読した\t->水準だった\n",
            "# 24 開始文節 : 代々木ゼミナールの\t->判定では\t->水準だった\n",
            "# 25 開始文節 : 判定では\t->水準だった\n",
            "# 26 開始文節 : 東大の\t->合格は\t->難しいが\t->水準だった\n",
            "# 27 開始文節 : 合格は\t->難しいが\t->水準だった\n",
            "# 28 開始文節 : 難しいが\t->水準だった\n",
            "# 29 開始文節 : 私立大学には\t->水準だった\n",
            "# 30 開始文節 : 合格できる\t->水準だった\n",
            "--- Sentence 0034 ---\n",
            "#  0 開始文節 : 2014年には\t->提唱された\n",
            "#  1 開始文節 : 日本の\t->人工知能学者である\t->齊藤元章により\t->先立ち\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  2 開始文節 : 人工知能学者である\t->齊藤元章により\t->先立ち\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  3 開始文節 : 齊藤元章により\t->先立ち\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  4 開始文節 : 特異点に\t->先立ち\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  5 開始文節 : 先立ち\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  6 開始文節 : オートメーション化と\t->進歩により\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  7 開始文節 : コンピューター技術の\t->進歩により\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  8 開始文節 : 進歩により\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "#  9 開始文節 : 衣食住の\t->生産コストが\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "# 10 開始文節 : 生産コストが\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "# 11 開始文節 : ゼロに\t->限りなく\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "# 12 開始文節 : 限りなく\t->近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "# 13 開始文節 : 近づくという\t->プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "# 14 開始文節 : プレ・シンギュラリティという\t->概念も\t->提唱された\n",
            "# 15 開始文節 : 概念も\t->提唱された\n",
            "--- Sentence 0035 ---\n",
            "#  0 開始文節 : ジェフホーキンスが\t->続けているが\t->展開している\n",
            "#  1 開始文節 : 実現に\t->向けて\t->続けているが\t->展開している\n",
            "#  2 開始文節 : 向けて\t->続けているが\t->展開している\n",
            "#  3 開始文節 : 研究を\t->続けているが\t->展開している\n",
            "#  4 開始文節 : 続けているが\t->展開している\n",
            "#  5 開始文節 : 著書\t->考える\t->コンピューターの\t->中で\t->展開している\n",
            "#  6 開始文節 : 考える\t->コンピューターの\t->中で\t->展開している\n",
            "#  7 開始文節 : 脳考える\t->コンピューターの\t->中で\t->展開している\n",
            "#  8 開始文節 : コンピューターの\t->中で\t->展開している\n",
            "#  9 開始文節 : 中で\t->展開している\n",
            "# 10 開始文節 : 自己連想記憶理論という\t->独自の\t->理論を\t->展開している\n",
            "# 11 開始文節 : 独自の\t->理論を\t->展開している\n",
            "# 12 開始文節 : 理論を\t->展開している\n",
            "--- Sentence 0036 ---\n",
            "#  0 開始文節 : 世界各国において\t->進んでいるが\t->進行している\t->ものの\t->多い\n",
            "#  1 開始文節 : 軍事民間共に\t->進んでいるが\t->進行している\t->ものの\t->多い\n",
            "#  2 開始文節 : 実用化に\t->向け\t->進んでいるが\t->進行している\t->ものの\t->多い\n",
            "#  3 開始文節 : 向け\t->進んでいるが\t->進行している\t->ものの\t->多い\n",
            "#  4 開始文節 : 研究開発が\t->進んでいるが\t->進行している\t->ものの\t->多い\n",
            "#  5 開始文節 : 進んでいるが\t->進行している\t->ものの\t->多い\n",
            "#  6 開始文節 : とくに\t->進行している\t->ものの\t->多い\n",
            "#  7 開始文節 : 無人戦闘機や\t->無人自動車ロボットカーの\t->開発が\t->進行している\t->ものの\t->多い\n",
            "#  8 開始文節 : 無人自動車ロボットカーの\t->開発が\t->進行している\t->ものの\t->多い\n",
            "#  9 開始文節 : 開発が\t->進行している\t->ものの\t->多い\n",
            "# 10 開始文節 : 進行している\t->ものの\t->多い\n",
            "# 11 開始文節 : ものの\t->多い\n",
            "# 12 開始文節 : 2010年代には\t->利用されているが\t->多い\n",
            "# 13 開始文節 : まだ\t->完全な\t->自動化は\t->利用されているが\t->多い\n",
            "# 14 開始文節 : 完全な\t->自動化は\t->利用されているが\t->多い\n",
            "# 15 開始文節 : 自動化は\t->利用されているが\t->多い\n",
            "# 16 開始文節 : 試験的な\t->ものに\t->留まった\t->UCAVは\t->利用されているが\t->多い\n",
            "# 17 開始文節 : ものに\t->留まった\t->UCAVは\t->利用されているが\t->多い\n",
            "# 18 開始文節 : 留まった\t->UCAVは\t->利用されているが\t->多い\n",
            "# 19 開始文節 : UCAVは\t->利用されているが\t->多い\n",
            "# 20 開始文節 : 利用されているが\t->多い\n",
            "# 21 開始文節 : 一部操作は\t->多い\n",
            "# 22 開始文節 : 地上から\t->行っている\t->ものが\t->多い\n",
            "# 23 開始文節 : 行っている\t->ものが\t->多い\n",
            "# 24 開始文節 : ものが\t->多い\n",
            "--- Sentence 0037 ---\n",
            "#  0 開始文節 : ロボット向けとしては\t->登場している\t->用いている\t->行動する\n",
            "#  1 開始文節 : CSAILの\t->ロドニー・ブルックスが\t->提唱した\t->包摂アーキテクチャという\t->理論が\t->登場している\t->用いている\t->行動する\n",
            "#  2 開始文節 : ロドニー・ブルックスが\t->提唱した\t->包摂アーキテクチャという\t->理論が\t->登場している\t->用いている\t->行動する\n",
            "#  3 開始文節 : 提唱した\t->包摂アーキテクチャという\t->理論が\t->登場している\t->用いている\t->行動する\n",
            "#  4 開始文節 : 包摂アーキテクチャという\t->理論が\t->登場している\t->用いている\t->行動する\n",
            "#  5 開始文節 : 理論が\t->登場している\t->用いている\t->行動する\n",
            "#  6 開始文節 : 登場している\t->用いている\t->行動する\n",
            "#  7 開始文節 : これは\t->用いている\t->行動する\n",
            "#  8 開始文節 : 従来型の\t->知が\t->先行する\t->ものではなく\t->用いている\t->行動する\n",
            "#  9 開始文節 : 我思う\t->故に\t->ありの\t->知が\t->先行する\t->ものではなく\t->用いている\t->行動する\n",
            "# 10 開始文節 : 故に\t->ありの\t->知が\t->先行する\t->ものではなく\t->用いている\t->行動する\n",
            "# 11 開始文節 : 我\t->ありの\t->知が\t->先行する\t->ものではなく\t->用いている\t->行動する\n",
            "# 12 開始文節 : ありの\t->知が\t->先行する\t->ものではなく\t->用いている\t->行動する\n",
            "# 13 開始文節 : 知が\t->先行する\t->ものではなく\t->用いている\t->行動する\n",
            "# 14 開始文節 : 先行する\t->ものではなく\t->用いている\t->行動する\n",
            "# 15 開始文節 : ものではなく\t->用いている\t->行動する\n",
            "# 16 開始文節 : 体の\t->神経ネットワークのみを\t->用いて\t->学習する\t->行動型システムを\t->用いている\t->行動する\n",
            "# 17 開始文節 : 神経ネットワークのみを\t->用いて\t->学習する\t->行動型システムを\t->用いている\t->行動する\n",
            "# 18 開始文節 : 用いて\t->学習する\t->行動型システムを\t->用いている\t->行動する\n",
            "# 19 開始文節 : 環境から\t->学習する\t->行動型システムを\t->用いている\t->行動する\n",
            "# 20 開始文節 : 学習する\t->行動型システムを\t->用いている\t->行動する\n",
            "# 21 開始文節 : 行動型システムを\t->用いている\t->行動する\n",
            "# 22 開始文節 : 用いている\t->行動する\n",
            "# 23 開始文節 : これに\t->基づいた\t->ゲンギスと\t->呼ばれる\t->六本足の\t->ロボットは\t->行動する\n",
            "# 24 開始文節 : 基づいた\t->ゲンギスと\t->呼ばれる\t->六本足の\t->ロボットは\t->行動する\n",
            "# 25 開始文節 : ゲンギスと\t->呼ばれる\t->六本足の\t->ロボットは\t->行動する\n",
            "# 26 開始文節 : 呼ばれる\t->六本足の\t->ロボットは\t->行動する\n",
            "# 27 開始文節 : 六本足の\t->ロボットは\t->行動する\n",
            "# 28 開始文節 : ロボットは\t->行動する\n",
            "# 29 開始文節 : いわゆる\t->脳を\t->持たないにも\t->関わらず\t->行動する\n",
            "# 30 開始文節 : 脳を\t->持たないにも\t->関わらず\t->行動する\n",
            "# 31 開始文節 : 持たないにも\t->関わらず\t->行動する\n",
            "# 32 開始文節 : 関わらず\t->行動する\n",
            "# 33 開始文節 : まるで\t->生きているかの\t->ように\t->行動する\n",
            "# 34 開始文節 : 生きているかの\t->ように\t->行動する\n",
            "# 35 開始文節 : ように\t->行動する\n",
            "--- Sentence 0038 ---\n",
            "#  0 開始文節 : 2015年\t->10月に\t->作成した\t->AlphaGoが\t->注目され\t->進められている\n",
            "#  1 開始文節 : 10月に\t->作成した\t->AlphaGoが\t->注目され\t->進められている\n",
            "#  2 開始文節 : 米DeepMind社が\t->作成した\t->AlphaGoが\t->注目され\t->進められている\n",
            "#  3 開始文節 : 作成した\t->AlphaGoが\t->注目され\t->進められている\n",
            "#  4 開始文節 : AlphaGoが\t->注目され\t->進められている\n",
            "#  5 開始文節 : 人間の\t->プロ囲碁棋士に\t->勝利して以降は\t->注目され\t->進められている\n",
            "#  6 開始文節 : プロ囲碁棋士に\t->勝利して以降は\t->注目され\t->進められている\n",
            "#  7 開始文節 : 勝利して以降は\t->注目され\t->進められている\n",
            "#  8 開始文節 : ディープラーニングと\t->呼ばれる\t->手法が\t->注目され\t->進められている\n",
            "#  9 開始文節 : 呼ばれる\t->手法が\t->注目され\t->進められている\n",
            "# 10 開始文節 : 手法が\t->注目され\t->進められている\n",
            "# 11 開始文節 : 注目され\t->進められている\n",
            "# 12 開始文節 : 人工知能自体の\t->研究の\t->他にも\t->影響についても\t->進められている\n",
            "# 13 開始文節 : 研究の\t->他にも\t->影響についても\t->進められている\n",
            "# 14 開始文節 : 他にも\t->影響についても\t->進められている\n",
            "# 15 開始文節 : 人工知能が\t->与える\t->影響についても\t->進められている\n",
            "# 16 開始文節 : 雇用などに\t->与える\t->影響についても\t->進められている\n",
            "# 17 開始文節 : 与える\t->影響についても\t->進められている\n",
            "# 18 開始文節 : 影響についても\t->進められている\n",
            "# 19 開始文節 : 研究が\t->進められている\n",
            "--- Sentence 0039 ---\n",
            "#  0 開始文節 : 2016年\t->10月\t->開発\t->解決した\n",
            "#  1 開始文節 : 10月\t->開発\t->解決した\n",
            "#  2 開始文節 : DeepMindが\t->発表し\t->開発\t->解決した\n",
            "#  3 開始文節 : 入力された\t->情報の\t->関連性を\t->導き出し\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "#  4 開始文節 : 情報の\t->関連性を\t->導き出し\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "#  5 開始文節 : 関連性を\t->導き出し\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "#  6 開始文節 : 導き出し\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "#  7 開始文節 : 仮説に\t->近い\t->ものを\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "#  8 開始文節 : 近い\t->ものを\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "#  9 開始文節 : ものを\t->導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "# 10 開始文節 : 導き出す\t->人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "# 11 開始文節 : 人工知能技術\t->ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "# 12 開始文節 : ディファレンシャブル・ニューラル・コンピューターを\t->発表し\t->開発\t->解決した\n",
            "# 13 開始文節 : 発表し\t->開発\t->解決した\n",
            "# 14 開始文節 : 同年\t->11月\t->開発\t->解決した\n",
            "# 15 開始文節 : 11月\t->開発\t->解決した\n",
            "# 16 開始文節 : 大量の\t->データが\t->可能にする\t->深層学習システムを\t->開発\t->解決した\n",
            "# 17 開始文節 : データが\t->可能にする\t->深層学習システムを\t->開発\t->解決した\n",
            "# 18 開始文節 : 不要の\t->ワンショット学習を\t->可能にする\t->深層学習システムを\t->開発\t->解決した\n",
            "# 19 開始文節 : ワンショット学習を\t->可能にする\t->深層学習システムを\t->開発\t->解決した\n",
            "# 20 開始文節 : 可能にする\t->深層学習システムを\t->開発\t->解決した\n",
            "# 21 開始文節 : 深層学習システムを\t->開発\t->解決した\n",
            "# 22 開始文節 : 翌2017年\t->6月\t->開発\t->解決した\n",
            "# 23 開始文節 : 6月\t->開発\t->解決した\n",
            "# 24 開始文節 : 関係推論のような\t->人間並みの\t->認識能力を\t->持つ\t->システムを\t->開発\t->解決した\n",
            "# 25 開始文節 : 人間並みの\t->認識能力を\t->持つ\t->システムを\t->開発\t->解決した\n",
            "# 26 開始文節 : 認識能力を\t->持つ\t->システムを\t->開発\t->解決した\n",
            "# 27 開始文節 : 持つ\t->システムを\t->開発\t->解決した\n",
            "# 28 開始文節 : システムを\t->開発\t->解決した\n",
            "# 29 開始文節 : 開発\t->解決した\n",
            "# 30 開始文節 : 2017年\t->8月には\t->解決した\n",
            "# 31 開始文節 : 8月には\t->解決した\n",
            "# 32 開始文節 : 記号接地問題(シンボルグラウンディング問題)を\t->解決した\n",
            "--- Sentence 0040 ---\n",
            "#  0 開始文節 : 2006年の\t->ディープラーニングの\t->発明と\t->整備\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  1 開始文節 : ディープラーニングの\t->発明と\t->整備\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  2 開始文節 : 発明と\t->整備\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  3 開始文節 : 2010年以降の\t->ビッグデータ収集環境の\t->整備\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  4 開始文節 : ビッグデータ収集環境の\t->整備\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  5 開始文節 : 整備\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  6 開始文節 : 計算資源と\t->なる\t->GPUの\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  7 開始文節 : なる\t->GPUの\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  8 開始文節 : GPUの\t->高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "#  9 開始文節 : 高性能化により\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 10 開始文節 : 2012年に\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 11 開始文節 : ディープラーニングが\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 12 開始文節 : 画像処理コンテストで\t->付けて\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 13 開始文節 : 他の\t->手法に\t->付けて\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 14 開始文節 : 手法に\t->付けて\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 15 開始文節 : 圧倒的大差を\t->付けて\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 16 開始文節 : 付けて\t->優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 17 開始文節 : 優勝した\t->ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 18 開始文節 : ことで\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 19 開始文節 : 技術的特異点という\t->概念は\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 20 開始文節 : 概念は\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 21 開始文節 : 急速に\t->集め\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 22 開始文節 : 世界中の\t->識者の\t->注目を\t->集め\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 23 開始文節 : 識者の\t->注目を\t->集め\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 24 開始文節 : 注目を\t->集め\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 25 開始文節 : 集め\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 26 開始文節 : 現実味を\t->持って\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 27 開始文節 : 持って\t->受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 28 開始文節 : 受け止められるようになった\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 29 開始文節 : ディープラーニングの\t->発明と\t->普及を\t->受けて\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 30 開始文節 : 発明と\t->普及を\t->受けて\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 31 開始文節 : 急速な\t->普及を\t->受けて\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 32 開始文節 : 普及を\t->受けて\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 33 開始文節 : 受けて\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 34 開始文節 : 研究開発の\t->現場においては\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 35 開始文節 : 現場においては\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 36 開始文節 : デミス・ハサビス\t->率いる\t->DeepMindを\t->筆頭に\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 37 開始文節 : 率いる\t->DeepMindを\t->筆頭に\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 38 開始文節 : DeepMindを\t->筆頭に\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 39 開始文節 : 筆頭に\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 40 開始文節 : Vicarious\t->IBMCorticalLearningCenter\t->全脳アーキテクチャ\t->PEZYComputing\t->OpenCog\t->GoodAI\t->nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 41 開始文節 : IBMCorticalLearningCenter\t->全脳アーキテクチャ\t->PEZYComputing\t->OpenCog\t->GoodAI\t->nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 42 開始文節 : 全脳アーキテクチャ\t->PEZYComputing\t->OpenCog\t->GoodAI\t->nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 43 開始文節 : PEZYComputing\t->OpenCog\t->GoodAI\t->nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 44 開始文節 : OpenCog\t->GoodAI\t->nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 45 開始文節 : GoodAI\t->nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 46 開始文節 : nnaisense\t->IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 47 開始文節 : IBMSyNAPSE等\t->汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 48 開始文節 : 汎用人工知能\t->AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 49 開始文節 : AGIを\t->開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 50 開始文節 : 開発する\t->プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 51 開始文節 : プロジェクトが\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 52 開始文節 : 数多く\t->立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 53 開始文節 : 立ち上げられている\t->されている\t->提唱され始めている\n",
            "# 54 開始文節 : これらの\t->研究開発の\t->現場では\t->されている\t->提唱され始めている\n",
            "# 55 開始文節 : 研究開発の\t->現場では\t->されている\t->提唱され始めている\n",
            "# 56 開始文節 : 現場では\t->されている\t->提唱され始めている\n",
            "# 57 開始文節 : 脳を\t->リバースエンジニアリングして\t->構築された\t->機械学習を\t->組み合わせる\t->アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 58 開始文節 : リバースエンジニアリングして\t->構築された\t->機械学習を\t->組み合わせる\t->アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 59 開始文節 : 構築された\t->機械学習を\t->組み合わせる\t->アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 60 開始文節 : 神経科学と\t->機械学習を\t->組み合わせる\t->アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 61 開始文節 : 機械学習を\t->組み合わせる\t->アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 62 開始文節 : 組み合わせる\t->アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 63 開始文節 : アプローチが\t->有望と\t->されている\t->提唱され始めている\n",
            "# 64 開始文節 : 有望と\t->されている\t->提唱され始めている\n",
            "# 65 開始文節 : されている\t->提唱され始めている\n",
            "# 66 開始文節 : 結果として\t->提唱され始めている\n",
            "# 67 開始文節 : HierarchicalTemporalMemory(HTM)理論\t->ComplementaryLearningSystems(CLS)理論の\t->更新版等\t->タスクのみを\t->扱う\t->ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 68 開始文節 : ComplementaryLearningSystems(CLS)理論の\t->更新版等\t->タスクのみを\t->扱う\t->ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 69 開始文節 : 更新版等\t->タスクのみを\t->扱う\t->ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 70 開始文節 : 単一の\t->タスクのみを\t->扱う\t->ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 71 開始文節 : タスクのみを\t->扱う\t->ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 72 開始文節 : 扱う\t->ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 73 開始文節 : ディープラーニングから\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 74 開始文節 : 更に\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 75 開始文節 : 一歩\t->進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 76 開始文節 : 進んだ\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 77 開始文節 : 複数の\t->タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 78 開始文節 : タスクを\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 79 開始文節 : 同時に\t->扱う\t->理論が\t->提唱され始めている\n",
            "# 80 開始文節 : 扱う\t->理論が\t->提唱され始めている\n",
            "# 81 開始文節 : 理論が\t->提唱され始めている\n",
            "--- Sentence 0041 ---\n",
            "#  0 開始文節 : 3Dゲームのような\t->仮想空間で\t->動かし\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  1 開始文節 : 仮想空間で\t->動かし\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  2 開始文節 : モデルを\t->動かし\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  3 開始文節 : 動かし\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  4 開始文節 : 現実世界の\t->ことを\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  5 開始文節 : ことを\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  6 開始文節 : 高速に\t->学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  7 開始文節 : 学ばせるといった\t->ことも\t->上げている\t->学習\n",
            "#  8 開始文節 : ことも\t->上げている\t->学習\n",
            "#  9 開始文節 : 大きな\t->成果を\t->上げている\t->学習\n",
            "# 10 開始文節 : 成果を\t->上げている\t->学習\n",
            "# 11 開始文節 : 上げている\t->学習\n",
            "# 12 開始文節 : シミュレーションによる\t->学習\n",
            "--- Sentence 0042 ---\n",
            "#  0 開始文節 : また\t->いる\n",
            "#  1 開始文節 : 数は\t->少ないが\t->不可能と\t->考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  2 開始文節 : 少ないが\t->不可能と\t->考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  3 開始文節 : AGIだけでは\t->不可能と\t->考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  4 開始文節 : 知能の\t->再現は\t->不可能と\t->考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  5 開始文節 : 再現は\t->不可能と\t->考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  6 開始文節 : 不可能と\t->考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  7 開始文節 : 考えて\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  8 開始文節 : 身体知を\t->再現する\t->ために\t->必要だと\t->する\t->研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "#  9 開始文節 : 再現する\t->ために\t->必要だと\t->する\t->研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 10 開始文節 : ために\t->必要だと\t->する\t->研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 11 開始文節 : 全人体シミュレーションが\t->必要だと\t->する\t->研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 12 開始文節 : 必要だと\t->する\t->研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 13 開始文節 : する\t->研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 14 開始文節 : 研究者やより\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 15 開始文節 : 生物に\t->近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 16 開始文節 : 近い\t->振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 17 開始文節 : 振る舞いを\t->見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 18 開始文節 : 見せる\t->AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 19 開始文節 : AL\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 20 開始文節 : 人工生命の\t->作成に\t->挑む\t->研究者\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 21 開始文節 : 作成に\t->挑む\t->研究者\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 22 開始文節 : 挑む\t->研究者\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 23 開始文節 : 研究者\t->知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 24 開始文節 : 知能と\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 25 開始文節 : 密接な\t->関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 26 開始文節 : 関係に\t->あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 27 開始文節 : あると\t->思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 28 開始文節 : 思われる\t->意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 29 開始文節 : 意識の\t->デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 30 開始文節 : デジタル的再現\t->人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 31 開始文節 : 人工意識に\t->挑戦する\t->研究者も\t->いる\n",
            "# 32 開始文節 : 挑戦する\t->研究者も\t->いる\n",
            "# 33 開始文節 : 研究者も\t->いる\n",
            "--- Sentence 0043 ---\n",
            "#  0 開始文節 : リーズナブルな\t->コストで\t->入るようになった\t->ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  1 開始文節 : コストで\t->入るようになった\t->ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  2 開始文節 : 大量の\t->計算リソースが\t->入るようになった\t->ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  3 開始文節 : 計算リソースが\t->入るようになった\t->ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  4 開始文節 : 手に\t->入るようになった\t->ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  5 開始文節 : 入るようになった\t->ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  6 開始文節 : ことで\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  7 開始文節 : ビッグデータが\t->出現し\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  8 開始文節 : 出現し\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "#  9 開始文節 : 企業が\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 10 開始文節 : 膨大な\t->データの\t->活用に\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 11 開始文節 : データの\t->活用に\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 12 開始文節 : 活用に\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 13 開始文節 : 極めて\t->強い\t->関心を\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 14 開始文節 : 強い\t->関心を\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 15 開始文節 : 関心を\t->寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 16 開始文節 : 寄せており\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 17 開始文節 : 全世界的に\t->行って\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 18 開始文節 : 民間企業主導で\t->行って\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 19 開始文節 : 莫大な\t->投資を\t->行って\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 20 開始文節 : 投資を\t->行って\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 21 開始文節 : 行って\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 22 開始文節 : 人工知能に関する\t->研究開発競争が\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 23 開始文節 : 研究開発競争が\t->展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 24 開始文節 : 展開されている\t->整備され始めている\t->目立つようになった\n",
            "# 25 開始文節 : また\t->整備され始めている\t->目立つようになった\n",
            "# 26 開始文節 : 2011年の\t->D-WaveSystemsによる\t->量子アニーリング方式の\t->製品化を\t->嚆矢として\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 27 開始文節 : D-WaveSystemsによる\t->量子アニーリング方式の\t->製品化を\t->嚆矢として\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 28 開始文節 : 量子アニーリング方式の\t->製品化を\t->嚆矢として\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 29 開始文節 : 製品化を\t->嚆矢として\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 30 開始文節 : 嚆矢として\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 31 開始文節 : 量子コンピュータという\t->超並列処理が\t->可能な\t->次世代の\t->ITインフラが\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 32 開始文節 : 超並列処理が\t->可能な\t->次世代の\t->ITインフラが\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 33 開始文節 : 可能な\t->次世代の\t->ITインフラが\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 34 開始文節 : 次世代の\t->ITインフラが\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 35 開始文節 : ITインフラが\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 36 開始文節 : 急速に\t->実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 37 開始文節 : 実用化され始めた\t->事で\t->整備され始めている\t->目立つようになった\n",
            "# 38 開始文節 : 事で\t->整備され始めている\t->目立つようになった\n",
            "# 39 開始文節 : 人工知能の\t->高速化にも\t->関わる\t->組み合わせ最適化問題を\t->解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 40 開始文節 : 高速化にも\t->関わる\t->組み合わせ最適化問題を\t->解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 41 開始文節 : 深く\t->関わる\t->組み合わせ最適化問題を\t->解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 42 開始文節 : 関わる\t->組み合わせ最適化問題を\t->解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 43 開始文節 : 組み合わせ最適化問題を\t->解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 44 開始文節 : リアルタイムに\t->解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 45 開始文節 : 解決できる\t->環境が\t->整備され始めている\t->目立つようになった\n",
            "# 46 開始文節 : 環境が\t->整備され始めている\t->目立つようになった\n",
            "# 47 開始文節 : 整備され始めている\t->目立つようになった\n",
            "# 48 開始文節 : この\t->動向を\t->受ける\t->形で\t->目立つようになった\n",
            "# 49 開始文節 : 動向を\t->受ける\t->形で\t->目立つようになった\n",
            "# 50 開始文節 : 受ける\t->形で\t->目立つようになった\n",
            "# 51 開始文節 : 形で\t->目立つようになった\n",
            "# 52 開始文節 : 2016年頃から\t->目立つようになった\n",
            "# 53 開始文節 : 一般向けの\t->ニュース番組でも\t->目立つようになった\n",
            "# 54 開始文節 : ニュース番組でも\t->目立つようになった\n",
            "# 55 開始文節 : 人工知能の\t->量子コンピュータに関する\t->報道が\t->目立つようになった\n",
            "# 56 開始文節 : 研究開発や\t->量子コンピュータに関する\t->報道が\t->目立つようになった\n",
            "# 57 開始文節 : 新しい\t->量子コンピュータに関する\t->報道が\t->目立つようになった\n",
            "# 58 開始文節 : サービス展開や\t->量子コンピュータに関する\t->報道が\t->目立つようになった\n",
            "# 59 開始文節 : 量子コンピュータに関する\t->報道が\t->目立つようになった\n",
            "# 60 開始文節 : 報道が\t->目立つようになった\n",
            "--- Sentence 0044 ---\n",
            "#  0 開始文節 : 2017年には\t->なった\t->予測されている\n",
            "#  1 開始文節 : イーロン・マスクが\t->取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  2 開始文節 : 急速に\t->進化し続ける\t->人工知能に対して\t->取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  3 開始文節 : 進化し続ける\t->人工知能に対して\t->取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  4 開始文節 : 人工知能に対して\t->取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  5 開始文節 : 人間が\t->取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  6 開始文節 : 遅れを\t->取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  7 開始文節 : 取らないようにする\t->ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  8 開始文節 : ために\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "#  9 開始文節 : 人間の\t->脳を\t->接続する\t->ブレイン・マシン・インターフェースを\t->研究開発する\t->ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 10 開始文節 : 脳を\t->接続する\t->ブレイン・マシン・インターフェースを\t->研究開発する\t->ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 11 開始文節 : 機械に\t->接続する\t->ブレイン・マシン・インターフェースを\t->研究開発する\t->ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 12 開始文節 : 接続する\t->ブレイン・マシン・インターフェースを\t->研究開発する\t->ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 13 開始文節 : ブレイン・マシン・インターフェースを\t->研究開発する\t->ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 14 開始文節 : 研究開発する\t->ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 15 開始文節 : ニューラ・リンク社を\t->立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 16 開始文節 : 立ち上げていた\t->ことを\t->公表し\t->なった\t->予測されている\n",
            "# 17 開始文節 : ことを\t->公表し\t->なった\t->予測されている\n",
            "# 18 開始文節 : 公表し\t->なった\t->予測されている\n",
            "# 19 開始文節 : 世界中で\t->なった\t->予測されている\n",
            "# 20 開始文節 : 話題に\t->なった\t->予測されている\n",
            "# 21 開始文節 : なった\t->予測されている\n",
            "# 22 開始文節 : ブレイン・マシン・インターフェースにより\t->予測されている\n",
            "# 23 開始文節 : 人の\t->インターネットが\t->出現する\t->事が\t->予測されている\n",
            "# 24 開始文節 : インターネットが\t->出現する\t->事が\t->予測されている\n",
            "# 25 開始文節 : 出現する\t->事が\t->予測されている\n",
            "# 26 開始文節 : 事が\t->予測されている\n",
            "--- Sentence 0045 ---\n",
            "#  0 開始文節 : 2017年\t->10月には\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  1 開始文節 : 10月には\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  2 開始文節 : ジェフリーヒントンにより\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  3 開始文節 : 要素間の\t->位置関係まで\t->含めて\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  4 開始文節 : 相対的な\t->位置関係まで\t->含めて\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  5 開始文節 : 位置関係まで\t->含めて\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  6 開始文節 : 含めて\t->学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  7 開始文節 : 学習できる\t->カプセルネットワークが\t->提唱された\n",
            "#  8 開始文節 : カプセルネットワークが\t->提唱された\n",
            "--- Sentence 0046 ---\n",
            "#  0 開始文節 : 2018年\t->3月\t->16日の\t->国際大学GLOCOMの\t->提言に\t->よると\t->分析されている\n",
            "#  1 開始文節 : 3月\t->16日の\t->国際大学GLOCOMの\t->提言に\t->よると\t->分析されている\n",
            "#  2 開始文節 : 16日の\t->国際大学GLOCOMの\t->提言に\t->よると\t->分析されている\n",
            "#  3 開始文節 : 国際大学GLOCOMの\t->提言に\t->よると\t->分析されている\n",
            "#  4 開始文節 : 提言に\t->よると\t->分析されている\n",
            "#  5 開始文節 : よると\t->分析されている\n",
            "#  6 開始文節 : 課題解決型の\t->AIを\t->活用する\t->事で\t->寄与できると\t->分析されている\n",
            "#  7 開始文節 : AIを\t->活用する\t->事で\t->寄与できると\t->分析されている\n",
            "#  8 開始文節 : 活用する\t->事で\t->寄与できると\t->分析されている\n",
            "#  9 開始文節 : 事で\t->寄与できると\t->分析されている\n",
            "# 10 開始文節 : 社会変革に\t->寄与できると\t->分析されている\n",
            "# 11 開始文節 : 寄与できると\t->分析されている\n",
            "--- Sentence 0047 ---\n",
            "#  0 開始文節 : 2018年\t->8月\t->公表\t->いう\n",
            "#  1 開始文節 : 8月\t->公表\t->いう\n",
            "#  2 開始文節 : OpenAIが\t->実装し\t->行う\t->AIを\t->公表\t->いう\n",
            "#  3 開始文節 : 好奇心を\t->実装し\t->行う\t->AIを\t->公表\t->いう\n",
            "#  4 開始文節 : 実装し\t->行う\t->AIを\t->公表\t->いう\n",
            "#  5 開始文節 : ノーゲームスコア\t->ノーゴール\t->無報酬で\t->行う\t->AIを\t->公表\t->いう\n",
            "#  6 開始文節 : ノーゴール\t->無報酬で\t->行う\t->AIを\t->公表\t->いう\n",
            "#  7 開始文節 : 無報酬で\t->行う\t->AIを\t->公表\t->いう\n",
            "#  8 開始文節 : 目的\t->なき\t->探索を\t->行う\t->AIを\t->公表\t->いう\n",
            "#  9 開始文節 : なき\t->探索を\t->行う\t->AIを\t->公表\t->いう\n",
            "# 10 開始文節 : 探索を\t->行う\t->AIを\t->公表\t->いう\n",
            "# 11 開始文節 : 行う\t->AIを\t->公表\t->いう\n",
            "# 12 開始文節 : AIを\t->公表\t->いう\n",
            "# 13 開始文節 : 公表\t->いう\n",
            "# 14 開始文節 : これまでの\t->AIで\t->人間らしいと\t->いう\n",
            "# 15 開始文節 : AIで\t->人間らしいと\t->いう\n",
            "# 16 開始文節 : 最も\t->人間らしいと\t->いう\n",
            "# 17 開始文節 : 人間らしいと\t->いう\n",
            "--- Sentence 0048 ---\n",
            "#  0 開始文節 : 2018年\t->9月\t->開発した\n",
            "#  1 開始文節 : 9月\t->開発した\n",
            "#  2 開始文節 : MITリンカーン研究所は\t->開発した\n",
            "#  3 開始文節 : 従来ブラックボックスであった\t->ニューラルネットワークの\t->推論を\t->識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "#  4 開始文節 : ニューラルネットワークの\t->推論を\t->識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "#  5 開始文節 : 推論を\t->識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "#  6 開始文節 : どのような\t->段階を\t->経て\t->識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "#  7 開始文節 : 段階を\t->経て\t->識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "#  8 開始文節 : 経て\t->識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "#  9 開始文節 : 識別したのかが\t->分かる\t->アーキテクチャを\t->開発した\n",
            "# 10 開始文節 : 明確に\t->分かる\t->アーキテクチャを\t->開発した\n",
            "# 11 開始文節 : 分かる\t->アーキテクチャを\t->開発した\n",
            "# 12 開始文節 : アーキテクチャを\t->開発した\n",
            "--- Sentence 0049 ---\n",
            "#  0 開始文節 : 2019年に\t->入ると\t->されてきた\t->言語処理において\t->あり\t->至った\t->)\n",
            "#  1 開始文節 : 入ると\t->されてきた\t->言語処理において\t->あり\t->至った\t->)\n",
            "#  2 開始文節 : これまで\t->されてきた\t->言語処理において\t->あり\t->至った\t->)\n",
            "#  3 開始文節 : 深層学習では\t->困難と\t->されてきた\t->言語処理において\t->あり\t->至った\t->)\n",
            "#  4 開始文節 : 困難と\t->されてきた\t->言語処理において\t->あり\t->至った\t->)\n",
            "#  5 開始文節 : されてきた\t->言語処理において\t->あり\t->至った\t->)\n",
            "#  6 開始文節 : 言語処理において\t->あり\t->至った\t->)\n",
            "#  7 開始文節 : 大きな\t->進展が\t->あり\t->至った\t->)\n",
            "#  8 開始文節 : 進展が\t->あり\t->至った\t->)\n",
            "#  9 開始文節 : あり\t->至った\t->)\n",
            "# 10 開始文節 : Wikipediaなどを\t->使用した\t->読解テストで\t->上回るに\t->至った\t->)\n",
            "# 11 開始文節 : 使用した\t->読解テストで\t->上回るに\t->至った\t->)\n",
            "# 12 開始文節 : 読解テストで\t->上回るに\t->至った\t->)\n",
            "# 13 開始文節 : 人間を\t->上回るに\t->至った\t->)\n",
            "# 14 開始文節 : 上回るに\t->至った\t->)\n",
            "# 15 開始文節 : 至った\t->)\n",
            "# 16 開始文節 : (BERT\t->ROBERT\t->)\n",
            "# 17 開始文節 : ROBERT\t->)\n",
            "--- Sentence 0050 ---\n",
            "#  0 開始文節 : アメリカでは\t->発表\n",
            "#  1 開始文節 : 2013年に\t->発表\n",
            "#  2 開始文節 : 時の\t->大統領バラク・オバマが\t->発表\n",
            "#  3 開始文節 : 大統領バラク・オバマが\t->発表\n",
            "#  4 開始文節 : 脳研究プロジェクト\t->を\t->発表\n",
            "#  5 開始文節 : を\t->発表\n",
            "--- Sentence 0051 ---\n",
            "#  0 開始文節 : Googleは\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  1 開始文節 : アレン脳科学研究所と\t->連携し\t->生まれた\t->データを\t->処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  2 開始文節 : 連携し\t->生まれた\t->データを\t->処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  3 開始文節 : 脳スキャンによって\t->生まれた\t->データを\t->処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  4 開始文節 : 生まれた\t->データを\t->処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  5 開始文節 : 大量の\t->データを\t->処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  6 開始文節 : データを\t->処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  7 開始文節 : 処理する\t->ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  8 開始文節 : ための\t->ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "#  9 開始文節 : ソフトウェアを\t->開発している\t->達していると\t->いう\t->行っている\n",
            "# 10 開始文節 : 開発している\t->達していると\t->いう\t->行っている\n",
            "# 11 開始文節 : 2016年の\t->時点で\t->達していると\t->いう\t->行っている\n",
            "# 12 開始文節 : 時点で\t->達していると\t->いう\t->行っている\n",
            "# 13 開始文節 : Googleが\t->管理している\t->Brainmapの\t->データ量は\t->達していると\t->いう\t->行っている\n",
            "# 14 開始文節 : 管理している\t->Brainmapの\t->データ量は\t->達していると\t->いう\t->行っている\n",
            "# 15 開始文節 : Brainmapの\t->データ量は\t->達していると\t->いう\t->行っている\n",
            "# 16 開始文節 : データ量は\t->達していると\t->いう\t->行っている\n",
            "# 17 開始文節 : すでに\t->達していると\t->いう\t->行っている\n",
            "# 18 開始文節 : 1Zettaバイトに\t->達していると\t->いう\t->行っている\n",
            "# 19 開始文節 : 達していると\t->いう\t->行っている\n",
            "# 20 開始文節 : いう\t->行っている\n",
            "# 21 開始文節 : Googleは\t->始めており\t->行っている\n",
            "# 22 開始文節 : ドイツの\t->マックスプランク研究所とも\t->始めており\t->行っている\n",
            "# 23 開始文節 : マックスプランク研究所とも\t->始めており\t->行っている\n",
            "# 24 開始文節 : 共同研究を\t->始めており\t->行っている\n",
            "# 25 開始文節 : 始めており\t->行っている\n",
            "# 26 開始文節 : 脳の\t->電子顕微鏡写真から\t->再構成するという\t->研究を\t->行っている\n",
            "# 27 開始文節 : 電子顕微鏡写真から\t->再構成するという\t->研究を\t->行っている\n",
            "# 28 開始文節 : 神経回路を\t->再構成するという\t->研究を\t->行っている\n",
            "# 29 開始文節 : 再構成するという\t->研究を\t->行っている\n",
            "# 30 開始文節 : 研究を\t->行っている\n",
            "--- Sentence 0052 ---\n",
            "#  0 開始文節 : 中国では\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  1 開始文節 : 2016年の\t->第13次5カ年計画から\t->位置づけ\t->立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  2 開始文節 : 第13次5カ年計画から\t->位置づけ\t->立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  3 開始文節 : AIを\t->位置づけ\t->立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  4 開始文節 : 国家プロジェクトに\t->位置づけ\t->立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  5 開始文節 : 位置づけ\t->立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  6 開始文節 : 脳研究プロジェクトとしても\t->立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  7 開始文節 : 立ち上げ\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  8 開始文節 : 官民一体で\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "#  9 開始文節 : AIの\t->研究開発を\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 10 開始文節 : 研究開発を\t->推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 11 開始文節 : 推進してる\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 12 開始文節 : 中国の\t->教育機関では\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 13 開始文節 : 教育機関では\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 14 開始文節 : 18歳以下の\t->天才児を\t->集めて\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 15 開始文節 : 天才児を\t->集めて\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 16 開始文節 : 集めて\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 17 開始文節 : 公然と\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 18 開始文節 : AI兵器の\t->開発に\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 19 開始文節 : 開発に\t->投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 20 開始文節 : 投じられてもいる\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 21 開始文節 : マサチューセッツ工科大学\t->MITの\t->教授やなどに\t->よれば\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 22 開始文節 : MITの\t->教授やなどに\t->よれば\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 23 開始文節 : 教授やなどに\t->よれば\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 24 開始文節 : よれば\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 25 開始文節 : 中国では\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 26 開始文節 : プライバシー意識の\t->強い\t->欧米と\t->比較して\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 27 開始文節 : 強い\t->欧米と\t->比較して\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 28 開始文節 : 欧米と\t->比較して\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 29 開始文節 : 比較して\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 30 開始文節 : AIの\t->研究や\t->実験を\t->しやすい\t->環境に\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 31 開始文節 : 研究や\t->実験を\t->しやすい\t->環境に\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 32 開始文節 : 新技術の\t->実験を\t->しやすい\t->環境に\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 33 開始文節 : 実験を\t->しやすい\t->環境に\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 34 開始文節 : しやすい\t->環境に\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 35 開始文節 : 環境に\t->あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 36 開始文節 : あると\t->されている\t->主張している\t->される\t->取り上げた\n",
            "# 37 開始文節 : されている\t->主張している\t->される\t->取り上げた\n",
            "# 38 開始文節 : 日本で\t->推進している\t->齊藤元章も\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 39 開始文節 : スーパーコンピュータの\t->研究開発を\t->推進している\t->齊藤元章も\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 40 開始文節 : 研究開発を\t->推進している\t->齊藤元章も\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 41 開始文節 : 推進している\t->齊藤元章も\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 42 開始文節 : 齊藤元章も\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 43 開始文節 : AIの\t->開発において\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 44 開始文節 : 開発において\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 45 開始文節 : 中国が\t->リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 46 開始文節 : リードする\t->可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 47 開始文節 : 可能性を\t->主張している\t->される\t->取り上げた\n",
            "# 48 開始文節 : 主張している\t->される\t->取り上げた\n",
            "# 49 開始文節 : 世界の\t->ディープラーニング用計算機の\t->4分の3は\t->される\t->取り上げた\n",
            "# 50 開始文節 : ディープラーニング用計算機の\t->4分の3は\t->される\t->取り上げた\n",
            "# 51 開始文節 : 4分の3は\t->される\t->取り上げた\n",
            "# 52 開始文節 : 中国が\t->占めてるとも\t->される\t->取り上げた\n",
            "# 53 開始文節 : 占めてるとも\t->される\t->取り上げた\n",
            "# 54 開始文節 : される\t->取り上げた\n",
            "# 55 開始文節 : 米国政府に\t->よれば\t->取り上げた\n",
            "# 56 開始文節 : よれば\t->取り上げた\n",
            "# 57 開始文節 : 2013年から\t->なっている\t->独占している\t->取り上げた\n",
            "# 58 開始文節 : ディープラーニングに関する\t->論文数では\t->なっている\t->独占している\t->取り上げた\n",
            "# 59 開始文節 : 論文数では\t->なっている\t->独占している\t->取り上げた\n",
            "# 60 開始文節 : 中国が\t->なっている\t->独占している\t->取り上げた\n",
            "# 61 開始文節 : 米国を\t->超えて\t->なっている\t->独占している\t->取り上げた\n",
            "# 62 開始文節 : 超えて\t->なっている\t->独占している\t->取り上げた\n",
            "# 63 開始文節 : 世界一と\t->なっている\t->独占している\t->取り上げた\n",
            "# 64 開始文節 : なっている\t->独占している\t->取り上げた\n",
            "# 65 開始文節 : やなど\t->大会でも\t->独占している\t->取り上げた\n",
            "# 66 開始文節 : AIの\t->大会でも\t->独占している\t->取り上げた\n",
            "# 67 開始文節 : 世界的な\t->大会でも\t->独占している\t->取り上げた\n",
            "# 68 開始文節 : 大会でも\t->独占している\t->取り上げた\n",
            "# 69 開始文節 : 中国勢が\t->独占している\t->取り上げた\n",
            "# 70 開始文節 : 上位を\t->独占している\t->取り上げた\n",
            "# 71 開始文節 : 独占している\t->取り上げた\n",
            "# 72 開始文節 : 大手AI企業Google\t->マイクロソフト\t->アップルなどの\t->幹部でも\t->あった\t->台湾系アメリカ人科学者のは\t->取り上げた\n",
            "# 73 開始文節 : マイクロソフト\t->アップルなどの\t->幹部でも\t->あった\t->台湾系アメリカ人科学者のは\t->取り上げた\n",
            "# 74 開始文節 : アップルなどの\t->幹部でも\t->あった\t->台湾系アメリカ人科学者のは\t->取り上げた\n",
            "# 75 開始文節 : 幹部でも\t->あった\t->台湾系アメリカ人科学者のは\t->取り上げた\n",
            "# 76 開始文節 : あった\t->台湾系アメリカ人科学者のは\t->取り上げた\n",
            "# 77 開始文節 : 台湾系アメリカ人科学者のは\t->取り上げた\n",
            "# 78 開始文節 : 中国が\t->握りつつあると\t->する\t->を\t->著して\t->取り上げた\n",
            "# 79 開始文節 : AIで\t->握りつつあると\t->する\t->を\t->著して\t->取り上げた\n",
            "# 80 開始文節 : 覇権を\t->握りつつあると\t->する\t->を\t->著して\t->取り上げた\n",
            "# 81 開始文節 : 握りつつあると\t->する\t->を\t->著して\t->取り上げた\n",
            "# 82 開始文節 : する\t->を\t->著して\t->取り上げた\n",
            "# 83 開始文節 : を\t->著して\t->取り上げた\n",
            "# 84 開始文節 : 著して\t->取り上げた\n",
            "# 85 開始文節 : アメリカの\t->メディアなどが\t->取り上げた\n",
            "# 86 開始文節 : 政界や\t->メディアなどが\t->取り上げた\n",
            "# 87 開始文節 : メディアなどが\t->取り上げた\n",
            "--- Sentence 0053 ---\n",
            "#  0 開始文節 : フランス大統領エマニュエル・マクロンは\t->招致した\t->決定されている\n",
            "#  1 開始文節 : AI分野の\t->開発支援に\t->向け\t->支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  2 開始文節 : 開発支援に\t->向け\t->支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  3 開始文節 : 向け\t->支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  4 開始文節 : 5年で\t->支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  5 開始文節 : 15億ドル\t->約1600億円を\t->支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  6 開始文節 : 約1600億円を\t->支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  7 開始文節 : 支出すると\t->宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  8 開始文節 : 宣言し\t->開き\t->招致した\t->決定されている\n",
            "#  9 開始文節 : AI研究所を\t->開き\t->招致した\t->決定されている\n",
            "# 10 開始文節 : パリに\t->開き\t->招致した\t->決定されている\n",
            "# 11 開始文節 : 開き\t->招致した\t->決定されている\n",
            "# 12 開始文節 : フェイスブック\t->グーグル\t->サムスン\t->DeepMind\t->富士通などを\t->招致した\t->決定されている\n",
            "# 13 開始文節 : グーグル\t->サムスン\t->DeepMind\t->富士通などを\t->招致した\t->決定されている\n",
            "# 14 開始文節 : サムスン\t->DeepMind\t->富士通などを\t->招致した\t->決定されている\n",
            "# 15 開始文節 : DeepMind\t->富士通などを\t->招致した\t->決定されている\n",
            "# 16 開始文節 : 富士通などを\t->招致した\t->決定されている\n",
            "# 17 開始文節 : 招致した\t->決定されている\n",
            "# 18 開始文節 : イギリスとも\t->連携も\t->決定されている\n",
            "# 19 開始文節 : AI研究における\t->連携も\t->決定されている\n",
            "# 20 開始文節 : 長期的な\t->連携も\t->決定されている\n",
            "# 21 開始文節 : 連携も\t->決定されている\n",
            "--- Sentence 0054 ---\n",
            "#  0 開始文節 : EU全体としても\t->方向\n",
            "#  1 開始文節 : Horizon2020計画を通じて\t->方向\n",
            "#  2 開始文節 : 215億ユーロが\t->投じられる\t->方向\n",
            "#  3 開始文節 : 投じられる\t->方向\n",
            "--- Sentence 0055 ---\n",
            "#  0 開始文節 : 韓国は\t->する\t->作られた\t->いう\n",
            "#  1 開始文節 : 20億ドルを\t->2022年までに\t->する\t->作られた\t->いう\n",
            "#  2 開始文節 : 2022年までに\t->する\t->作られた\t->いう\n",
            "#  3 開始文節 : 投資を\t->する\t->作られた\t->いう\n",
            "#  4 開始文節 : する\t->作られた\t->いう\n",
            "#  5 開始文節 : 6つの\t->AI機関を\t->設立し\t->作られた\t->いう\n",
            "#  6 開始文節 : AI機関を\t->設立し\t->作られた\t->いう\n",
            "#  7 開始文節 : 設立し\t->作られた\t->いう\n",
            "#  8 開始文節 : 褒賞制度も\t->作られた\t->いう\n",
            "#  9 開始文節 : 作られた\t->いう\n",
            "# 10 開始文節 : 目標は\t->入ることだと\t->いう\n",
            "# 11 開始文節 : 2022年までに\t->入ることだと\t->いう\n",
            "# 12 開始文節 : AIの\t->世界トップ4に\t->入ることだと\t->いう\n",
            "# 13 開始文節 : 世界トップ4に\t->入ることだと\t->いう\n",
            "# 14 開始文節 : 入ることだと\t->いう\n",
            "--- Sentence 0056 ---\n",
            "#  0 開始文節 : 日経新聞調べに\t->よると\t->7位だった\n",
            "#  1 開始文節 : よると\t->7位だった\n",
            "#  2 開始文節 : 国別の\t->AI研究論文数は\t->3位インドで\t->7位だった\n",
            "#  3 開始文節 : AI研究論文数は\t->3位インドで\t->7位だった\n",
            "#  4 開始文節 : 1位\t->2位中国\t->3位インドで\t->7位だった\n",
            "#  5 開始文節 : 米国\t->2位中国\t->3位インドで\t->7位だった\n",
            "#  6 開始文節 : 2位中国\t->3位インドで\t->7位だった\n",
            "#  7 開始文節 : 3位インドで\t->7位だった\n",
            "#  8 開始文節 : 日本は\t->7位だった\n",
            "--- Sentence 0057 ---\n",
            "#  0 開始文節 : プログラミング言語は\t->使われている\n",
            "#  1 開始文節 : C++の\t->ほか\t->使われている\n",
            "#  2 開始文節 : ほか\t->使われている\n",
            "#  3 開始文節 : Pythonが\t->使われている\n",
            "#  4 開始文節 : 広く\t->使われている\n",
            "--- Sentence 0058 ---\n",
            "#  0 開始文節 : 深層学習を\t->利用するには\t->微分\t->確率統計といった\t->大学レベル以上の\t->数学知識が\t->なる\n",
            "#  1 開始文節 : 利用するには\t->微分\t->確率統計といった\t->大学レベル以上の\t->数学知識が\t->なる\n",
            "#  2 開始文節 : 微分\t->確率統計といった\t->大学レベル以上の\t->数学知識が\t->なる\n",
            "#  3 開始文節 : 線形代数\t->確率統計といった\t->大学レベル以上の\t->数学知識が\t->なる\n",
            "#  4 開始文節 : 確率統計といった\t->大学レベル以上の\t->数学知識が\t->なる\n",
            "#  5 開始文節 : 大学レベル以上の\t->数学知識が\t->なる\n",
            "#  6 開始文節 : 数学知識が\t->なる\n",
            "#  7 開始文節 : 必要と\t->なる\n",
            "--- Sentence 0059 ---\n",
            "#  0 開始文節 : 脳シミュレーションを\t->行うには\t->重要となる\n",
            "#  1 開始文節 : 行うには\t->重要となる\n",
            "#  2 開始文節 : 脳神経科学の\t->知識も\t->重要となる\n",
            "#  3 開始文節 : 知識も\t->重要となる\n",
            "--- Sentence 0060 ---\n",
            "#  0 開始文節 : 人工知能学会の\t->松尾豊は\t->いる\n",
            "#  1 開始文節 : 松尾豊は\t->いる\n",
            "#  2 開始文節 : 著書\t->否定しているが\t->いる\n",
            "#  3 開始文節 : 人工知能は\t->否定しているが\t->いる\n",
            "#  4 開始文節 : 人間を\t->超えるか内に\t->於いて\t->起こす\t->可能性を\t->否定しているが\t->いる\n",
            "#  5 開始文節 : 超えるか内に\t->於いて\t->起こす\t->可能性を\t->否定しているが\t->いる\n",
            "#  6 開始文節 : 於いて\t->起こす\t->可能性を\t->否定しているが\t->いる\n",
            "#  7 開始文節 : 人間に対して\t->起こす\t->可能性を\t->否定しているが\t->いる\n",
            "#  8 開始文節 : 反乱を\t->起こす\t->可能性を\t->否定しているが\t->いる\n",
            "#  9 開始文節 : 起こす\t->可能性を\t->否定しているが\t->いる\n",
            "# 10 開始文節 : 可能性を\t->否定しているが\t->いる\n",
            "# 11 開始文節 : 否定しているが\t->いる\n",
            "# 12 開始文節 : 人工知能の\t->危険性について\t->いる\n",
            "# 13 開始文節 : 危険性について\t->いる\n",
            "# 14 開始文節 : 警鐘を\t->鳴らしている\t->著名人も\t->いる\n",
            "# 15 開始文節 : 鳴らしている\t->著名人も\t->いる\n",
            "# 16 開始文節 : 著名人も\t->いる\n",
            "--- Sentence 0061 ---\n",
            "#  0 開始文節 : MITの\t->教授は\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  1 開始文節 : 教授は\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  2 開始文節 : 莫大な\t->資金力と\t->弾圧を\t->併せ持つ\t->中華人民共和国が\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  3 開始文節 : 資金力と\t->弾圧を\t->併せ持つ\t->中華人民共和国が\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  4 開始文節 : 人権の\t->弾圧を\t->併せ持つ\t->中華人民共和国が\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  5 開始文節 : 弾圧を\t->併せ持つ\t->中華人民共和国が\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  6 開始文節 : 併せ持つ\t->中華人民共和国が\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  7 開始文節 : 中華人民共和国が\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  8 開始文節 : 人工知能の\t->開発競争で\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#  9 開始文節 : 開発競争で\t->成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 10 開始文節 : 成功すれば\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 11 開始文節 : 民主的な\t->国家が\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 12 開始文節 : 国家が\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 13 開始文節 : 技術革新に\t->優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 14 開始文節 : 優位という\t->既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 15 開始文節 : 既成概念が\t->変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 16 開始文節 : 変わると\t->述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 17 開始文節 : 述べ\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 18 開始文節 : ディープラーニングの\t->父の\t->一人と\t->呼ばれているは\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 19 開始文節 : 父の\t->一人と\t->呼ばれているは\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 20 開始文節 : 一人と\t->呼ばれているは\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 21 開始文節 : 呼ばれているは\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 22 開始文節 : 中国が\t->利用している\t->ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 23 開始文節 : 市民の\t->政治目的で\t->利用している\t->ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 24 開始文節 : 監視や\t->政治目的で\t->利用している\t->ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 25 開始文節 : 政治目的で\t->利用している\t->ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 26 開始文節 : 人工知能を\t->利用している\t->ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 27 開始文節 : 利用している\t->ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 28 開始文節 : ことに\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 29 開始文節 : 警鐘を\t->鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 30 開始文節 : 鳴らしており\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 31 開始文節 : 海外の\t->メディアなどは\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 32 開始文節 : 人権団体や\t->メディアなどは\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 33 開始文節 : メディアなどは\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 34 開始文節 : 中国に\t->代表される\t->人工知能で\t->抑圧する\t->政治体制を\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 35 開始文節 : 代表される\t->人工知能で\t->抑圧する\t->政治体制を\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 36 開始文節 : 人工知能で\t->抑圧する\t->政治体制を\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 37 開始文節 : 人権を\t->抑圧する\t->政治体制を\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 38 開始文節 : 抑圧する\t->政治体制を\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 39 開始文節 : 政治体制を\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 40 開始文節 : デジタル権威主義\t->デジタル独裁\t->デジタル警察国家\t->デジタル全体主義\t->AI独裁と\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 41 開始文節 : デジタル独裁\t->デジタル警察国家\t->デジタル全体主義\t->AI独裁と\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 42 開始文節 : デジタル警察国家\t->デジタル全体主義\t->AI独裁と\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 43 開始文節 : デジタル全体主義\t->AI独裁と\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 44 開始文節 : AI独裁と\t->呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 45 開始文節 : 呼んだ\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 46 開始文節 : 中国では\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 47 開始文節 : ヘルメットや\t->帽子に\t->埋め込んだ\t->センサーから\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 48 開始文節 : 帽子に\t->埋め込んだ\t->センサーから\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 49 開始文節 : 埋め込んだ\t->センサーから\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 50 開始文節 : センサーから\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 51 開始文節 : 国民の\t->脳波と\t->感情を\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 52 開始文節 : 脳波と\t->感情を\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 53 開始文節 : 感情を\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 54 開始文節 : 人工知能で\t->監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 55 開始文節 : 監視する\t->政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 56 開始文節 : 政府支援の\t->プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 57 開始文節 : プロジェクトが\t->推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 58 開始文節 : 推し進められ\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 59 開始文節 : ネット検閲と\t->官僚や\t->刑務所の\t->囚人から\t->歩行者まで\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 60 開始文節 : 官僚や\t->刑務所の\t->囚人から\t->歩行者まで\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 61 開始文節 : 刑務所の\t->囚人から\t->歩行者まで\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 62 開始文節 : 囚人から\t->歩行者まで\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 63 開始文節 : 横断歩道の\t->歩行者まで\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 64 開始文節 : 歩行者まで\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 65 開始文節 : 監視を\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 66 開始文節 : 人工知能に\t->行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 67 開始文節 : 行わせ\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 68 開始文節 : 監視カメラと\t->警察の\t->ロボットに\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 69 開始文節 : 警察の\t->ロボットに\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 70 開始文節 : サングラス型スマートグラスや\t->ロボットに\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 71 開始文節 : ロボットに\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 72 開始文節 : 顔認識システム\t->天網を\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 73 開始文節 : 天網を\t->搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 74 開始文節 : 搭載するなど\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 75 開始文節 : 人工知能による\t->監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 76 開始文節 : 監視社会管理社会化が\t->行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 77 開始文節 : 行われている\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 78 開始文節 : 新疆ウイグル自治区では\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 79 開始文節 : 監視カメラや\t->携帯電話などから\t->収集した\t->個人情報を\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 80 開始文節 : 携帯電話などから\t->収集した\t->個人情報を\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 81 開始文節 : 収集した\t->個人情報を\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 82 開始文節 : 個人情報を\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 83 開始文節 : 人工知能で\t->解析するや\t->人種プロファイリングで\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 84 開始文節 : 解析するや\t->人種プロファイリングで\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 85 開始文節 : 人種プロファイリングで\t->選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 86 開始文節 : 選別した\t->少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 87 開始文節 : 少数民族の\t->ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 88 開始文節 : ウイグル族を\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 89 開始文節 : 法的手続きを\t->経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 90 開始文節 : 経ずに\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 91 開始文節 : 2017年\t->6月時点で\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 92 開始文節 : 6月時点で\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 93 開始文節 : 約1万5千人も\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 94 開始文節 : テロや\t->犯罪を\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 95 開始文節 : 犯罪を\t->犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 96 開始文節 : 犯す\t->可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 97 開始文節 : 可能性が\t->あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 98 開始文節 : あるとして\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "# 99 開始文節 : 新疆ウイグル再教育キャンプに\t->予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#100 開始文節 : 予防拘禁していると\t->する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#101 開始文節 : する\t->中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#102 開始文節 : 中国政府の\t->内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#103 開始文節 : 内部文書であるが\t->報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#104 開始文節 : 報じられており\t->なっている\t->破壊された\t->懸念されている\n",
            "#105 開始文節 : AIを\t->使った\t->政府による\t->特定の\t->民族の\t->コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#106 開始文節 : 使った\t->政府による\t->特定の\t->民族の\t->コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#107 開始文節 : 政府による\t->特定の\t->民族の\t->コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#108 開始文節 : 特定の\t->民族の\t->コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#109 開始文節 : 民族の\t->コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#110 開始文節 : 選別や\t->コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#111 開始文節 : コンピュータが\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#112 開始文節 : 人間を\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#113 開始文節 : 強制収容所に\t->送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#114 開始文節 : 送る\t->人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#115 開始文節 : 人権侵害は\t->なっている\t->破壊された\t->懸念されている\n",
            "#116 開始文節 : 前例が\t->ないとして\t->なっている\t->破壊された\t->懸念されている\n",
            "#117 開始文節 : ないとして\t->なっている\t->破壊された\t->懸念されている\n",
            "#118 開始文節 : 国際問題に\t->なっている\t->破壊された\t->懸念されている\n",
            "#119 開始文節 : なっている\t->破壊された\t->懸念されている\n",
            "#120 開始文節 : 香港では\t->破壊された\t->懸念されている\n",
            "#121 開始文節 : 中国本土と\t->同様の\t->人工知能による\t->監視社会化を\t->恐れ\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#122 開始文節 : 同様の\t->人工知能による\t->監視社会化を\t->恐れ\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#123 開始文節 : 人工知能による\t->監視社会化を\t->恐れ\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#124 開始文節 : 監視社会化を\t->恐れ\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#125 開始文節 : 恐れ\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#126 開始文節 : 2019年\t->香港民主化デモが\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#127 開始文節 : 香港民主化デモが\t->起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#128 開始文節 : 起きた\t->際は\t->破壊された\t->懸念されている\n",
            "#129 開始文節 : 際は\t->破壊された\t->懸念されている\n",
            "#130 開始文節 : 監視カメラを\t->搭載した\t->スマート街灯が\t->破壊された\t->懸念されている\n",
            "#131 開始文節 : 搭載した\t->スマート街灯が\t->破壊された\t->懸念されている\n",
            "#132 開始文節 : スマート街灯が\t->破壊された\t->懸念されている\n",
            "#133 開始文節 : 市民に\t->破壊された\t->懸念されている\n",
            "#134 開始文節 : 次々と\t->破壊された\t->懸念されている\n",
            "#135 開始文節 : 破壊された\t->懸念されている\n",
            "#136 開始文節 : 中国は\t->輸出しており\t->懸念されている\n",
            "#137 開始文節 : AI監視技術を\t->輸出しており\t->懸念されている\n",
            "#138 開始文節 : 中東アジアアフリカなど\t->世界各国に\t->輸出しており\t->懸念されている\n",
            "#139 開始文節 : 世界各国に\t->輸出しており\t->懸念されている\n",
            "#140 開始文節 : 輸出しており\t->懸念されている\n",
            "#141 開始文節 : 国際連合の\t->専門機関である\t->国際電気通信連合\t->ITUを通じて\t->主導してる\t->ことから\t->懸念されている\n",
            "#142 開始文節 : 専門機関である\t->国際電気通信連合\t->ITUを通じて\t->主導してる\t->ことから\t->懸念されている\n",
            "#143 開始文節 : 国際電気通信連合\t->ITUを通じて\t->主導してる\t->ことから\t->懸念されている\n",
            "#144 開始文節 : ITUを通じて\t->主導してる\t->ことから\t->懸念されている\n",
            "#145 開始文節 : 中国が\t->主導してる\t->ことから\t->懸念されている\n",
            "#146 開始文節 : AI監視技術の\t->国際標準化も\t->主導してる\t->ことから\t->懸念されている\n",
            "#147 開始文節 : 国際標準化も\t->主導してる\t->ことから\t->懸念されている\n",
            "#148 開始文節 : 主導してる\t->ことから\t->懸念されている\n",
            "#149 開始文節 : ことから\t->懸念されている\n",
            "#150 開始文節 : 中国のような\t->人権侵害が\t->拡散する\t->ことが\t->懸念されている\n",
            "#151 開始文節 : 人権侵害が\t->拡散する\t->ことが\t->懸念されている\n",
            "#152 開始文節 : 世界に\t->拡散する\t->ことが\t->懸念されている\n",
            "#153 開始文節 : 拡散する\t->ことが\t->懸念されている\n",
            "#154 開始文節 : ことが\t->懸念されている\n",
            "#155 開始文節 : 人権団体などから\t->懸念されている\n",
            "--- Sentence 0062 ---\n",
            "#  0 開始文節 : 中国の\t->社会信用システムに\t->代表されるような\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  1 開始文節 : 社会信用システムに\t->代表されるような\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  2 開始文節 : 代表されるような\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  3 開始文節 : 人工知能で\t->活用して\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  4 開始文節 : ビッグデータを\t->活用して\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  5 開始文節 : 活用して\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  6 開始文節 : 人々の\t->適性を\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  7 開始文節 : 適性を\t->決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  8 開始文節 : 決める\t->制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "#  9 開始文節 : 制度は\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "# 10 開始文節 : 社会階層間の\t->格差を\t->固定化する\t->ことに\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "# 11 開始文節 : 格差を\t->固定化する\t->ことに\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "# 12 開始文節 : 固定化する\t->ことに\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "# 13 開始文節 : ことに\t->繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "# 14 開始文節 : 繋がると\t->する\t->懸念が\t->あり\t->施行された\n",
            "# 15 開始文節 : する\t->懸念が\t->あり\t->施行された\n",
            "# 16 開始文節 : 懸念が\t->あり\t->施行された\n",
            "# 17 開始文節 : あり\t->施行された\n",
            "# 18 開始文節 : 欧州連合では\t->施行された\n",
            "# 19 開始文節 : 2018年\t->5月から\t->施行された\n",
            "# 20 開始文節 : 5月から\t->施行された\n",
            "# 21 開始文節 : 人工知能の\t->ビッグデータ分析のみによる\t->差別を\t->認めない\t->EU一般データ保護規則が\t->施行された\n",
            "# 22 開始文節 : ビッグデータ分析のみによる\t->差別を\t->認めない\t->EU一般データ保護規則が\t->施行された\n",
            "# 23 開始文節 : 雇用や\t->融資での\t->差別を\t->認めない\t->EU一般データ保護規則が\t->施行された\n",
            "# 24 開始文節 : 融資での\t->差別を\t->認めない\t->EU一般データ保護規則が\t->施行された\n",
            "# 25 開始文節 : 差別を\t->認めない\t->EU一般データ保護規則が\t->施行された\n",
            "# 26 開始文節 : 認めない\t->EU一般データ保護規則が\t->施行された\n",
            "# 27 開始文節 : EU一般データ保護規則が\t->施行された\n",
            "--- Sentence 0063 ---\n",
            "#  0 開始文節 : マサチューセッツ工科大学が\t->なった\n",
            "#  1 開始文節 : 顔認識システムの\t->精度で\t->なった\n",
            "#  2 開始文節 : 精度で\t->なった\n",
            "#  3 開始文節 : Microsoftと\t->中国の\t->Megviiは\t->なった\n",
            "#  4 開始文節 : 中国の\t->Megviiは\t->なった\n",
            "#  5 開始文節 : Megviiは\t->なった\n",
            "#  6 開始文節 : 9割超で\t->達したのに対して\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "#  7 開始文節 : IBMは\t->達したのに対して\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "#  8 開始文節 : 8割に\t->達したのに対して\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "#  9 開始文節 : 達したのに対して\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 10 開始文節 : Amazonは\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 11 開始文節 : 6割で\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 12 開始文節 : 人種差別的な\t->バイアスが\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 13 開始文節 : バイアスが\t->あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 14 開始文節 : あると\t->する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 15 開始文節 : する\t->研究を\t->発表した\t->際は\t->なった\n",
            "# 16 開始文節 : 研究を\t->発表した\t->際は\t->なった\n",
            "# 17 開始文節 : 発表した\t->際は\t->なった\n",
            "# 18 開始文節 : 際は\t->なった\n",
            "# 19 開始文節 : Amazonと\t->なった\n",
            "# 20 開始文節 : 論争に\t->なった\n",
            "--- Sentence 0064 ---\n",
            "#  0 開始文節 : 主要国の\t->軍隊は\t->試みている\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  1 開始文節 : 軍隊は\t->試みている\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  2 開始文節 : ミサイル防衛の\t->分野での\t->自動化を\t->試みている\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  3 開始文節 : 分野での\t->自動化を\t->試みている\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  4 開始文節 : 自動化を\t->試みている\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  5 開始文節 : 試みている\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  6 開始文節 : アメリカ海軍は\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  7 開始文節 : 完全自動の\t->防空システム\t->ファランクスCIWSを\t->導入し\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  8 開始文節 : 防空システム\t->ファランクスCIWSを\t->導入し\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "#  9 開始文節 : ファランクスCIWSを\t->導入し\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "# 10 開始文節 : 導入し\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "# 11 開始文節 : ガトリング砲により\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "# 12 開始文節 : 対艦ミサイルを\t->破壊できる\t->射殺している\t->ある\t->ある\n",
            "# 13 開始文節 : 破壊できる\t->射殺している\t->ある\t->ある\n",
            "# 14 開始文節 : イスラエル軍は\t->所有し\t->射殺している\t->ある\t->ある\n",
            "# 15 開始文節 : 対空迎撃ミサイルシステム\t->アイアンドームを\t->所有し\t->射殺している\t->ある\t->ある\n",
            "# 16 開始文節 : アイアンドームを\t->所有し\t->射殺している\t->ある\t->ある\n",
            "# 17 開始文節 : 所有し\t->射殺している\t->ある\t->ある\n",
            "# 18 開始文節 : ガザ地区との\t->境界線には\t->射殺している\t->ある\t->ある\n",
            "# 19 開始文節 : 境界線には\t->射殺している\t->ある\t->ある\n",
            "# 20 開始文節 : 標的を\t->自動検知する\t->サムソンRCWSを\t->稼働させて\t->射殺している\t->ある\t->ある\n",
            "# 21 開始文節 : 自動検知する\t->サムソンRCWSを\t->稼働させて\t->射殺している\t->ある\t->ある\n",
            "# 22 開始文節 : ガーディアムや\t->サムソンRCWSを\t->稼働させて\t->射殺している\t->ある\t->ある\n",
            "# 23 開始文節 : サムソンRCWSを\t->稼働させて\t->射殺している\t->ある\t->ある\n",
            "# 24 開始文節 : 稼働させて\t->射殺している\t->ある\t->ある\n",
            "# 25 開始文節 : 複数の\t->人間を\t->射殺している\t->ある\t->ある\n",
            "# 26 開始文節 : 人間を\t->射殺している\t->ある\t->ある\n",
            "# 27 開始文節 : 射殺している\t->ある\t->ある\n",
            "# 28 開始文節 : 今後\t->生み\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 29 開始文節 : AIは\t->生み\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 30 開始文節 : 新しい\t->軍事能力を\t->生み\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 31 開始文節 : 軍事能力を\t->生み\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 32 開始文節 : 生み\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 33 開始文節 : 軍の\t->指揮\t->訓練\t->展開を\t->変え\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 34 開始文節 : 指揮\t->訓練\t->展開を\t->変え\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 35 開始文節 : 訓練\t->展開を\t->変え\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 36 開始文節 : 部隊の\t->展開を\t->変え\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 37 開始文節 : 展開を\t->変え\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 38 開始文節 : 変え\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 39 開始文節 : 戦争を\t->一変させ\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 40 開始文節 : 一変させ\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 41 開始文節 : その\t->変化は\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 42 開始文節 : 変化は\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 43 開始文節 : 大国間の\t->軍事バランスを\t->決める\t->ことに\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 44 開始文節 : 軍事バランスを\t->決める\t->ことに\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 45 開始文節 : 決める\t->ことに\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 46 開始文節 : ことに\t->なるとの\t->主張も\t->ある\t->ある\n",
            "# 47 開始文節 : なるとの\t->主張も\t->ある\t->ある\n",
            "# 48 開始文節 : 主張も\t->ある\t->ある\n",
            "# 49 開始文節 : ある\t->ある\n",
            "# 50 開始文節 : P-\t->1(哨戒機)のように\t->搭載される\t->ことも\t->ある\n",
            "# 51 開始文節 : 1(哨戒機)のように\t->搭載される\t->ことも\t->ある\n",
            "# 52 開始文節 : 戦闘指揮システムに\t->搭載される\t->ことも\t->ある\n",
            "# 53 開始文節 : 支援用に\t->搭載される\t->ことも\t->ある\n",
            "# 54 開始文節 : 搭載される\t->ことも\t->ある\n",
            "# 55 開始文節 : ことも\t->ある\n",
            "--- Sentence 0065 ---\n",
            "#  0 開始文節 : 2016年\t->6月\t->開発した\t->ALPHAは\t->発表された\t->動作可能\t->した\n",
            "#  1 開始文節 : 6月\t->開発した\t->ALPHAは\t->発表された\t->動作可能\t->した\n",
            "#  2 開始文節 : 米シンシナティ大学の\t->研究チームが\t->開発した\t->ALPHAは\t->発表された\t->動作可能\t->した\n",
            "#  3 開始文節 : 研究チームが\t->開発した\t->ALPHAは\t->発表された\t->動作可能\t->した\n",
            "#  4 開始文節 : 開発した\t->ALPHAは\t->発表された\t->動作可能\t->した\n",
            "#  5 開始文節 : ALPHAは\t->発表された\t->動作可能\t->した\n",
            "#  6 開始文節 : 元米軍パイロットとの\t->模擬空戦で\t->勝利したと\t->発表された\t->動作可能\t->した\n",
            "#  7 開始文節 : 模擬空戦で\t->勝利したと\t->発表された\t->動作可能\t->した\n",
            "#  8 開始文節 : 一方的に\t->勝利したと\t->発表された\t->動作可能\t->した\n",
            "#  9 開始文節 : 勝利したと\t->発表された\t->動作可能\t->した\n",
            "# 10 開始文節 : 発表された\t->動作可能\t->した\n",
            "# 11 開始文節 : AIプログラムは\t->使用しており\t->動作可能\t->した\n",
            "# 12 開始文節 : 遺伝的アルゴリズムと\t->ファジィ制御を\t->使用しており\t->動作可能\t->した\n",
            "# 13 開始文節 : ファジィ制御を\t->使用しており\t->動作可能\t->した\n",
            "# 14 開始文節 : 使用しており\t->動作可能\t->した\n",
            "# 15 開始文節 : アルゴリズムの\t->動作に\t->高い\t->処理能力は\t->せず\t->動作可能\t->した\n",
            "# 16 開始文節 : 動作に\t->高い\t->処理能力は\t->せず\t->動作可能\t->した\n",
            "# 17 開始文節 : 高い\t->処理能力は\t->せず\t->動作可能\t->した\n",
            "# 18 開始文節 : 処理能力は\t->せず\t->動作可能\t->した\n",
            "# 19 開始文節 : 必要と\t->せず\t->動作可能\t->した\n",
            "# 20 開始文節 : せず\t->動作可能\t->した\n",
            "# 21 開始文節 : RaspberryPi上で\t->動作可能\t->した\n",
            "# 22 開始文節 : 動作可能\t->した\n",
            "# 23 開始文節 : アメリカ合衆国国防総省は\t->した\n",
            "# 24 開始文節 : 人道上の\t->観点から\t->介さない\t->自律殺傷兵器の\t->開発禁止令を\t->出し\t->した\n",
            "# 25 開始文節 : 観点から\t->介さない\t->自律殺傷兵器の\t->開発禁止令を\t->出し\t->した\n",
            "# 26 開始文節 : 人間の\t->判断を\t->介さない\t->自律殺傷兵器の\t->開発禁止令を\t->出し\t->した\n",
            "# 27 開始文節 : 判断を\t->介さない\t->自律殺傷兵器の\t->開発禁止令を\t->出し\t->した\n",
            "# 28 開始文節 : 介さない\t->自律殺傷兵器の\t->開発禁止令を\t->出し\t->した\n",
            "# 29 開始文節 : 自律殺傷兵器の\t->開発禁止令を\t->出し\t->した\n",
            "# 30 開始文節 : 開発禁止令を\t->出し\t->した\n",
            "# 31 開始文節 : 2012年に\t->出し\t->した\n",
            "# 32 開始文節 : 出し\t->した\n",
            "# 33 開始文節 : 2017年には\t->した\n",
            "# 34 開始文節 : これを\t->した\n",
            "# 35 開始文節 : 恒久的な\t->ものに\t->した\n",
            "# 36 開始文節 : ものに\t->した\n",
            "--- Sentence 0066 ---\n",
            "#  0 開始文節 : 人工知能に\t->勝ち残る\t->力として\t->注目されている\n",
            "#  1 開始文節 : 人間が\t->勝ち残る\t->力として\t->注目されている\n",
            "#  2 開始文節 : 勝ち残る\t->力として\t->注目されている\n",
            "#  3 開始文節 : 力として\t->注目されている\n",
            "#  4 開始文節 : OODAループが\t->注目されている\n",
            "--- Sentence 0067 ---\n",
            "#  0 開始文節 : 一部の\t->科学者や\t->首脳らは\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  1 開始文節 : 科学者や\t->首脳らは\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  2 開始文節 : ハイテク企業の\t->首脳らは\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  3 開始文節 : 首脳らは\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  4 開始文節 : AIの\t->軍事利用により\t->加速すると\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  5 開始文節 : 軍事利用により\t->加速すると\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  6 開始文節 : 世界の\t->不安定化は\t->加速すると\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  7 開始文節 : 不安定化は\t->加速すると\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  8 開始文節 : 加速すると\t->主張している\t->予測\t->盛り込まれなかった\n",
            "#  9 開始文節 : 主張している\t->予測\t->盛り込まれなかった\n",
            "# 10 開始文節 : 2015年に\t->開催された\t->人工知能国際合同会議で\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 11 開始文節 : ブエノスアイレスで\t->開催された\t->人工知能国際合同会議で\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 12 開始文節 : 開催された\t->人工知能国際合同会議で\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 13 開始文節 : 人工知能国際合同会議で\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 14 開始文節 : スティーブンホーキング\t->アメリカ宇宙ベンチャー企業の\t->スペースX創業者の\t->イーロン・マスク\t->スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 15 開始文節 : アメリカ宇宙ベンチャー企業の\t->スペースX創業者の\t->イーロン・マスク\t->スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 16 開始文節 : スペースX創業者の\t->イーロン・マスク\t->スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 17 開始文節 : イーロン・マスク\t->スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 18 開始文節 : アップル社の\t->共同創業者の\t->スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 19 開始文節 : 共同創業者の\t->スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 20 開始文節 : スティーブ・ウォズニアックら\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 21 開始文節 : 科学者と\t->企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 22 開始文節 : 企業家らにより\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 23 開始文節 : 公開書簡が\t->出されたが\t->予測\t->盛り込まれなかった\n",
            "# 24 開始文節 : 出されたが\t->予測\t->盛り込まれなかった\n",
            "# 25 開始文節 : そこには\t->予測\t->盛り込まれなかった\n",
            "# 26 開始文節 : 自動操縦による\t->銃火器を\t->操る\t->人型ロボットなど\t->AI搭載型兵器は\t->予測\t->盛り込まれなかった\n",
            "# 27 開始文節 : 無人爆撃機や\t->銃火器を\t->操る\t->人型ロボットなど\t->AI搭載型兵器は\t->予測\t->盛り込まれなかった\n",
            "# 28 開始文節 : 銃火器を\t->操る\t->人型ロボットなど\t->AI搭載型兵器は\t->予測\t->盛り込まれなかった\n",
            "# 29 開始文節 : 操る\t->人型ロボットなど\t->AI搭載型兵器は\t->予測\t->盛り込まれなかった\n",
            "# 30 開始文節 : 人型ロボットなど\t->AI搭載型兵器は\t->予測\t->盛り込まれなかった\n",
            "# 31 開始文節 : AI搭載型兵器は\t->予測\t->盛り込まれなかった\n",
            "# 32 開始文節 : 火薬\t->核兵器に\t->続く\t->第3の\t->革命と\t->とらえられ\t->予測\t->盛り込まれなかった\n",
            "# 33 開始文節 : 核兵器に\t->続く\t->第3の\t->革命と\t->とらえられ\t->予測\t->盛り込まれなかった\n",
            "# 34 開始文節 : 続く\t->第3の\t->革命と\t->とらえられ\t->予測\t->盛り込まれなかった\n",
            "# 35 開始文節 : 第3の\t->革命と\t->とらえられ\t->予測\t->盛り込まれなかった\n",
            "# 36 開始文節 : 革命と\t->とらえられ\t->予測\t->盛り込まれなかった\n",
            "# 37 開始文節 : とらえられ\t->予測\t->盛り込まれなかった\n",
            "# 38 開始文節 : うち\t->一部は\t->実用可能となると\t->予測\t->盛り込まれなかった\n",
            "# 39 開始文節 : 一部は\t->実用可能となると\t->予測\t->盛り込まれなかった\n",
            "# 40 開始文節 : 数年以内に\t->実用可能となると\t->予測\t->盛り込まれなかった\n",
            "# 41 開始文節 : 実用可能となると\t->予測\t->盛り込まれなかった\n",
            "# 42 開始文節 : 予測\t->盛り込まれなかった\n",
            "# 43 開始文節 : 国家の\t->不安定化\t->暗殺\t->抑圧\t->選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 44 開始文節 : 不安定化\t->暗殺\t->抑圧\t->選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 45 開始文節 : 暗殺\t->抑圧\t->選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 46 開始文節 : 抑圧\t->選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 47 開始文節 : 特定の\t->民族への\t->選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 48 開始文節 : 民族への\t->選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 49 開始文節 : 選別攻撃などに\t->利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 50 開始文節 : 利用され\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 51 開始文節 : 兵器の\t->開発競争が\t->ならないと\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 52 開始文節 : 開発競争が\t->ならないと\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 53 開始文節 : 人類にとって\t->ならないと\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 54 開始文節 : 有益な\t->ものとは\t->ならないと\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 55 開始文節 : ものとは\t->ならないと\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 56 開始文節 : ならないと\t->記された\t->求めている\t->盛り込まれなかった\n",
            "# 57 開始文節 : 記された\t->求めている\t->盛り込まれなかった\n",
            "# 58 開始文節 : 同年\t->4月には\t->求めている\t->盛り込まれなかった\n",
            "# 59 開始文節 : 4月には\t->求めている\t->盛り込まれなかった\n",
            "# 60 開始文節 : ハーバード大学ロースクールと\t->国際人権団体である\t->ヒューマン・ライツ・ウォッチが\t->求めている\t->盛り込まれなかった\n",
            "# 61 開始文節 : 国際人権団体である\t->ヒューマン・ライツ・ウォッチが\t->求めている\t->盛り込まれなかった\n",
            "# 62 開始文節 : ヒューマン・ライツ・ウォッチが\t->求めている\t->盛り込まれなかった\n",
            "# 63 開始文節 : 自動操縦型武器の\t->禁止を\t->求めている\t->盛り込まれなかった\n",
            "# 64 開始文節 : 禁止を\t->求めている\t->盛り込まれなかった\n",
            "# 65 開始文節 : 求めている\t->盛り込まれなかった\n",
            "# 66 開始文節 : 2017年\t->11月には\t->行われ\t->盛り込まれなかった\n",
            "# 67 開始文節 : 11月には\t->行われ\t->盛り込まれなかった\n",
            "# 68 開始文節 : 国際連合で\t->行われ\t->盛り込まれなかった\n",
            "# 69 開始文節 : AIの\t->軍事利用に関する\t->初の\t->公式専門家会議が\t->行われ\t->盛り込まれなかった\n",
            "# 70 開始文節 : 軍事利用に関する\t->初の\t->公式専門家会議が\t->行われ\t->盛り込まれなかった\n",
            "# 71 開始文節 : 初の\t->公式専門家会議が\t->行われ\t->盛り込まれなかった\n",
            "# 72 開始文節 : 公式専門家会議が\t->行われ\t->盛り込まれなかった\n",
            "# 73 開始文節 : 行われ\t->盛り込まれなかった\n",
            "# 74 開始文節 : 2019年\t->8月に\t->盛り込まれなかった\n",
            "# 75 開始文節 : 8月に\t->盛り込まれなかった\n",
            "# 76 開始文節 : 同会議は\t->盛り込まれなかった\n",
            "# 77 開始文節 : AI兵器の\t->運用を\t->めぐる\t->国際ルールを\t->採択するも\t->盛り込まれなかった\n",
            "# 78 開始文節 : 運用を\t->めぐる\t->国際ルールを\t->採択するも\t->盛り込まれなかった\n",
            "# 79 開始文節 : めぐる\t->国際ルールを\t->採択するも\t->盛り込まれなかった\n",
            "# 80 開始文節 : 事実上初の\t->国際ルールを\t->採択するも\t->盛り込まれなかった\n",
            "# 81 開始文節 : 国際ルールを\t->採択するも\t->盛り込まれなかった\n",
            "# 82 開始文節 : 採択するも\t->盛り込まれなかった\n",
            "# 83 開始文節 : 法的拘束力は\t->盛り込まれなかった\n",
            "--- Sentence 0068 ---\n",
            "#  0 開始文節 : 新冷戦や\t->米中冷戦の\t->状態に\t->あるとも\t->評されている\t->米国中国ロシアは\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  1 開始文節 : 米中冷戦の\t->状態に\t->あるとも\t->評されている\t->米国中国ロシアは\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  2 開始文節 : 状態に\t->あるとも\t->評されている\t->米国中国ロシアは\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  3 開始文節 : あるとも\t->評されている\t->米国中国ロシアは\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  4 開始文節 : 評されている\t->米国中国ロシアは\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  5 開始文節 : 米国中国ロシアは\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  6 開始文節 : 核開発に\t->匹敵する\t->開発競争を\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  7 開始文節 : 匹敵する\t->開発競争を\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  8 開始文節 : 開発競争を\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#  9 開始文節 : 人工知能の\t->軍事利用をめぐって\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 10 開始文節 : 軍事利用をめぐって\t->行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 11 開始文節 : 行っている\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 12 開始文節 : 中国は\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 13 開始文節 : 2017年\t->6月に\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 14 開始文節 : 6月に\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 15 開始文節 : 119機の\t->ドローン群の\t->自律飛行実験で\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 16 開始文節 : ドローン群の\t->自律飛行実験で\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 17 開始文節 : 自律飛行実験で\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 18 開始文節 : 前年\t->2016年に\t->103機の\t->飛行実験に\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 19 開始文節 : 2016年に\t->103機の\t->飛行実験に\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 20 開始文節 : 103機の\t->飛行実験に\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 21 開始文節 : 飛行実験に\t->成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 22 開始文節 : 成功した\t->米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 23 開始文節 : 米軍の\t->記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 24 開始文節 : 記録を\t->更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 25 開始文節 : 更新して\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 26 開始文節 : 翌\t->2018年\t->5月には\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 27 開始文節 : 2018年\t->5月には\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 28 開始文節 : 5月には\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 29 開始文節 : 北米の\t->都市を\t->爆撃する\t->CGの\t->映像も\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 30 開始文節 : 都市を\t->爆撃する\t->CGの\t->映像も\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 31 開始文節 : 爆撃する\t->CGの\t->映像も\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 32 開始文節 : CGの\t->映像も\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 33 開始文節 : 映像も\t->発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 34 開始文節 : 発表し\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 35 開始文節 : 同年\t->6月には\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 36 開始文節 : 6月には\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 37 開始文節 : 56隻の\t->自律無人艇を\t->使った\t->世界最大規模の\t->試験を\t->行うなど\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 38 開始文節 : 自律無人艇を\t->使った\t->世界最大規模の\t->試験を\t->行うなど\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 39 開始文節 : 使った\t->世界最大規模の\t->試験を\t->行うなど\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 40 開始文節 : 世界最大規模の\t->試験を\t->行うなど\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 41 開始文節 : 試験を\t->行うなど\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 42 開始文節 : 行うなど\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 43 開始文節 : AIの\t->軍事利用の\t->技術\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 44 開始文節 : 軍事利用の\t->技術\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 45 開始文節 : 技術\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 46 開始文節 : 特に\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 47 開始文節 : スウォームと\t->呼ばれる\t->徘徊型兵器などの\t->自律兵器の\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 48 開始文節 : 呼ばれる\t->徘徊型兵器などの\t->自律兵器の\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 49 開始文節 : 大量の\t->徘徊型兵器などの\t->自律兵器の\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 50 開始文節 : 徘徊型兵器などの\t->自律兵器の\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 51 開始文節 : 自律兵器の\t->統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 52 開始文節 : 統合運用で\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 53 開始文節 : 中国が\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 54 開始文節 : 急速に\t->進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 55 開始文節 : 進展しており\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 56 開始文節 : アメリカに\t->追い付く\t->可能性が\t->ある\t->ことについて\t->懸念し\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 57 開始文節 : 追い付く\t->可能性が\t->ある\t->ことについて\t->懸念し\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 58 開始文節 : 可能性が\t->ある\t->ことについて\t->懸念し\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 59 開始文節 : ある\t->ことについて\t->懸念し\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 60 開始文節 : ことについて\t->懸念し\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 61 開始文節 : 懸念し\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 62 開始文節 : アメリカ側では\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 63 開始文節 : 将来に\t->備える\t->必要が\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 64 開始文節 : 備える\t->必要が\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 65 開始文節 : 必要が\t->あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 66 開始文節 : あるとの\t->主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 67 開始文節 : 主張も\t->されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 68 開始文節 : されている\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 69 開始文節 : 中国の\t->軍用AI開発は\t->与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 70 開始文節 : 軍用AI開発は\t->与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 71 開始文節 : アメリカの\t->政界に\t->与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 72 開始文節 : 軍部や\t->政界に\t->与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 73 開始文節 : 政界に\t->与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 74 開始文節 : 危機感を\t->与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 75 開始文節 : 与え\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 76 開始文節 : 2019年\t->3月に\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 77 開始文節 : 3月に\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 78 開始文節 : ジョセフダンフォード統合参謀本部議長や\t->パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 79 開始文節 : パトリック・シャナハン国防長官代行\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 80 開始文節 : ドナルドトランプ大統領は\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 81 開始文節 : 中国での\t->AI研究拠点の\t->設立などで\t->協力していると\t->非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 82 開始文節 : AI研究拠点の\t->設立などで\t->協力していると\t->非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 83 開始文節 : 設立などで\t->協力していると\t->非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 84 開始文節 : 中国人民解放軍に\t->協力していると\t->非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 85 開始文節 : 協力していると\t->非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 86 開始文節 : Googleを\t->非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 87 開始文節 : 非難し\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 88 開始文節 : Googleの\t->CEOサンダー・ピチャイは\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 89 開始文節 : CEOサンダー・ピチャイは\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 90 開始文節 : ダンフォードや\t->トランプ大統領と\t->面談して\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 91 開始文節 : トランプ大統領と\t->面談して\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 92 開始文節 : 面談して\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 93 開始文節 : 中国の\t->AI研究拠点の\t->成果は\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 94 開始文節 : AI研究拠点の\t->成果は\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 95 開始文節 : 成果は\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 96 開始文節 : 中国に\t->限らず\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 97 開始文節 : 限らず\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 98 開始文節 : 全ての\t->人々に\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "# 99 開始文節 : 人々に\t->開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#100 開始文節 : 開放されていると\t->釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#101 開始文節 : 釈明する\t->事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#102 開始文節 : 事態に\t->なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#103 開始文節 : なった\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#104 開始文節 : アメリカでは\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#105 開始文節 : Googleが\t->協力する\t->極秘計画\t->メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#106 開始文節 : 米軍の\t->AIの\t->軍事利用に\t->協力する\t->極秘計画\t->メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#107 開始文節 : AIの\t->軍事利用に\t->協力する\t->極秘計画\t->メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#108 開始文節 : 軍事利用に\t->協力する\t->極秘計画\t->メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#109 開始文節 : 協力する\t->極秘計画\t->メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#110 開始文節 : 極秘計画\t->メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#111 開始文節 : メイヴン計画を\t->行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#112 開始文節 : 行っていた\t->ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#113 開始文節 : ことが\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#114 開始文節 : Googleの\t->社員に\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#115 開始文節 : 社員に\t->暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#116 開始文節 : 暴露されており\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#117 開始文節 : 2018年\t->12月の\t->アメリカ議会の\t->公聴会では\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#118 開始文節 : 12月の\t->アメリカ議会の\t->公聴会では\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#119 開始文節 : アメリカ議会の\t->公聴会では\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#120 開始文節 : 公聴会では\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#121 開始文節 : 同様に\t->暴露された\t->中国政府に\t->協力する\t->秘密計画\t->とともに\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#122 開始文節 : 暴露された\t->中国政府に\t->協力する\t->秘密計画\t->とともに\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#123 開始文節 : 中国政府に\t->協力する\t->秘密計画\t->とともに\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#124 開始文節 : 協力する\t->秘密計画\t->とともに\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#125 開始文節 : 秘密計画\t->とともに\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#126 開始文節 : とともに\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#127 開始文節 : 人工知能を\t->用いた\t->人権侵害は\t->拒否すると\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#128 開始文節 : 用いた\t->人権侵害は\t->拒否すると\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#129 開始文節 : 兵器開発や\t->人権侵害は\t->拒否すると\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#130 開始文節 : 人権侵害は\t->拒否すると\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#131 開始文節 : 拒否すると\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#132 開始文節 : Googleが\t->誓った\t->6月の\t->人工知能開発\t->6原則との\t->整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#133 開始文節 : 誓った\t->6月の\t->人工知能開発\t->6原則との\t->整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#134 開始文節 : 同年\t->6月の\t->人工知能開発\t->6原則との\t->整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#135 開始文節 : 6月の\t->人工知能開発\t->6原則との\t->整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#136 開始文節 : 人工知能開発\t->6原則との\t->整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#137 開始文節 : 6原則との\t->整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#138 開始文節 : 整合性で\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#139 開始文節 : 追及を\t->受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#140 開始文節 : 受けた\t->否定した\t->呼んだ\t->鳴らした\n",
            "#141 開始文節 : 中国軍の\t->戦闘機J-20の\t->標的選択支援アルゴリズムに\t->関わったと\t->報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#142 開始文節 : 戦闘機J-20の\t->標的選択支援アルゴリズムに\t->関わったと\t->報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#143 開始文節 : 標的選択支援アルゴリズムに\t->関わったと\t->報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#144 開始文節 : グーグルの\t->AI研究者が\t->関わったと\t->報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#145 開始文節 : AI研究者が\t->関わったと\t->報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#146 開始文節 : 関わったと\t->報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#147 開始文節 : 報道された\t->際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#148 開始文節 : 際は\t->否定した\t->呼んだ\t->鳴らした\n",
            "#149 開始文節 : AIではなく\t->モデリングと\t->否定した\t->呼んだ\t->鳴らした\n",
            "#150 開始文節 : 統計学的な\t->モデリングと\t->否定した\t->呼んだ\t->鳴らした\n",
            "#151 開始文節 : モデリングと\t->否定した\t->呼んだ\t->鳴らした\n",
            "#152 開始文節 : 否定した\t->呼んだ\t->鳴らした\n",
            "#153 開始文節 : また\t->呼んだ\t->鳴らした\n",
            "#154 開始文節 : Microsoftが\t->発表した\t->際も\t->呼んだ\t->鳴らした\n",
            "#155 開始文節 : 中国軍の\t->教育機関と\t->AIの\t->共同研究を\t->発表した\t->際も\t->呼んだ\t->鳴らした\n",
            "#156 開始文節 : 教育機関と\t->AIの\t->共同研究を\t->発表した\t->際も\t->呼んだ\t->鳴らした\n",
            "#157 開始文節 : AIの\t->共同研究を\t->発表した\t->際も\t->呼んだ\t->鳴らした\n",
            "#158 開始文節 : 共同研究を\t->発表した\t->際も\t->呼んだ\t->鳴らした\n",
            "#159 開始文節 : 発表した\t->際も\t->呼んだ\t->鳴らした\n",
            "#160 開始文節 : 際も\t->呼んだ\t->鳴らした\n",
            "#161 開始文節 : 同様に\t->呼んだ\t->鳴らした\n",
            "#162 開始文節 : 波紋を\t->呼んだ\t->鳴らした\n",
            "#163 開始文節 : 呼んだ\t->鳴らした\n",
            "#164 開始文節 : 2019年\t->11月に\t->鳴らした\n",
            "#165 開始文節 : 11月に\t->鳴らした\n",
            "#166 開始文節 : マーク・エスパー国防長官は\t->鳴らした\n",
            "#167 開始文節 : 中国が\t->構築しているだけでなく\t->販売している\t->ことに\t->鳴らした\n",
            "#168 開始文節 : AIによって\t->構築しているだけでなく\t->販売している\t->ことに\t->鳴らした\n",
            "#169 開始文節 : 新しい\t->監視国家を\t->構築しているだけでなく\t->販売している\t->ことに\t->鳴らした\n",
            "#170 開始文節 : 監視国家を\t->構築しているだけでなく\t->販売している\t->ことに\t->鳴らした\n",
            "#171 開始文節 : 構築しているだけでなく\t->販売している\t->ことに\t->鳴らした\n",
            "#172 開始文節 : 中東で\t->拡散させて\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#173 開始文節 : 翼竜や\t->彩虹など\t->無人攻撃機を\t->拡散させて\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#174 開始文節 : 彩虹など\t->無人攻撃機を\t->拡散させて\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#175 開始文節 : 無人攻撃機を\t->拡散させて\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#176 開始文節 : 大量に\t->拡散させて\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#177 開始文節 : 拡散させて\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#178 開始文節 : AIで\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#179 開始文節 : 自律的に\t->攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#180 開始文節 : 攻撃する\t->ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#181 開始文節 : ドローン兵器も\t->販売している\t->ことに\t->鳴らした\n",
            "#182 開始文節 : 販売している\t->ことに\t->鳴らした\n",
            "#183 開始文節 : ことに\t->鳴らした\n",
            "#184 開始文節 : 警鐘を\t->鳴らした\n",
            "--- Sentence 0069 ---\n",
            "#  0 開始文節 : ロシアと\t->中国は\t->実用化してると\t->される\t->ハッキングの\t->自動化の\t->他\t->挙げられている\n",
            "#  1 開始文節 : 中国は\t->実用化してると\t->される\t->ハッキングの\t->自動化の\t->他\t->挙げられている\n",
            "#  2 開始文節 : 既に\t->実用化してると\t->される\t->ハッキングの\t->自動化の\t->他\t->挙げられている\n",
            "#  3 開始文節 : 実用化してると\t->される\t->ハッキングの\t->自動化の\t->他\t->挙げられている\n",
            "#  4 開始文節 : される\t->ハッキングの\t->自動化の\t->他\t->挙げられている\n",
            "#  5 開始文節 : ハッキングの\t->自動化の\t->他\t->挙げられている\n",
            "#  6 開始文節 : 自動化の\t->他\t->挙げられている\n",
            "#  7 開始文節 : 他\t->挙げられている\n",
            "#  8 開始文節 : 特定の\t->個人を\t->攻撃したり\t->なりすましたり\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "#  9 開始文節 : 個人を\t->攻撃したり\t->なりすましたり\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 10 開始文節 : 攻撃したり\t->なりすましたり\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 11 開始文節 : ディープフェイクで\t->なりすましたり\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 12 開始文節 : なりすましたり\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 13 開始文節 : ボット投稿により\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 14 開始文節 : 世論を\t->操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 15 開始文節 : 操る\t->等の\t->懸念が\t->挙げられている\n",
            "# 16 開始文節 : 等の\t->懸念が\t->挙げられている\n",
            "# 17 開始文節 : 懸念が\t->挙げられている\n",
            "--- Sentence 0070 ---\n",
            "#  0 開始文節 : Googleは\t->発表した\t->要請した\t->発表した\n",
            "#  1 開始文節 : 2019年\t->3月\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  2 開始文節 : 3月\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  3 開始文節 : 人工知能プロジェクトを\t->指導する\t->ために\t->構成される\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  4 開始文節 : 倫理面で\t->指導する\t->ために\t->構成される\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  5 開始文節 : 指導する\t->ために\t->構成される\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  6 開始文節 : ために\t->構成される\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  7 開始文節 : 哲学者政策立案者経済学者テクノロジスト等で\t->構成される\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  8 開始文節 : 構成される\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "#  9 開始文節 : AI倫理委員会を\t->設置すると\t->発表した\t->要請した\t->発表した\n",
            "# 10 開始文節 : 設置すると\t->発表した\t->要請した\t->発表した\n",
            "# 11 開始文節 : 発表した\t->要請した\t->発表した\n",
            "# 12 開始文節 : しかし\t->要請した\t->発表した\n",
            "# 13 開始文節 : 倫理委員会には\t->含まれており\t->要請した\t->発表した\n",
            "# 14 開始文節 : 反科学反マイノリティ・地球温暖化懐疑論等を\t->支持する\t->人物も\t->含まれており\t->要請した\t->発表した\n",
            "# 15 開始文節 : 支持する\t->人物も\t->含まれており\t->要請した\t->発表した\n",
            "# 16 開始文節 : 人物も\t->含まれており\t->要請した\t->発表した\n",
            "# 17 開始文節 : 含まれており\t->要請した\t->発表した\n",
            "# 18 開始文節 : Google社員らは\t->要請した\t->発表した\n",
            "# 19 開始文節 : 解任を\t->要請した\t->発表した\n",
            "# 20 開始文節 : 要請した\t->発表した\n",
            "# 21 開始文節 : 4月4日\t->発表した\n",
            "# 22 開始文節 : Googleは\t->発表した\n",
            "# 23 開始文節 : 倫理委員会が\t->発表した\n",
            "# 24 開始文節 : 期待どおりに\t->機能できない\t->ことが\t->判明したという\t->理由で\t->発表した\n",
            "# 25 開始文節 : 機能できない\t->ことが\t->判明したという\t->理由で\t->発表した\n",
            "# 26 開始文節 : ことが\t->判明したという\t->理由で\t->発表した\n",
            "# 27 開始文節 : 判明したという\t->理由で\t->発表した\n",
            "# 28 開始文節 : 理由で\t->発表した\n",
            "# 29 開始文節 : 委員会の\t->解散を\t->発表した\n",
            "# 30 開始文節 : 解散を\t->発表した\n",
            "--- Sentence 0071 ---\n",
            "#  0 開始文節 : 東洋哲学を\t->吸収させるという\t->三宅陽一郎の\t->テーマに\t->応じて\t->語る\t->通じている\n",
            "#  1 開始文節 : AIに\t->吸収させるという\t->三宅陽一郎の\t->テーマに\t->応じて\t->語る\t->通じている\n",
            "#  2 開始文節 : 吸収させるという\t->三宅陽一郎の\t->テーマに\t->応じて\t->語る\t->通じている\n",
            "#  3 開始文節 : 三宅陽一郎の\t->テーマに\t->応じて\t->語る\t->通じている\n",
            "#  4 開始文節 : テーマに\t->応じて\t->語る\t->通じている\n",
            "#  5 開始文節 : 応じて\t->語る\t->通じている\n",
            "#  6 開始文節 : 井口尊仁は\t->挙げ\t->語る\t->通じている\n",
            "#  7 開始文節 : 鳥居TORIIという\t->自分の\t->プロジェクトを\t->挙げ\t->語る\t->通じている\n",
            "#  8 開始文節 : 自分の\t->プロジェクトを\t->挙げ\t->語る\t->通じている\n",
            "#  9 開始文節 : プロジェクトを\t->挙げ\t->語る\t->通じている\n",
            "# 10 開始文節 : 挙げ\t->語る\t->通じている\n",
            "# 11 開始文節 : われわれは\t->アニミズムで\t->ありますと\t->語る\t->通じている\n",
            "# 12 開始文節 : アニミズムで\t->ありますと\t->語る\t->通じている\n",
            "# 13 開始文節 : あらゆる\t->ものに\t->見いだす\t->文化が\t->ありますと\t->語る\t->通じている\n",
            "# 14 開始文節 : ものに\t->見いだす\t->文化が\t->ありますと\t->語る\t->通じている\n",
            "# 15 開始文節 : 霊的存在を\t->見いだす\t->文化が\t->ありますと\t->語る\t->通じている\n",
            "# 16 開始文節 : 見いだす\t->文化が\t->ありますと\t->語る\t->通じている\n",
            "# 17 開始文節 : 文化が\t->ありますと\t->語る\t->通じている\n",
            "# 18 開始文節 : ありますと\t->語る\t->通じている\n",
            "# 19 開始文節 : 三宅および\t->立石従寛に\t->語る\t->通じている\n",
            "# 20 開始文節 : 立石従寛に\t->語る\t->通じている\n",
            "# 21 開始文節 : 語る\t->通じている\n",
            "# 22 開始文節 : アニミズム的人工知能論は\t->通じている\n",
            "# 23 開始文節 : 現代アートや\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 24 開始文節 : 禅の\t->悟りを\t->やって\t->やらせるかを\t->論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 25 開始文節 : 悟りを\t->やって\t->やらせるかを\t->論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 26 開始文節 : どう\t->やって\t->やらせるかを\t->論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 27 開始文節 : やって\t->やらせるかを\t->論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 28 開始文節 : AIに\t->やらせるかを\t->論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 29 開始文節 : やらせるかを\t->論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 30 開始文節 : 論じた\t->三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 31 開始文節 : 三宅の\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 32 開始文節 : 人工知能の\t->ための\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 33 開始文節 : ための\t->哲学塾東洋哲学篇にも\t->通じている\n",
            "# 34 開始文節 : 哲学塾東洋哲学篇にも\t->通じている\n",
            "--- Sentence 0072 ---\n",
            "#  0 開始文節 : 元Googleエンジニアの\t->アンソニーレバンドウスキーは\t->示している\n",
            "#  1 開始文節 : アンソニーレバンドウスキーは\t->示している\n",
            "#  2 開始文節 : 2017年\t->示している\n",
            "#  3 開始文節 : AIを\t->する\t->宗教団体\t->示している\n",
            "#  4 開始文節 : 神と\t->する\t->宗教団体\t->示している\n",
            "#  5 開始文節 : する\t->宗教団体\t->示している\n",
            "#  6 開始文節 : 宗教団体\t->示している\n",
            "#  7 開始文節 : WayoftheFuture(未来の\t->道)」を\t->創立している\t->示している\n",
            "#  8 開始文節 : 道)」を\t->創立している\t->示している\n",
            "#  9 開始文節 : 創立している\t->示している\n",
            "# 10 開始文節 : 団体の\t->使命は\t->表現されており\t->報道した\t->示している\n",
            "# 11 開始文節 : 使命は\t->表現されており\t->報道した\t->示している\n",
            "# 12 開始文節 : 人工知能\t->AIに\t->基づいた\t->Godheadの\t->実現を\t->促進し\t->開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 13 開始文節 : AIに\t->基づいた\t->Godheadの\t->実現を\t->促進し\t->開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 14 開始文節 : 基づいた\t->Godheadの\t->実現を\t->促進し\t->開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 15 開始文節 : Godheadの\t->実現を\t->促進し\t->開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 16 開始文節 : 実現を\t->促進し\t->開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 17 開始文節 : 促進し\t->開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 18 開始文節 : 開発する\t->ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 19 開始文節 : ことそして\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 20 開始文節 : Godheadの\t->理解と\t->崇拝を通して\t->良くする\t->ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 21 開始文節 : 理解と\t->崇拝を通して\t->良くする\t->ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 22 開始文節 : 崇拝を通して\t->良くする\t->ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 23 開始文節 : 社会を\t->良くする\t->ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 24 開始文節 : より\t->良くする\t->ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 25 開始文節 : 良くする\t->ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 26 開始文節 : ことに\t->貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 27 開始文節 : 貢献する\t->ことと\t->表現されており\t->報道した\t->示している\n",
            "# 28 開始文節 : ことと\t->表現されており\t->報道した\t->示している\n",
            "# 29 開始文節 : 抽象的に\t->表現されており\t->報道した\t->示している\n",
            "# 30 開始文節 : 表現されており\t->報道した\t->示している\n",
            "# 31 開始文節 : 多くの\t->海外メディアは\t->報道した\t->示している\n",
            "# 32 開始文節 : 海外メディアは\t->報道した\t->示している\n",
            "# 33 開始文節 : SF映画や\t->歴史などと\t->関連付けて\t->報道した\t->示している\n",
            "# 34 開始文節 : 歴史などと\t->関連付けて\t->報道した\t->示している\n",
            "# 35 開始文節 : 関連付けて\t->報道した\t->示している\n",
            "# 36 開始文節 : 報道した\t->示している\n",
            "# 37 開始文節 : Uberと\t->Googleの\t->Waymoは\t->示している\n",
            "# 38 開始文節 : Googleの\t->Waymoは\t->示している\n",
            "# 39 開始文節 : Waymoは\t->示している\n",
            "# 40 開始文節 : レバンドウスキーが\t->盗用した\t->ことを\t->訴え\t->行っている\t->一方\t->示している\n",
            "# 41 開始文節 : 自動運転に関する\t->機密情報を\t->盗用した\t->ことを\t->訴え\t->行っている\t->一方\t->示している\n",
            "# 42 開始文節 : 機密情報を\t->盗用した\t->ことを\t->訴え\t->行っている\t->一方\t->示している\n",
            "# 43 開始文節 : 盗用した\t->ことを\t->訴え\t->行っている\t->一方\t->示している\n",
            "# 44 開始文節 : ことを\t->訴え\t->行っている\t->一方\t->示している\n",
            "# 45 開始文節 : 訴え\t->行っている\t->一方\t->示している\n",
            "# 46 開始文節 : 裁判を\t->行っている\t->一方\t->示している\n",
            "# 47 開始文節 : 行っている\t->一方\t->示している\n",
            "# 48 開始文節 : 一方\t->示している\n",
            "# 49 開始文節 : レバンドウスキーは\t->示している\n",
            "# 50 開始文節 : Uberの\t->元CEO\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 51 開始文節 : 元CEO\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 52 開始文節 : トラビスカラニックに対し\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 53 開始文節 : ボット\t->ひとつずつ\t->征服するんだと\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 54 開始文節 : ひとつずつ\t->征服するんだと\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 55 開始文節 : 我々は\t->征服するんだと\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 56 開始文節 : 世界を\t->征服するんだと\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 57 開始文節 : 征服するんだと\t->発言するなど\t->振る舞いを\t->示している\n",
            "# 58 開始文節 : 発言するなど\t->振る舞いを\t->示している\n",
            "# 59 開始文節 : 野心的な\t->振る舞いを\t->示している\n",
            "# 60 開始文節 : 振る舞いを\t->示している\n",
            "--- Sentence 0073 ---\n",
            "#  0 開始文節 : 相愛大学人文学部教授の\t->釈徹宗は\t->述べている\t->いう\n",
            "#  1 開始文節 : 釈徹宗は\t->述べている\t->いう\n",
            "#  2 開始文節 : 哲学や\t->思想や\t->文学と\t->霊性論との\t->線引きも\t->不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  3 開始文節 : 思想や\t->文学と\t->霊性論との\t->線引きも\t->不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  4 開始文節 : 文学と\t->霊性論との\t->線引きも\t->不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  5 開始文節 : 宗教や\t->霊性論との\t->線引きも\t->不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  6 開始文節 : 霊性論との\t->線引きも\t->不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  7 開始文節 : 線引きも\t->不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  8 開始文節 : 不明瞭になってきていますと\t->述べている\t->いう\n",
            "#  9 開始文節 : 述べている\t->いう\n",
            "# 10 開始文節 : 哲学者倫理学者である\t->内田樹に\t->よれば\t->いう\n",
            "# 11 開始文節 : 内田樹に\t->よれば\t->いう\n",
            "# 12 開始文節 : よれば\t->いう\n",
            "# 13 開始文節 : 本物の\t->哲学者は\t->していると\t->いう\n",
            "# 14 開始文節 : 哲学者は\t->していると\t->いう\n",
            "# 15 開始文節 : みんな\t->していると\t->いう\n",
            "# 16 開始文節 : 死者と\t->幽霊と\t->異界の\t->話を\t->していると\t->いう\n",
            "# 17 開始文節 : 幽霊と\t->異界の\t->話を\t->していると\t->いう\n",
            "# 18 開始文節 : 異界の\t->話を\t->していると\t->いう\n",
            "# 19 開始文節 : 話を\t->していると\t->いう\n",
            "# 20 開始文節 : していると\t->いう\n",
            "--- Sentence 0074 ---\n",
            "#  0 開始文節 : 発明家レイカーツワイルが\t->言うには\t->話題である\t->主張している\n",
            "#  1 開始文節 : 言うには\t->話題である\t->主張している\n",
            "#  2 開始文節 : 哲学者ジョンサールが\t->提起した\t->AIの\t->論争は\t->話題である\t->主張している\n",
            "#  3 開始文節 : 提起した\t->AIの\t->論争は\t->話題である\t->主張している\n",
            "#  4 開始文節 : 強い\t->AIの\t->論争は\t->話題である\t->主張している\n",
            "#  5 開始文節 : AIと\t->弱い\t->AIの\t->論争は\t->話題である\t->主張している\n",
            "#  6 開始文節 : 弱い\t->AIの\t->論争は\t->話題である\t->主張している\n",
            "#  7 開始文節 : AIの\t->論争は\t->話題である\t->主張している\n",
            "#  8 開始文節 : 論争は\t->話題である\t->主張している\n",
            "#  9 開始文節 : AIの\t->哲学議論で\t->話題である\t->主張している\n",
            "# 10 開始文節 : 哲学議論で\t->話題である\t->主張している\n",
            "# 11 開始文節 : ホットな\t->話題である\t->主張している\n",
            "# 12 開始文節 : 話題である\t->主張している\n",
            "# 13 開始文節 : 哲学者ジョンサールおよび\t->ダニエル・デネットに\t->よると\t->主張している\n",
            "# 14 開始文節 : ダニエル・デネットに\t->よると\t->主張している\n",
            "# 15 開始文節 : よると\t->主張している\n",
            "# 16 開始文節 : サールの\t->部屋や\t->中国脳といった\t->機能主義に\t->批判的な\t->思考実験は\t->主張している\n",
            "# 17 開始文節 : 中国語の\t->部屋や\t->中国脳といった\t->機能主義に\t->批判的な\t->思考実験は\t->主張している\n",
            "# 18 開始文節 : 部屋や\t->中国脳といった\t->機能主義に\t->批判的な\t->思考実験は\t->主張している\n",
            "# 19 開始文節 : ネドブロックらの\t->中国脳といった\t->機能主義に\t->批判的な\t->思考実験は\t->主張している\n",
            "# 20 開始文節 : 中国脳といった\t->機能主義に\t->批判的な\t->思考実験は\t->主張している\n",
            "# 21 開始文節 : 機能主義に\t->批判的な\t->思考実験は\t->主張している\n",
            "# 22 開始文節 : 批判的な\t->思考実験は\t->主張している\n",
            "# 23 開始文節 : 思考実験は\t->主張している\n",
            "# 24 開始文節 : 真の\t->意識が\t->実現できないと\t->主張している\n",
            "# 25 開始文節 : 意識が\t->実現できないと\t->主張している\n",
            "# 26 開始文節 : 形式論理システムによって\t->実現できないと\t->主張している\n",
            "# 27 開始文節 : 実現できないと\t->主張している\n",
            "--- Sentence 0075 ---\n",
            "#  0 開始文節 : 科学を\t->語るとは\t->ことかにおいて\t->言う\n",
            "#  1 開始文節 : 語るとは\t->ことかにおいて\t->言う\n",
            "#  2 開始文節 : どういう\t->ことかにおいて\t->言う\n",
            "#  3 開始文節 : ことかにおいて\t->言う\n",
            "#  4 開始文節 : 科学者の\t->須藤靖は\t->言う\n",
            "#  5 開始文節 : 須藤靖は\t->言う\n",
            "#  6 開始文節 : 科学についての\t->哲学的考察\t->科学哲学が\t->指摘している\t->言う\n",
            "#  7 開始文節 : 哲学的考察\t->科学哲学が\t->指摘している\t->言う\n",
            "#  8 開始文節 : 科学哲学が\t->指摘している\t->言う\n",
            "#  9 開始文節 : 実際には\t->指摘している\t->言う\n",
            "# 10 開始文節 : 科学と\t->断絶している\t->ことを\t->指摘している\t->言う\n",
            "# 11 開始文節 : 断絶している\t->ことを\t->指摘している\t->言う\n",
            "# 12 開始文節 : ことを\t->指摘している\t->言う\n",
            "# 13 開始文節 : 指摘している\t->言う\n",
            "# 14 開始文節 : また\t->言う\n",
            "# 15 開始文節 : 心や\t->意識という\t->問題を\t->解明してきた\t->脳科学計算機科学\t->コンピュータ科学人工知能研究開発等に\t->関連して\t->批判している\t->評価されている\t->言う\n",
            "# 16 開始文節 : 意識という\t->問題を\t->解明してきた\t->脳科学計算機科学\t->コンピュータ科学人工知能研究開発等に\t->関連して\t->批判している\t->評価されている\t->言う\n",
            "# 17 開始文節 : 問題を\t->解明してきた\t->脳科学計算機科学\t->コンピュータ科学人工知能研究開発等に\t->関連して\t->批判している\t->評価されている\t->言う\n",
            "# 18 開始文節 : 解明してきた\t->脳科学計算機科学\t->コンピュータ科学人工知能研究開発等に\t->関連して\t->批判している\t->評価されている\t->言う\n",
            "# 19 開始文節 : 脳科学計算機科学\t->コンピュータ科学人工知能研究開発等に\t->関連して\t->批判している\t->評価されている\t->言う\n",
            "# 20 開始文節 : コンピュータ科学人工知能研究開発等に\t->関連して\t->批判している\t->評価されている\t->言う\n",
            "# 21 開始文節 : 関連して\t->批判している\t->評価されている\t->言う\n",
            "# 22 開始文節 : 科学者の\t->フランシスクリックは\t->批判している\t->評価されている\t->言う\n",
            "# 23 開始文節 : フランシスクリックは\t->批判している\t->評価されている\t->言う\n",
            "# 24 開始文節 : 哲学者たちは\t->2000年という\t->長い間\t->残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 25 開始文節 : 2000年という\t->長い間\t->残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 26 開始文節 : 長い間\t->残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 27 開始文節 : ほとんど\t->残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 28 開始文節 : 何も\t->残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 29 開始文節 : 成果を\t->残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 30 開始文節 : 残してこなかったと\t->批判している\t->評価されている\t->言う\n",
            "# 31 開始文節 : 批判している\t->評価されている\t->言う\n",
            "# 32 開始文節 : こうした\t->観点において\t->評価されている\t->言う\n",
            "# 33 開始文節 : 観点において\t->評価されている\t->言う\n",
            "# 34 開始文節 : 哲学は\t->過ぎないと\t->評価されている\t->言う\n",
            "# 35 開始文節 : 二流どころか\t->三流の\t->学問科学に\t->過ぎないと\t->評価されている\t->言う\n",
            "# 36 開始文節 : 三流の\t->学問科学に\t->過ぎないと\t->評価されている\t->言う\n",
            "# 37 開始文節 : 学問科学に\t->過ぎないと\t->評価されている\t->言う\n",
            "# 38 開始文節 : 過ぎないと\t->評価されている\t->言う\n",
            "# 39 開始文節 : 評価されている\t->言う\n",
            "# 40 開始文節 : 脳科学者の\t->澤口俊之は\t->言う\n",
            "# 41 開始文節 : 澤口俊之は\t->言う\n",
            "# 42 開始文節 : クリックに\t->賛同し\t->述べている\t->言う\n",
            "# 43 開始文節 : 賛同し\t->述べている\t->言う\n",
            "# 44 開始文節 : これは\t->なるが\t->思うと\t->述べている\t->言う\n",
            "# 45 開始文節 : 私の\t->ため息まじりの\t->愚痴に\t->なるが\t->思うと\t->述べている\t->言う\n",
            "# 46 開始文節 : ため息まじりの\t->愚痴に\t->なるが\t->思うと\t->述べている\t->言う\n",
            "# 47 開始文節 : 愚痴に\t->なるが\t->思うと\t->述べている\t->言う\n",
            "# 48 開始文節 : なるが\t->思うと\t->述べている\t->言う\n",
            "# 49 開始文節 : 哲学者や\t->思想家というのは\t->思うと\t->述べている\t->言う\n",
            "# 50 開始文節 : 思想家というのは\t->思うと\t->述べている\t->言う\n",
            "# 51 開始文節 : つくづく\t->思うと\t->述べている\t->言う\n",
            "# 52 開始文節 : 暇だと\t->思うと\t->述べている\t->言う\n",
            "# 53 開始文節 : 思うと\t->述べている\t->言う\n",
            "# 54 開始文節 : 述べている\t->言う\n",
            "# 55 開始文節 : 実際\t->言う\n",
            "# 56 開始文節 : 哲学は\t->始まったと\t->伝えており\t->言う\n",
            "# 57 開始文節 : 暇\t->スコレーから\t->始まったと\t->伝えており\t->言う\n",
            "# 58 開始文節 : スコレーから\t->始まったと\t->伝えており\t->言う\n",
            "# 59 開始文節 : 始まったと\t->伝えており\t->言う\n",
            "# 60 開始文節 : アリストテレスが\t->伝えており\t->言う\n",
            "# 61 開始文節 : 伝えており\t->言う\n",
            "# 62 開始文節 : 上記のような\t->否定的発言も\t->的外れではないと\t->言う\n",
            "# 63 開始文節 : 否定的発言も\t->的外れではないと\t->言う\n",
            "# 64 開始文節 : 的外れではないと\t->言う\n",
            "# 65 開始文節 : 科学哲学者の\t->野家啓一は\t->言う\n",
            "# 66 開始文節 : 野家啓一は\t->言う\n",
            "--- Sentence 0076 ---\n",
            "#  0 開始文節 : 哲学者は\t->語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  1 開始文節 : 科学とは\t->違う\t->日常的言語で\t->語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  2 開始文節 : 違う\t->日常的言語で\t->語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  3 開始文節 : 日常的言語で\t->語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  4 開始文節 : 存在や\t->宇宙を\t->語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  5 開始文節 : 宇宙を\t->語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  6 開始文節 : 語ろうとしてきた\t->信用していなかった\t->されている\n",
            "#  7 開始文節 : しかし\t->信用していなかった\t->されている\n",
            "#  8 開始文節 : 理論物理学者ディラックは\t->信用していなかった\t->されている\n",
            "#  9 開始文節 : 哲学者を\t->信用していなかった\t->されている\n",
            "# 10 開始文節 : ことさら\t->信用していなかった\t->されている\n",
            "# 11 開始文節 : 信用していなかった\t->されている\n",
            "# 12 開始文節 : ディラックが\t->見た\t->ところ\t->されている\n",
            "# 13 開始文節 : 見た\t->ところ\t->されている\n",
            "# 14 開始文節 : ところ\t->されている\n",
            "# 15 開始文節 : ウィトゲンシュタインを\t->含め\t->量子力学どころか\t->理解していない\t->されている\n",
            "# 16 開始文節 : 含め\t->量子力学どころか\t->理解していない\t->されている\n",
            "# 17 開始文節 : 哲学者たちは\t->量子力学どころか\t->理解していない\t->されている\n",
            "# 18 開始文節 : 量子力学どころか\t->理解していない\t->されている\n",
            "# 19 開始文節 : パスカル以降の\t->確率の\t->概念さえ\t->理解していない\t->されている\n",
            "# 20 開始文節 : 確率の\t->概念さえ\t->理解していない\t->されている\n",
            "# 21 開始文節 : 概念さえ\t->理解していない\t->されている\n",
            "# 22 開始文節 : 理解していない\t->されている\n",
            "# 23 開始文節 : 非科学的な\t->日常的言語を\t->使っても\t->できないというのが\t->考えだと\t->されている\n",
            "# 24 開始文節 : 日常的言語を\t->使っても\t->できないというのが\t->考えだと\t->されている\n",
            "# 25 開始文節 : いくら\t->使っても\t->できないというのが\t->考えだと\t->されている\n",
            "# 26 開始文節 : 使っても\t->できないというのが\t->考えだと\t->されている\n",
            "# 27 開始文節 : 正確な\t->意思疎通を\t->行う\t->ことは\t->できないというのが\t->考えだと\t->されている\n",
            "# 28 開始文節 : 意思疎通を\t->行う\t->ことは\t->できないというのが\t->考えだと\t->されている\n",
            "# 29 開始文節 : 行う\t->ことは\t->できないというのが\t->考えだと\t->されている\n",
            "# 30 開始文節 : ことは\t->できないというのが\t->考えだと\t->されている\n",
            "# 31 開始文節 : できないというのが\t->考えだと\t->されている\n",
            "# 32 開始文節 : ディラックの\t->考えだと\t->されている\n",
            "# 33 開始文節 : 考えだと\t->されている\n",
            "--- Sentence 0077 ---\n",
            "#  0 開始文節 : 生命情報科学者神経科学者の\t->合原一幸編著\t->創られるに\t->よれば\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  1 開始文節 : 合原一幸編著\t->創られるに\t->よれば\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  2 開始文節 : 人工知能は\t->創られるに\t->よれば\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  3 開始文節 : こうして\t->創られるに\t->よれば\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  4 開始文節 : 創られるに\t->よれば\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  5 開始文節 : よれば\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  6 開始文節 : AIの\t->発展に\t->伴って\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  7 開始文節 : 急激な\t->発展に\t->伴って\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  8 開始文節 : 発展に\t->伴って\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "#  9 開始文節 : 伴って\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 10 開始文節 : 技術的特異点\t->シンギュラリティの\t->哲学が\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 11 開始文節 : シンギュラリティの\t->哲学が\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 12 開始文節 : 思想や\t->哲学が\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 13 開始文節 : 哲学が\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 14 開始文節 : 一部で\t->論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 15 開始文節 : 論じられているが\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 16 開始文節 : 特異点と\t->言っても\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 17 開始文節 : 言っても\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 18 開始文節 : 数学的な\t->話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 19 開始文節 : 話ではない\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 20 開始文節 : 前掲書は\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 21 開始文節 : そもそも\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 22 開始文節 : シンギュラリティと\t->関係した\t->議論における\t->超えるという\t->言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 23 開始文節 : 関係した\t->議論における\t->超えるという\t->言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 24 開始文節 : 議論における\t->超えるという\t->言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 25 開始文節 : 人間の\t->脳を\t->超えるという\t->言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 26 開始文節 : 脳を\t->超えるという\t->言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 27 開始文節 : 超えるという\t->言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 28 開始文節 : 言明自体が\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 29 開始文節 : うまく\t->定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 30 開始文節 : 定義できていないと\t->記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 31 開始文節 : 記している\t->起こり得るかもしれない\t->考えられている\n",
            "# 32 開始文節 : 確かに\t->起こり得るかもしれない\t->考えられている\n",
            "# 33 開始文節 : 脳を\t->捉える\t->観点から\t->見れば\t->起こり得るかもしれない\t->考えられている\n",
            "# 34 開始文節 : デジタル情報処理システムとして\t->捉える\t->観点から\t->見れば\t->起こり得るかもしれない\t->考えられている\n",
            "# 35 開始文節 : 捉える\t->観点から\t->見れば\t->起こり得るかもしれない\t->考えられている\n",
            "# 36 開始文節 : 観点から\t->見れば\t->起こり得るかもしれない\t->考えられている\n",
            "# 37 開始文節 : 見れば\t->起こり得るかもしれない\t->考えられている\n",
            "# 38 開始文節 : シンギュラリティは\t->起こり得るかもしれない\t->考えられている\n",
            "# 39 開始文節 : 起こり得るかもしれない\t->考えられている\n",
            "# 40 開始文節 : しかし\t->考えられている\n",
            "# 41 開始文節 : 実際の\t->脳は\t->考えられている\n",
            "# 42 開始文節 : 脳は\t->考えられている\n",
            "# 43 開始文節 : そのような\t->システムではなく\t->ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 44 開始文節 : 単純な\t->システムではなく\t->ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 45 開始文節 : システムではなく\t->ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 46 開始文節 : デジタルと\t->アナログが\t->融合した\t->ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 47 開始文節 : アナログが\t->融合した\t->ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 48 開始文節 : 融合した\t->ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 49 開始文節 : ハイブリッド系である\t->ことが\t->示されている\t->考えられている\n",
            "# 50 開始文節 : ことが\t->示されている\t->考えられている\n",
            "# 51 開始文節 : 脳神経科学の\t->観察結果で\t->示されている\t->考えられている\n",
            "# 52 開始文節 : 観察結果で\t->示されている\t->考えられている\n",
            "# 53 開始文節 : 示されている\t->考えられている\n",
            "# 54 開始文節 : 前掲書に\t->よると\t->考えられている\n",
            "# 55 開始文節 : よると\t->考えられている\n",
            "# 56 開始文節 : 神経膜では\t->存在し\t->生み出されている\t->ため\t->考えられている\n",
            "# 57 開始文節 : 様々な\t->ノイズが\t->存在し\t->生み出されている\t->ため\t->考えられている\n",
            "# 58 開始文節 : ノイズが\t->存在し\t->生み出されている\t->ため\t->考えられている\n",
            "# 59 開始文節 : 存在し\t->生み出されている\t->ため\t->考えられている\n",
            "# 60 開始文節 : この\t->ノイズ付きの\t->アナログ量によって\t->生み出されている\t->ため\t->考えられている\n",
            "# 61 開始文節 : ノイズ付きの\t->アナログ量によって\t->生み出されている\t->ため\t->考えられている\n",
            "# 62 開始文節 : アナログ量によって\t->生み出されている\t->ため\t->考えられている\n",
            "# 63 開始文節 : 脳内の\t->ニューロンの\t->カオスが\t->生み出されている\t->ため\t->考えられている\n",
            "# 64 開始文節 : ニューロンの\t->カオスが\t->生み出されている\t->ため\t->考えられている\n",
            "# 65 開始文節 : カオスが\t->生み出されている\t->ため\t->考えられている\n",
            "# 66 開始文節 : 生み出されている\t->ため\t->考えられている\n",
            "# 67 開始文節 : ため\t->考えられている\n",
            "# 68 開始文節 : このような\t->状況を\t->記述する\t->ことは\t->困難と\t->考えられている\n",
            "# 69 開始文節 : 状況を\t->記述する\t->ことは\t->困難と\t->考えられている\n",
            "# 70 開始文節 : デジタルで\t->記述する\t->ことは\t->困難と\t->考えられている\n",
            "# 71 開始文節 : 記述する\t->ことは\t->困難と\t->考えられている\n",
            "# 72 開始文節 : ことは\t->困難と\t->考えられている\n",
            "# 73 開始文節 : 極めて\t->困難と\t->考えられている\n",
            "# 74 開始文節 : 困難と\t->考えられている\n",
            "--- Sentence 0078 ---\n",
            "#  0 開始文節 : 数学者の\t->田中一之は\t->述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  1 開始文節 : 田中一之は\t->述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  2 開始文節 : 一般の\t->哲学者は\t->専門家ではないと\t->述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  3 開始文節 : 哲学者は\t->専門家ではないと\t->述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  4 開始文節 : 論理の\t->専門家ではないと\t->述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  5 開始文節 : 専門家ではないと\t->述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  6 開始文節 : 述べており\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  7 開始文節 : 計算機科学者\t->コンピュータ科学者電子工学者の\t->トルケルフランセーンは\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  8 開始文節 : コンピュータ科学者電子工学者の\t->トルケルフランセーンは\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "#  9 開始文節 : トルケルフランセーンは\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 10 開始文節 : 哲学者たちによる\t->言及の\t->多くが\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 11 開始文節 : 数学的な\t->言及の\t->多くが\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 12 開始文節 : 言及の\t->多くが\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 13 開始文節 : 多くが\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 14 開始文節 : ひどい\t->自由連想に\t->基づいていると\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 15 開始文節 : 誤解や\t->自由連想に\t->基づいていると\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 16 開始文節 : 自由連想に\t->基づいていると\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 17 開始文節 : 基づいていると\t->批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 18 開始文節 : 批判している\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 19 開始文節 : 田中に\t->よると\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 20 開始文節 : よると\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 21 開始文節 : ゲーデルの\t->不完全性定理について\t->書いた\t->本が\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 22 開始文節 : 不完全性定理について\t->書いた\t->本が\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 23 開始文節 : 哲学者が\t->書いた\t->本が\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 24 開始文節 : 書いた\t->本が\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 25 開始文節 : 本が\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 26 開始文節 : フランセーンの\t->本と\t->同じ\t->頃に\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 27 開始文節 : 本と\t->同じ\t->頃に\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 28 開始文節 : 同じ\t->頃に\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 29 開始文節 : 頃に\t->書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 30 開始文節 : 書店販売されていたが\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 31 開始文節 : 哲学者の\t->本は\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 32 開始文節 : 本は\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 33 開始文節 : 専門誌によって\t->酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 34 開始文節 : 酷評された\t->説明していた\t->見られる\t->定理ではない\n",
            "# 35 開始文節 : その\t->本は\t->高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 36 開始文節 : 本は\t->高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 37 開始文節 : 全体として\t->高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 38 開始文節 : 読みやすく\t->高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 39 開始文節 : 一般読者からの\t->評判は\t->高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 40 開始文節 : 評判は\t->高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 41 開始文節 : 高かったが\t->説明していた\t->見られる\t->定理ではない\n",
            "# 42 開始文節 : ゲーデルの\t->証明の\t->核\t->不動点定理について\t->説明していた\t->見られる\t->定理ではない\n",
            "# 43 開始文節 : 証明の\t->核\t->不動点定理について\t->説明していた\t->見られる\t->定理ではない\n",
            "# 44 開始文節 : 核\t->不動点定理について\t->説明していた\t->見られる\t->定理ではない\n",
            "# 45 開始文節 : 不動点定理について\t->説明していた\t->見られる\t->定理ではない\n",
            "# 46 開始文節 : 根本的な\t->勘違いを\t->したまま\t->説明していた\t->見られる\t->定理ではない\n",
            "# 47 開始文節 : 勘違いを\t->したまま\t->説明していた\t->見られる\t->定理ではない\n",
            "# 48 開始文節 : したまま\t->説明していた\t->見られる\t->定理ではない\n",
            "# 49 開始文節 : 説明していた\t->見られる\t->定理ではない\n",
            "# 50 開始文節 : 同様の\t->間違いは\t->見られる\t->定理ではない\n",
            "# 51 開始文節 : 間違いは\t->見られる\t->定理ではない\n",
            "# 52 開始文節 : 他の\t->入門書などにも\t->見られる\t->定理ではない\n",
            "# 53 開始文節 : 入門書などにも\t->見られる\t->定理ではない\n",
            "# 54 開始文節 : 見られる\t->定理ではない\n",
            "# 55 開始文節 : フランセーンに\t->よれば\t->定理ではない\n",
            "# 56 開始文節 : よれば\t->定理ではない\n",
            "# 57 開始文節 : 不完全性定理に関する\t->誤解誤用は\t->起こっており\t->乱用されている\t->定理ではない\n",
            "# 58 開始文節 : 誤解誤用は\t->起こっており\t->乱用されている\t->定理ではない\n",
            "# 59 開始文節 : 哲学を\t->はじめ\t->起こっており\t->乱用されている\t->定理ではない\n",
            "# 60 開始文節 : はじめ\t->起こっており\t->乱用されている\t->定理ではない\n",
            "# 61 開始文節 : 一般に\t->起こっており\t->乱用されている\t->定理ではない\n",
            "# 62 開始文節 : 起こっており\t->乱用されている\t->定理ではない\n",
            "# 63 開始文節 : 宗教や\t->神学でも\t->乱用されている\t->定理ではない\n",
            "# 64 開始文節 : 神学でも\t->乱用されている\t->定理ではない\n",
            "# 65 開始文節 : 乱用されている\t->定理ではない\n",
            "# 66 開始文節 : 1931年に\t->示したのは\t->定理ではない\n",
            "# 67 開始文節 : ゲーデルが\t->示したのは\t->定理ではない\n",
            "# 68 開始文節 : 示したのは\t->定理ではない\n",
            "# 69 開始文節 : 特定の\t->形式体系formula_1において\t->存在であり\t->定理ではない\n",
            "# 70 開始文節 : 形式体系formula_1において\t->存在であり\t->定理ではない\n",
            "# 71 開始文節 : 決定\t->不能な\t->命題の\t->存在であり\t->定理ではない\n",
            "# 72 開始文節 : 不能な\t->命題の\t->存在であり\t->定理ではない\n",
            "# 73 開始文節 : 命題の\t->存在であり\t->定理ではない\n",
            "# 74 開始文節 : 存在であり\t->定理ではない\n",
            "# 75 開始文節 : 一般的な\t->意味での\t->不完全性についての\t->定理ではない\n",
            "# 76 開始文節 : 意味での\t->不完全性についての\t->定理ではない\n",
            "# 77 開始文節 : 不完全性についての\t->定理ではない\n",
            "--- Sentence 0079 ---\n",
            "#  0 開始文節 : 科学と\t->哲学\n",
            "--- Sentence 0080 ---\n",
            "#  0 開始文節 : 科学を\t->語るとは\t->ことかに\t->よると\t->研究するようになった\t->述べた\n",
            "#  1 開始文節 : 語るとは\t->ことかに\t->よると\t->研究するようになった\t->述べた\n",
            "#  2 開始文節 : どういう\t->ことかに\t->よると\t->研究するようになった\t->述べた\n",
            "#  3 開始文節 : ことかに\t->よると\t->研究するようになった\t->述べた\n",
            "#  4 開始文節 : よると\t->研究するようになった\t->述べた\n",
            "#  5 開始文節 : 学問の\t->扱う\t->問題が\t->整理され\t->分化した\t->ことで\t->研究するようになった\t->述べた\n",
            "#  6 開始文節 : 扱う\t->問題が\t->整理され\t->分化した\t->ことで\t->研究するようになった\t->述べた\n",
            "#  7 開始文節 : 問題が\t->整理され\t->分化した\t->ことで\t->研究するようになった\t->述べた\n",
            "#  8 開始文節 : 整理され\t->分化した\t->ことで\t->研究するようになった\t->述べた\n",
            "#  9 開始文節 : 分化した\t->ことで\t->研究するようになった\t->述べた\n",
            "# 10 開始文節 : ことで\t->研究するようになった\t->述べた\n",
            "# 11 開始文節 : 科学と\t->哲学も\t->異なる\t->問題を\t->研究するようになった\t->述べた\n",
            "# 12 開始文節 : 哲学も\t->異なる\t->問題を\t->研究するようになった\t->述べた\n",
            "# 13 開始文節 : それぞれ\t->異なる\t->問題を\t->研究するようになった\t->述べた\n",
            "# 14 開始文節 : 異なる\t->問題を\t->研究するようになった\t->述べた\n",
            "# 15 開始文節 : 問題を\t->研究するようになった\t->述べた\n",
            "# 16 開始文節 : 研究するようになった\t->述べた\n",
            "# 17 開始文節 : これは\t->述べた\n",
            "# 18 開始文節 : 研究分野の\t->細分化そのものであり\t->言う\t->述べた\n",
            "# 19 開始文節 : 細分化そのものであり\t->言う\t->述べた\n",
            "# 20 開始文節 : 立派な\t->進歩だと\t->言う\t->述べた\n",
            "# 21 開始文節 : 進歩だと\t->言う\t->述べた\n",
            "# 22 開始文節 : 宇宙物理学者の\t->須藤は\t->言う\t->述べた\n",
            "# 23 開始文節 : 須藤は\t->言う\t->述べた\n",
            "# 24 開始文節 : 言う\t->述べた\n",
            "# 25 開始文節 : 一方で\t->述べた\n",
            "# 26 開始文節 : 科学哲学者倫理学者の\t->伊勢田哲治は\t->述べた\n",
            "# 27 開始文節 : 伊勢田哲治は\t->述べた\n",
            "# 28 開始文節 : 様々な\t->要素を\t->含んだ\t->大きな問題を\t->扱う\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 29 開始文節 : 要素を\t->含んだ\t->大きな問題を\t->扱う\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 30 開始文節 : 含んだ\t->大きな問題を\t->扱う\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 31 開始文節 : 大きな問題を\t->扱う\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 32 開始文節 : 哲学的統一的に\t->扱う\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 33 開始文節 : 扱う\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 34 開始文節 : かつての\t->天文学について\t->言及した\t->返している\t->述べた\n",
            "# 35 開始文節 : 天文学について\t->言及した\t->返している\t->述べた\n",
            "# 36 開始文節 : 言及した\t->返している\t->述べた\n",
            "# 37 開始文節 : その後の\t->天文学では\t->扱わなくなりましたし\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 38 開始文節 : 天文学では\t->扱わなくなりましたし\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 39 開始文節 : その\t->哲学的問題を\t->扱わなくなりましたし\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 40 開始文節 : 哲学的問題を\t->扱わなくなりましたし\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 41 開始文節 : 扱わなくなりましたし\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 42 開始文節 : 今の\t->物理学でも\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 43 開始文節 : 物理学でも\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 44 開始文節 : そういう\t->問題を\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 45 開始文節 : 問題を\t->扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 46 開始文節 : 扱わないと\t->述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 47 開始文節 : 述べた\t->伊勢田に対し\t->返している\t->述べた\n",
            "# 48 開始文節 : 伊勢田に対し\t->返している\t->述べた\n",
            "# 49 開始文節 : 須藤は\t->返している\t->述べた\n",
            "# 50 開始文節 : その通りですが\t->あるのでしょうかと\t->返している\t->述べた\n",
            "# 51 開始文節 : それ自体に\t->あるのでしょうかと\t->返している\t->述べた\n",
            "# 52 開始文節 : 何か\t->あるのでしょうかと\t->返している\t->述べた\n",
            "# 53 開始文節 : 問題が\t->あるのでしょうかと\t->返している\t->述べた\n",
            "# 54 開始文節 : あるのでしょうかと\t->返している\t->述べた\n",
            "# 55 開始文節 : 返している\t->述べた\n",
            "# 56 開始文節 : 須藤は\t->述べた\n",
            "# 57 開始文節 : 次のようにも\t->述べた\n",
            "--- Sentence 0081 ---\n",
            "#  0 開始文節 : 須藤は\t->している\t->述べている\n",
            "#  1 開始文節 : 哲学的に\t->論じられている\t->原因という\t->言葉を\t->取り上げて\t->述べており\t->している\t->述べている\n",
            "#  2 開始文節 : 論じられている\t->原因という\t->言葉を\t->取り上げて\t->述べており\t->している\t->述べている\n",
            "#  3 開始文節 : 原因という\t->言葉を\t->取り上げて\t->述べており\t->している\t->述べている\n",
            "#  4 開始文節 : 言葉を\t->取り上げて\t->述べており\t->している\t->述べている\n",
            "#  5 開始文節 : 取り上げて\t->述べており\t->している\t->述べている\n",
            "#  6 開始文節 : 原因という\t->言葉を\t->定義しない\t->限り\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "#  7 開始文節 : 言葉を\t->定義しない\t->限り\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "#  8 開始文節 : 具体的に\t->定義しない\t->限り\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "#  9 開始文節 : 定義しない\t->限り\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "# 10 開始文節 : 限り\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "# 11 開始文節 : それ以上の\t->議論は\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "# 12 開始文節 : 議論は\t->不可能ですと\t->述べており\t->している\t->述べている\n",
            "# 13 開始文節 : 不可能ですと\t->述べており\t->している\t->述べている\n",
            "# 14 開始文節 : 述べており\t->している\t->述べている\n",
            "# 15 開始文節 : 哲学者が\t->持っている\t->因果の\t->定義が\t->違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 16 開始文節 : 興味を\t->持っている\t->因果の\t->定義が\t->違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 17 開始文節 : 持っている\t->因果の\t->定義が\t->違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 18 開始文節 : 因果の\t->定義が\t->違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 19 開始文節 : 定義が\t->違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 20 開始文節 : 物理学者とは\t->違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 21 開始文節 : 違う\t->ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 22 開始文節 : ことは\t->確かでしょうと\t->している\t->述べている\n",
            "# 23 開始文節 : 確かでしょうと\t->している\t->述べている\n",
            "# 24 開始文節 : している\t->述べている\n",
            "# 25 開始文節 : 伊勢田は\t->述べている\n",
            "# 26 開始文節 : 思った\t->以上に\t->違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 27 開始文節 : 以上に\t->違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 28 開始文節 : 物理学者と\t->哲学者の\t->ものの\t->見え方の\t->違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 29 開始文節 : 哲学者の\t->ものの\t->見え方の\t->違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 30 開始文節 : ものの\t->見え方の\t->違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 31 開始文節 : 見え方の\t->違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 32 開始文節 : 違いというのは\t->大きいのかもしれませんと\t->述べている\n",
            "# 33 開始文節 : 大きいのかもしれませんと\t->述べている\n",
            "--- Sentence 0082 ---\n",
            "#  0 開始文節 : 対談で\t->答えている\n",
            "#  1 開始文節 : 須藤は\t->発言し\t->答えている\n",
            "#  2 開始文節 : これまで\t->行ってきました\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  3 開始文節 : けっこう\t->行ってきました\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  4 開始文節 : 長時間\t->行ってきました\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  5 開始文節 : 議論を\t->行ってきました\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  6 開始文節 : 行ってきました\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  7 開始文節 : おかげで\t->明らかになったとは\t->思いますが\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  8 開始文節 : 意見の\t->違いは\t->明らかになったとは\t->思いますが\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "#  9 開始文節 : 違いは\t->明らかになったとは\t->思いますが\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "# 10 開始文節 : 明らかになったとは\t->思いますが\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "# 11 開始文節 : 思いますが\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "# 12 開始文節 : 果たして\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "# 13 開始文節 : 何か\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "# 14 開始文節 : 決着が\t->つくのでしょうかと\t->発言し\t->答えている\n",
            "# 15 開始文節 : つくのでしょうかと\t->発言し\t->答えている\n",
            "# 16 開始文節 : 発言し\t->答えている\n",
            "# 17 開始文節 : 伊勢田は\t->答えている\n",
            "# 18 開始文節 : 決着は\t->つかないでしょうねと\t->答えている\n",
            "# 19 開始文節 : つかないでしょうねと\t->答えている\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 49. 名詞間の係り受けパスの抽出\n",
        "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj`(i<j)`のとき，係り受けパスは以下の仕様を満たすものとする．\n",
        "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
        "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
        "\n",
        "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
        "\n",
        "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
        "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
        "    Xは | Yの -> 会議で | 作り出した\n",
        "    Xは | Yで | 作り出した\n",
        "    Xは | Yという -> 用語を | 作り出した\n",
        "    Xは | Yを | 作り出した\n",
        "    Xに関する -> Yの\n",
        "    Xに関する -> 最初の -> Yで\n",
        "    Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
        "    Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
        "    Xの -> Yで\n",
        "    Xの -> 会議で | Yという -> 用語を | 作り出した\n",
        "    Xの -> 会議で | Yを | 作り出した\n",
        "    Xで | Yという -> 用語を | 作り出した\n",
        "    Xで | Yを | 作り出した\n",
        "    Xという -> Yを\n",
        "\n",
        "KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    Xは | Yに -> 関する -> 会議で | 作り出した。\n",
        "    Xは | Yで | 作り出した。\n",
        "    Xは | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xは | Yを | 作り出した。\n",
        "    Xに -> 関する -> Yで\n",
        "    Xに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xに -> 関する -> 会議で | Yを | 作り出した。\n",
        "    Xで | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xで | Yを | 作り出した。\n",
        "    Xと -> いう -> Yを"
      ],
      "metadata": {
        "id": "Ely3RRkBGCvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "何か急に競技プログラミングじみてきたな？？？？\n",
        "\n",
        "以下全てに共通する前提として\n",
        "- $X→Y$ (X→Yの順で出現する名詞)である\n",
        "- $X$と$Y$が直接係り受けにない場合の共通する係り受けを$Z$とする\n",
        "\n",
        "#### 出力形式は3種\n",
        "- A : YがXの係り先に存在している場合\n",
        "  - Xの文節 ->(中略)-> Yの文節 \n",
        "  > 係り受け木でいうと`[X,Y]`となっている、Xから下るとYが存在するイメージ\n",
        "- B : XとYが同じ係り先を持つ場合\n",
        "  - Xの文節 | Yの文節 | XとYの同じ係り先Zの文節\n",
        "  > 係り受け木でいうと`[X,Z],[Y,Z]`となって、共通するZで枝分かれしているイメージ\n",
        "- C : XとYが直接係り受けの関係にない場合\n",
        "  - Xの文節 ->(Zの前までの文節) | Yの文節 -> (Zの前までの文節) | Zの文節\n",
        "\n",
        "#### 方針\n",
        "- X,Yの決定\n",
        "  - For1 : 名詞Xを探索\n",
        "  - For2 : 名詞Yを探索\n",
        "- X,Y毎にかかり先の文節番号を保持するリストを作成\n",
        "  > 上記のジョン・マッカーシーの例文を取ると\n",
        "  - $X = [1,7]$ 「ジョン・マッカーシーは作り出した」\n",
        "  - $Y = [2,3,4,7]$ 「AIに関する最初の会議で作り出した」\n",
        "- X,Yの文節番号リストを探索\n",
        "  - For3 : Xの番号を探索\n",
        "    - IF : Xのリスト中にYの番号があれば **パターンA**\n",
        "    - IF : Xの直下とYの直下が同じ **パターンB**\n",
        "    - IF : XとYのリスト中に同じものがあれば **パターンC**\n",
        "    - それ以外はもう別の文章なんよ！\n"
      ],
      "metadata": {
        "id": "e0Ynj1cNPBdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "    \n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Kinou_Doushi = []\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(all_text[:]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    text = []\n",
        "    kakari_next = c.dst\n",
        "    if c.dst == -1:\n",
        "      continue\n",
        "    text.append(\"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs]))\n",
        "    while kakari_next != -1:\n",
        "      text.append(\"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[kakari_next].morphs]))\n",
        "      kakari_next = sentence[kakari_next].dst\n",
        "\n",
        "    kakari_text_chain = \"\\t->\".join(text)\n",
        "    print(f\"#{c.num:3g} 開始文節 : {kakari_text_chain}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gsNXyAThGCvC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "34d8b84b-e4c9-4983-ae52-5a92c9333411"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7830229e650f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EOS\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ai.ja.txt.parsed'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第6章: 機械学習"
      ],
      "metadata": {
        "id": "cNWf4DGTGUnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "zH78h42hGUnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxeItMqtGUn0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "jGlc3MXtGUn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VFTzIkC3GUn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "D6bg-elpGUn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d2jCPkqqGUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "DyUYiplYGUn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "py3mHPPMGUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "WBGDyhppGUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RohKtHJUGUn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "3W8gVsOqGUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HaEtVYZJGUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "hgOJHSCXGUn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cdan2_sdGUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "0QXqzLhTGUn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I81B5QhTGUn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Kmc1CZljGUn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FHuNDnDRGUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "JDxWM0EnGUn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QsC7jvbgGUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第7章: 単語ベクトル"
      ],
      "metadata": {
        "id": "fFWgl4_HGUn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "OVbyTLWpGUn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZ2_h-nGUn7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "5XMltajHGUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bgObeRJZGUn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "6f9guhGvGUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8rmuH7_YGUn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Qti4fMtgGUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2yn69ypIGUn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "0ZM9MJ8XGUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JooczH7ZGUn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "78YJXkQCGUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ar4cHATNGUn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "rEuas98LGUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BjyVGREXGUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "emZ2W5TrGUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6NzushwwGUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Yvs0rDyzGUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZYWCcB3QGUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "WfwSmXGcGUoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3ZqFnAP9GUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第8章: ニューラルネット"
      ],
      "metadata": {
        "id": "NMOIVGPMGUoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "nUHYvHeRGUoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgvBFenNGUoB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "wguxJnK7GUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PKcYF6hqGUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "jCJCzpXnGUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XdTFCjbkGUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "pdkstkG4GUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y_KUWM71GUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "IHANJMStGUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9mLCYkR8GUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "l892pPbAGUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LcBewpIiGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "udN-yYPuGUoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vPCNLDTzGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "aY6Qb7Z-GUoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0-IBSIsfGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "4sdE0PmCGUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3VQMfEdOGUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "ujNB-Nt6GUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8YTVV0N0GUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第9章: RNNとCNN"
      ],
      "metadata": {
        "id": "A_fskrYuGDe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "uVFVICBTGDe3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCe-W2miGDe4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "j_ek77AlGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RS4Aon27GDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "BpfRkFReGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d02RDDCSGDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "x9Zr__PdGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QRGzx3P3GDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "HTbmQJbsGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MwLJUY7rGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "I26_r7nhGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wu-ZxkLBGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "RTrizMr7GDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JFMLkF8fGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "v49GPvdkGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YU9CoWcTGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "RvlqPNHfGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CH0Jxw1sGDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "S_n5DroHGDe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9O2SdvbNGDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第10章: 機械翻訳"
      ],
      "metadata": {
        "id": "HxRHhP-yGd-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "EPolx2ELGd-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4X5oTb2Gd-T"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "JpIxmETQGd-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oz0QHcCAGd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "GsEmSl2yGd-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_iNZMWz3Gd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "M9x9C5g-Gd-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OI3QSandGd-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "TJY2NdzDGd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TzFDDn8WGd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "na9B1uP3Gd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gRpFpJbbGd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "VgUT4sUGGd-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wbx-9ATdGd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "f_NdGNIrGd-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V4XQOeVtGd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "E4A_wEhxGd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHj95k5AGd-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "c-wQyjZOGd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4wFHPnPmGd-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}