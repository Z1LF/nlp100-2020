{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "言語処理100本ノック2020[nlp2020].ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qJbCeI_9Etpb",
        "N8LrAkjcDWMW",
        "TiHqMcclED6B",
        "zZ06yBtKEIJx",
        "8jkcGrsWEL_n",
        "SkHpEeLmEObx",
        "Imyqw0ffEQIv",
        "nIOikAjiESFx",
        "WhYuAmdaEUq9",
        "wLGsCLQpEWdC",
        "XYef5GAZEX9r",
        "oeaShrnkFXUu",
        "oreVQ8KdFXU0",
        "Ju_6iik5FXU0",
        "IH4RiaQbFXU1",
        "3tC88THbFXU1",
        "_nBP6zSgFXU1",
        "gH__H51uFXU1",
        "B-n_nvttFXU2",
        "MxfCADGMFXU2",
        "OJ_CfkueFXU2",
        "xCyzhPazFXU2",
        "bPOa1uS0FuFZ",
        "bXLpbW81FuFo",
        "9yQsM0QzFuFo",
        "27f_mJLkFuFp",
        "ByFpF3jyFuFp",
        "50TgE0duFuFq",
        "24m8DBMKFuFq",
        "b_TWlK7XFuFq",
        "Zkk3HaqCFuFr",
        "QTYd8NJ9FuFr",
        "tg1CPd0BFuFr",
        "8t09a_dWGBrU",
        "TKE39_IJMWvh",
        "WdfbPA5qGBrj",
        "whtu8E8XGBrk",
        "1yHL2j5KGBrl",
        "F-dC7WJ7GBrl",
        "gsh5EawtGBrl",
        "pdyOrgv8GBrm",
        "K2Z0Gp0RGBrm",
        "TIBgB5lkGBrm",
        "NvdNnDEsGBrn",
        "3uTbIWTWGBrn",
        "7mrrvSw6Sjku",
        "WhmGwHRUGhid",
        "jW31L0z5JH_D",
        "efvr7lJWGCvA",
        "EkCDSwTMGCvA",
        "tki4hy54GCvB",
        "tZR-DWmxGCvB",
        "KsTUgQ7qGCvB",
        "ZY2_LUvmGCvB",
        "0dFw4MJIGCvC",
        "ASM7F7ZMGCvC",
        "Ely3RRkBGCvC",
        "cNWf4DGTGUnz",
        "fFWgl4_HGUn7",
        "NMOIVGPMGUoA",
        "A_fskrYuGDe3",
        "HxRHhP-yGd-S"
      ],
      "authorship_tag": "ABX9TyOUNuqHnfaYuh0kKqsbpzDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z1LF/nlp100-2020/blob/main/%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF2020%5Bnlp2020%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 言語処理100本ノック 2020\n",
        "\n",
        "- 問題リンク[ https://nlp100.github.io/ja/ ]\n",
        "- GitHub [ https://github.com/nlp100/nlp100.github.io ]\n",
        "- 参考リンク\n",
        "  - 文字列処理に関する情報 | note.nkmk.me [ https://note.nkmk.me/string/ ] \n",
        "    >Pythonやるときはこのサイトにお世話にならないと何もできないので…\n",
        "\n",
        "- 解答例\n",
        "  - 【言語処理100本ノック 2020】Pythonによる解答例まとめ by @yanmaru\n",
        "    - Qiita [ https://qiita.com/yamaru/items/0cac24710626333bd693 ]\n",
        "    > 解答例とはいえいきなり解答が書いてあって、その後に参考リンクが記されている\n",
        "      自分で悩んで解いた後に参考程度に見るのが良さそう\n",
        "      \n",
        "  - 「言語処理100本ノック 2020」をPythonで解く by u++(upura)\n",
        "    - はてなブログ [ https://upura.hatenablog.com/entry/2020/04/14/024948 ]\n",
        "    - GitHub [ https://github.com/upura/nlp100v2020 ]\n",
        "    > ブログの方では解説や参考記事も載っているので見やすい。\n",
        "\n",
        "  - Python-機械学習-自然言語処理-言語処理100本ノック 2020\n",
        "    - ブログ [ https://www.takapy.work/archive/category/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%202020 ]\n",
        "    > 個人ブログ。解説とかはなく単純に動くコードが乗っている備忘録的なもの。\n",
        "\n",
        "  - 100本ノックシリーズ by tomowarkar\n",
        "    - GitHub blog [ https://tomowarkar.github.io/blog/posts/100series/ ]\n",
        "    > 第5章のCaboChaの導入で見つけたブログ。\n",
        "      言語処理だけじゃなくデータサイエンス100本ノックもやってるみたい。\n",
        "\n",
        "\n",
        "  - Pythonで自然言語処理100本ノック 2020を解いたついでに死ぬほど詳しく解説を書いていく[第1章 準備運動] by @python_kenichi\n",
        "    - Qiita [ https://qiita.com/python_kenichi/items/b1fcecc4274511e4c26e ]\n",
        "    > 第1章のみだが丁寧に考え方や必要な知識を解説してくれている。続きがないのが惜しまれる。\n",
        "    \n",
        "      "
      ],
      "metadata": {
        "id": "BAYv_FCnErn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第1章: 準備運動"
      ],
      "metadata": {
        "id": "qJbCeI_9Etpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 00. 文字列の逆順\n",
        "文字列\"stressed\"の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．\n",
        "\n"
      ],
      "metadata": {
        "id": "N8LrAkjcDWMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8LNmNEUDVdc"
      },
      "outputs": [],
      "source": [
        "str = \"stressed\"\n",
        "ans = str[::-1]\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01. 「パタトクカシーー」\n",
        "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．\n"
      ],
      "metadata": {
        "id": "TiHqMcclED6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str=\"パタトクカシーー\"\n",
        "print(str[::2])"
      ],
      "metadata": {
        "id": "ymEKDM4oEbw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
        "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．\n",
        "\n"
      ],
      "metadata": {
        "id": "zZ06yBtKEIJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = \"パトカー\"\n",
        "str2 = \"タクシー\"\n",
        "ans = \"\".join([(str1[i]+str2[i]) for i in range(4)])\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "PE0gdpBVEHzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 03. 円周率\n",
        "\"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．\n",
        "\n",
        "- `str.split(\"X\")` = 文字Xを基準に分割を行う\n",
        "  - デフォの引数はスペースなので別にスペース入れる必要はなかったかも\n",
        "- `str.strip(\"X\")` = 文字列の先頭・末尾にある文字Xを除去する\n",
        "\n",
        "> 最初普通にsplitだけで「できたやん！」と思ったけど出力されたリストが\n",
        "  [3,1,4,1,6...]←！？となっていてカンマの存在に気づくなど"
      ],
      "metadata": {
        "id": "8jkcGrsWEL_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
        "sep_str = str.split()\n",
        "[len(s.strip(\",. \")) for s in sep_str]"
      ],
      "metadata": {
        "id": "6mcyVFiOEOHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04. 元素記号\n",
        "\"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．\n",
        "\n",
        "- `str.strip`を使わなくても`str.replace`で良かった…てかコッチのほうが汎用性高いし…\n",
        "- len(str)=intはイテラブルなオブジェクトではないのでちゃんとrange()する。\n",
        "- `値1 if [条件式1] else 値2 if [条件式2] else 値3`\n",
        "  - というふうにするとPythonで三項演算子が実装できる。\n",
        "  - 条件式1がTrueのときは値1…という感じ。\n",
        "- 辞書は`{}`を使って定義。\n",
        "  - `Dict[Key]=Value`で辞書DictのKeyの項目に値Valueを追加する\n",
        "\n",
        "> Pythonでも三項演算子のようなものが使えるらしい。リスト内包表記といい、後置といい、ifの置き方が自由すぎる\n",
        "\n",
        "> 連想配列、どの言語でも表記が思い出せなくてうまく使えないがち。\n",
        "\n",
        "#### enumerate\n",
        "`enumerate(Object)`とすると`[インデックス番号,要素]`が戻ってくる\n",
        "- コレを使うと簡単に書ける(追記コードブロック)"
      ],
      "metadata": {
        "id": "SkHpEeLmEObx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
        "numList = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
        "str = str.replace((\",\"or\".\"),\"\").split(\" \")\n",
        "ans = {}\n",
        "\n",
        "for i in range(len(str)):\n",
        "  str_head = str[i][0] if (i+1 in numList) else str[i][0:2]\n",
        "  #print(str_head)\n",
        "  ans[str_head] = i\n",
        "\n",
        "ans"
      ],
      "metadata": {
        "id": "D6B4gJsVEPqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_str = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
        "numList = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
        "str = raw_str.replace((\",\"or\".\"),\"\").split(\" \")\n",
        "ans = []\n",
        "\n",
        "for i, word in enumerate(str):\n",
        "  str_head = word[0] if (i+1 in numList) else word[:2] \n",
        "  ans.append([str_head,i])\n",
        "\n",
        "dict(ans)"
      ],
      "metadata": {
        "id": "MtrIrjFy1AJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 05. n-gram\n",
        "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，\"I am an NLPer\"という文から単語bi-gram，文字bi-gramを得よ．\n",
        "\n",
        "> まず「n-gram」ってなんやねん！！！\n",
        "  参考 → N-gramの作り方 @kazmaw | Qiita [https://qiita.com/kazmaw/items/4df328cba6429ec210fb]\n",
        "  n-gramは文字列などにおける連続したn個のまとまりでグループ化したもの\n",
        "\n",
        "- n個連続で拾う際にカウントは「最後の文字が含まれるまで」なので範囲は`len(X)-n+1`となることに注意\n",
        "- for文で一旦書いてから(できそうなら)内包表記に変換するとやりやすいかも(主観)\n",
        "\n",
        "> n-gramがわかれば大したことはしていない\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Imyqw0ffEQIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N_gram(n,X): # オブジェクトXのn-gramを返す関数\n",
        "  ## nがオブジェクト全体の長さよりも大きい場合はとりあえず全部返すようにしようか…わかんないけど…\n",
        "  if len(X) < n:\n",
        "    return [X]\n",
        "\n",
        "  \"\"\"\n",
        "  # for文による表記\n",
        "  ret=[]\n",
        "  for i in range(len(X)-n+1):\n",
        "    ret.append(X[i:i+n])\n",
        "  \"\"\"\n",
        "  ret = [X[i:i+n] for i in range(len(X)-n+1)]\n",
        "  \n",
        "  return ret\n",
        "\n",
        "raw_text = \"I am an NLPer\"\n",
        "words = raw_text.split(\" \")\n",
        "\n",
        "print(\"--- 文字 n-gram ---\")\n",
        "print(N_gram(1,raw_text))\n",
        "print(N_gram(2,raw_text))\n",
        "print(N_gram(3,raw_text))\n",
        "\n",
        "\n",
        "print(\"--- 単語 n-gram ---\")\n",
        "print(N_gram(1,words))\n",
        "print(N_gram(2,words))\n",
        "print(N_gram(3,words))\n"
      ],
      "metadata": {
        "id": "2zS9UmqWEcpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 06. 集合\n",
        "\"paraparaparadise\"と\"paragraph\"に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．\n",
        "\n",
        "- `set`型オブジェクト [ https://note.nkmk.me/python-set/ ]\n",
        "  - set型は重複しない要素の集合オブジェクト。\n",
        "  - 定義の際は`{}`で生成できる\n",
        "    - 空のsetを作りたいときは単に`s={}`とするとdict型になってしまうので、`s=set()`と明示的にコンストラクタを呼んで定義する必要がある\n",
        "  - 内部には異なる型のオブジェクトを(変更不能なイミュータブルな型であれば)含むことが出来る。\n",
        "    - 変数やlistのような更新可能な(ミュータブルな)オブジェクトは不可能\n",
        "  - set()を利用してリストやタプルから重複要素を取り除けるが、順序は保持されない。\n",
        "\n",
        "  - 関数など\n",
        "    - `set_A | set_B` または `set_A.union(set_B)` でAとBの **和集合** を得られる\n",
        "    - `set_A & set_B` または `set_A.intersection(set_B)` でAとBの **積集合** を得られる\n",
        "    - `set_A - set_B` または `set_A.difference(set_B)` でAとBの **差集合** を得られる (AがBより大きい場合)\n",
        "    - `set_A ^ set_B` または `set_A.symmetric_difference(set_B)` でAとBの **対称差集合** を得られる (XOR=どちらか片方にのみ含まれる要素の集合)\n",
        "    - `set_A.isdisjoint(set_B)` でAとBが **互いに素かどうか** を判定できる\n",
        "\n",
        "- print内で使えるformat関数\n",
        "  - `print(\"hoge : {0}, fuga : {1}\".format(ans1, ans2)` みたいに書くと勝手にansが代入できる。\n",
        "  - 文字列と変数を同時にprintしたいときに便利\n",
        "  - 毎回使おうとするけど表記が思い出せないやつ……。\n",
        "\n",
        "- ｆ文字列 [Python v3.6以降]\n",
        "  - `f\"hoge : {ans1}, fuga : {ans2}\"`で上記のformat記法と同じ結果が出せるやつ。\n",
        "  - 書き方としてはコッチのほうがかなりわかりやすい"
      ],
      "metadata": {
        "id": "nIOikAjiESFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N_gram(n,X): # オブジェクトXのn-gramを返す関数\n",
        "  if len(X) < n:\n",
        "    return [X]\n",
        "\n",
        "  ret = [X[i:i+n] for i in range(len(X)-n+1)]\n",
        "  return ret\n",
        "\n",
        "text1 = \"paraparaparadise\"\n",
        "text2 = \"paragraph\"\n",
        "\n",
        "# 文字 bi-gram の集合\n",
        "X = N_gram(2,text1)\n",
        "setX = set(X)\n",
        "Y = N_gram(2,text2)\n",
        "setY = set(Y)\n",
        "\n",
        "\n",
        "print(\"X | Y : {0}\".format(setX | setY))\n",
        "\n",
        "print(\"X & Y : {0}\".format(setX & setY))\n",
        "\n",
        "print(\"X - Y : {0}\".format(setX - setY))\n",
        "\n",
        "\n",
        "print('\"se\" in X : {0}'.format(\"se\" in setX))\n",
        "\n",
        "print('\"se\" in Y : {0}'.format(\"se\" in setY))"
      ],
      "metadata": {
        "id": "0DA_btFBERzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 07. テンプレートによる文生成\n",
        "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=\"気温\", z=22.4として，実行結果を確認せよ．\n",
        "\n",
        "> 偶然前の問題でformat関数を解説してしまったので一瞬で解けて終わった…"
      ],
      "metadata": {
        "id": "WhYuAmdaEUq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def templete(x,y,z):\n",
        "  return \"{0}時の{1}は{2}\".format(x,y,z)\n",
        "\n",
        "print(templete(12,\"気温\",22.4))"
      ],
      "metadata": {
        "id": "YFFP2kiQEdNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 08. 暗号文\n",
        "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
        "\n",
        "* 英小文字ならば(219 - 文字コード)の文字に置換\n",
        "* その他の文字はそのまま出力\n",
        "\n",
        "この関数を用い，英語のメッセージを暗号化・復号化せよ．\n",
        "\n",
        "#### 大文字/小文字の判定\n",
        "- `str.islower()`文字列strが全て小文字のときにTrueを返す\n",
        "  - 全角文字であっても大文字小文字の区別があれば判定される\n",
        "  - 全て数字や日本語の様に大文字と小文字の区別がない文字種の時はfalse\n",
        "\n",
        "- 同様に`str.isupper()`は文字列strが全て大文字のときにTrueを返す\n",
        "- `str.istitle()`は文字列strが最初の文字だけ大文字、他はすべて小文字のときにTrueを返す\n",
        "\n",
        "#### 文字コードの変換\n",
        "- `ord('文字') = asciiコード`\n",
        "- `chr(数値) = 文字`\n",
        "\n",
        "\n",
        "> どっちの作業も無知だたので…、"
      ],
      "metadata": {
        "id": "wLGsCLQpEWdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cipher(text):\n",
        "  ans = \"\"\n",
        "  for c in text:\n",
        "    ans += chr(219-ord(c)) if(c.islower()) else c\n",
        "  return ans\n",
        "\n",
        "message = \"This Text is 2022年 Marchに 作られた 暗号です !!\"\n",
        "print(cipher(message))\n",
        "## out > Tsrh Tvcg rh 2022年 Mzixsに 作られた 暗号です !!\n",
        "\n",
        "## 219-ord(c)で暗号化したものは、219-(219-ord(c))=ord(c)なので同じ関数で複合可能\n",
        "encrypted_message = \"Tsrh Tvcg rh 2022年 Mzixsに 作られた 暗号です !!\"\n",
        "print(cipher(encrypted_message))"
      ],
      "metadata": {
        "id": "iJ71003AEWMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 09. Typoglycemia\n",
        "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば\"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"）を与え，その実行結果を確認せよ．\n",
        "\n",
        "#### ランダムな並び替え [ https://note.nkmk.me/python-random-shuffle/ ]\n",
        "- `import random`でrandomモジュールを使用可能\n",
        "- `random.shuffle(list)`で元のリストをシャッフルする。\n",
        "- `random.sample(list)`で元のリストは変更せず、シャッフルされた新たなリストを作成する。\n",
        "  - 文字列やタプルは **変更不能(イミュータブル)な** オブジェクトなので`shuffle`を使うとエラーになる！注意\n",
        "  - 適用するときは`sample`の方を使おう\n",
        "- listしかシャッフルできないのでstrもリストの形式にネスト[list]する必要がある。\n",
        "\n",
        "- 文字列の結合は`\"<Space>\".join(list)`ですね……(第2問)"
      ],
      "metadata": {
        "id": "XYef5GAZEX9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def Typoglycemia(str):\n",
        "  if len(str)<=4:\n",
        "    return str\n",
        "  else:\n",
        "    shuffle = random.sample(str[1:-1], len(str[1:-1]))\n",
        "    return \"\".join([str[0]] + shuffle + [str[-1]])\n",
        "\n",
        "text = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
        "ans = [Typoglycemia(words) for words in text.split(\" \")]\n",
        "ans = \" \".join(ans)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "IVwxEd1FEYPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第2章: UNIXコマンド\n",
        "\n",
        "popular-names.txtは，アメリカで生まれた赤ちゃんの「名前」「性別」「人数」「年」をタブ区切り形式で格納したファイルである．以下の処理を行うプログラムを作成し，popular-names.txtを入力ファイルとして実行せよ．さらに，同様の処理をUNIXコマンドでも実行し，プログラムの実行結果を確認せよ．\n",
        "\n",
        "#### Linuxコマンドの使用\n",
        "Google Colaboratory上ではコマンド部に`!`をPrefixとしてLinuxコマンドが使用できる\n",
        "\n",
        "#### データのダウンロード\n",
        "Google Colaboratory上で指定データを使用するにはwgetコマンドを使用する\n",
        "> `!wget https://nlp100.github.io/data/popular-names.txt`\n",
        "<br>これを実行するとカレントディレクトリに対象のテキストファイルを置くことが出来る"
      ],
      "metadata": {
        "id": "oeaShrnkFXUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/popular-names.txt"
      ],
      "metadata": {
        "id": "n9AOmWoLT64c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cC8xCmK74J4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. 行数のカウント\n",
        "行数をカウントせよ．確認にはwcコマンドを用いよ．\n",
        "\n",
        "#### データの読み込み\n",
        "- pandasを利用 [ https://note.nkmk.me/python-pandas-read-csv-tsv/ ]\n",
        "  ##### 読み込み時メモ\n",
        "  - `pd.read_csv`は区切り文字が`,`で固定されているだけで`pd.read_table`と根本的に同じ。\n",
        "    > `pd.read_csv(hoge)` == `pd.read_table(hoge, sep=\",\")`\n",
        "  - read時に`names`オプションでカラム名を名付ける\n",
        "\n",
        "#### 確認\n",
        "- Linuxの`wc`コマンドは`$wc <filename>`で使用する\n",
        "  - 順に[行数, 単語数(空白で区切られた文字列の数), ファイル容量(byte)]で結果が表示される\n",
        "\n",
        "> Google Colaboratoryでローカルなデータ扱うの不利じゃない！？と思ったけど普通にできるんだ。<br>\n",
        "  Linuxのコマンドも普通に使えるっぽいですね"
      ],
      "metadata": {
        "id": "oreVQ8KdFXU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXfRnG9YFXU0"
      },
      "outputs": [],
      "source": [
        "# データがディレクトリに存在するか確認\n",
        "!ls\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print(df)\n",
        "\n",
        "print(f\"行数 : {len(df)}\\n\")\n",
        "\n",
        "!wc popular-names.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. タブをスペースに置換\n",
        "タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．\n",
        "\n",
        "#### ~~「確認」~~→置換に使用するコマンド\n",
        "- sedコマンド : *__S__tream __ED__itor* `sed [option] <filename>`\n",
        "  - sedは指定したファイルをコマンドに従って処理し、標準出力へと出力する。のでパイプとして使うのが一般的。\n",
        "  - オプション`-e`で処理内容を設定(省略時はオプション以外の第一引数が処理内容)\n",
        "    - `$ cat ... | sed -e ... -e ...`のように連続した処理も可能\n",
        "  - `-s` : 正規表現を使った置換を行う。\n",
        "- trコマンド : _**TR**anslate/**TR**ansliterate_ `tr [置換前の文字セット] [置換後の文字セット] <in_filename> <out>`\n",
        "  - `$ tr 012 abc`とすると [0→a,1→b,2→c] という置換が行われる。\n",
        "    - 1対1対応なので常に長さを揃える必要がある\n",
        "  -\n",
        "- expandコマンド `expand [option] <filename>`\n",
        "  - タブを空白に置き換えるコマンド\n",
        "    - `-t`オプションでタブを幾つの空白文字に置き換えるか設定する。デフォルトは8個分。\n",
        "  - 逆に、unexpandコマンド `unexpand [option] <filename>`は空白をタブに置換する\n",
        "\n",
        "\n",
        "#### 本当に確認用のコマンド\n",
        "- `head <filename>`\n",
        "  - 指定したファイルを頭から表示する。デフォルトでは先頭10行が表示される。\n",
        "  - `-c`で文字数、`-n`で行数を指定する\n",
        "\n",
        "> なぜかsedだけTabの幅がSpaceでも残ってしまった…どうすれば消えてくれるんだ…まあべつにええか…"
      ],
      "metadata": {
        "id": "Ju_6iik5FXU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -e \"s/<tab>/ /g\" popular-names.txt | head -v -n 5\n",
        "!cat popular-names.txt | tr \"\\t\" \" \"  | head -v -n 5\n",
        "!expand popular-names.txt -t 1 | head -v -n 5"
      ],
      "metadata": {
        "id": "J7jprPLSFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. 1列目をcol1.txtに，2列目をcol2.txtに保存\n",
        "各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．\n",
        "\n",
        "- cutコマンド : `cut [option] <filename>`\n",
        "  - ファイルを読み込んで、それぞれの行から指定した部分だけを切り出すコマンド。\n",
        "  - 切り出す部分の指定は`-[範囲指定オプション] <数字>`で指定する\n",
        "    - `-b`でバイト単位、`-c`で文字数単位、`-f`でフィールド(区切り文字毎)\n",
        "    > `cut -f 1,7 --delim=\":\" hoge.txt` -> hoge.txtの\":\"で区切られた部分の1番目と7番目の列を拾ってくる。\n",
        "  - `-d`,`--delimiter` オプションでどの文字を区切りとするか指定できる。デフォルトではタブが区切り文字になっている。\n",
        "\n",
        "#### pandasを使用する場合\n",
        "pandasではデフォルトで列切り出しが可能\n",
        "- `df[n]`でn列目を拾ってこれる\n",
        "  - カラム名を指定していたらその文字列で指定する必要がある\n",
        "- `df.to_csv(<filename>, index=False)`とかやれば書き出しができる\n",
        "  - 行番号を消すオプションは`index=False`"
      ],
      "metadata": {
        "id": "IH4RiaQbFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | head -n 5\n",
        "!cut -f 1 popular-names.txt > col1.txt\n",
        "!cut -f 2 popular-names.txt | head -n 5\n",
        "!cut -f 2 popular-names.txt > col2.txt\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print(df[\"Name\"][:5])\n",
        "print(df[\"Sex\"][:5])\n"
      ],
      "metadata": {
        "id": "WkpEVhdTFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. col1.txtとcol2.txtをマージ\n",
        "12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．\n",
        "\n",
        "- Pasteコマンド : `$ paste <file_1> <file_2>`\n",
        "  - 複数のファイルを行で連結するコマンド。\n",
        "  - `-d`オプションを使うと連結時の区切り文字を変更できる。デフォルトではタブ。\n",
        "\n",
        "#### pandasを使用する場合\n",
        "特定の行・列を抽出する場合はlocが結局無難\n",
        "- `df.loc[行 , 列]` : 指定した行と列を拾ってくる。\n",
        "  - 行・列は複数のlist表記も可能。\n",
        "- `df.iloc[行 , 列]` : 「数字で」指定した行と列を拾ってくる。\n",
        "  - locの指定がindex名やcolumn名なのに対してこちらは番号で指定できる"
      ],
      "metadata": {
        "id": "3tC88THbFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!paste col1.txt col2.txt | head -n 5\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print( df.iloc[0:5,0:2] )"
      ],
      "metadata": {
        "id": "CnuWxJRTFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. 先頭からN行を出力\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．\n",
        "\n",
        "#### headコマンド\n",
        "ファイルの上からn行を表示する。\n",
        "> もう今まで確認で散々使っとるやんけ……………！！！！\n",
        "\n",
        "\n",
        "#### Pythonでコマンドライン引数を受け取る\n",
        "`import sys`モジュール内の`argv`を使用する。\n",
        "- `args = sys.argv`で呼び出し時にコマンドラインによって与えた引数がargsに格納される\n",
        "- `args[0]`は実行ファイル名(`hoge.py`)になる\n"
      ],
      "metadata": {
        "id": "_nBP6zSgFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head popular-names.txt -n 5\n",
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowHead(df,n):\n",
        "  print(df.head(n))\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "\n",
        "ShowHead(df,args[1]) if len(args)==2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "8VFMVOOIFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. 末尾のN行を出力\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．\n",
        "\n",
        "> 末尾を参照するTailになっただけでやることは1個前と完全に同じ。"
      ],
      "metadata": {
        "id": "gH__H51uFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tail popular-names.txt -n 5\n",
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowTail(df,n):\n",
        "  print(df.tail(n))\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "ShowTail(df, 3)\n",
        "ShowTail(df,args[1]) if len(args)==2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "1j0SXdREFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. ファイルをN分割する\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．\n",
        "\n",
        "#### splitコマンド\n",
        "`!split -n 5 popular-names.txt`みたいな感じにすれば分割できるけど、分割したらファイルが勝手に出力されるので面倒なことになってしまう。Pandasだけでええか…\n",
        "\n",
        "\n",
        "> これもやってることは「分割の計算」→「切り出し」なので、表示さえできればやるだけ"
      ],
      "metadata": {
        "id": "B-n_nvttFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!split -n 5 popular-names.txt | head -n 5\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowHeadAndTail(df,n=2): ## 全部表示すると流石に面倒なので上下のn行だけ表示するようにして確認\n",
        "  print(df.head(n))\n",
        "  print(\" ……… \")\n",
        "  print(df.tail(n))\n",
        "\n",
        "def Separate(df,n):\n",
        "  size = int(len(df)/n) # 先に区切りの数を決めておくとキャスティングが統一されてラク\n",
        "\n",
        "  for i in range(n):\n",
        "    ShowHeadAndTail(df[i*size:(i+1)*size], 2)\n",
        "    print(\"----- -----\")\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "args[1] = 5 if len(args)>=2 else args[1]\n",
        "Separate(df,args[1]) if len(args)>=2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "428FqOK8FXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. １列目の文字列の異なり\n",
        "1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはcut, sort, uniqコマンドを用いよ．\n",
        "\n",
        "#### uniq コマンド\n",
        "重複している行を取り除くコマンド。 [ https://atmarkit.itmedia.co.jp/ait/articles/1611/14/news021.html ]\n",
        "- ただし、**uniqコマンドは隣り合った行しか比較しない**ので、先にsortコマンドを使って並べ替えるのが一般的らしい\n",
        "\n",
        "ちなみに、sortコマンドに`-u`という重複行を取り除くオプションがあるらしいのであんまり意味ないな…\n",
        "\n",
        "\n",
        "#### Pythonを使用した解答\n",
        "> 先の問題で「set」を使用した重複の排除があったので思わずそっちを使ったが、Pandasくんに便利な機能がある\n",
        "\n",
        "- `df.duplicated()` : 重複した行を抽出する\n",
        "  - 戻り値は重複した行がTrueとなるBool値の行。デフォルトでは全ての列要素が一致したときに同じ行とみなされる。\n",
        "  - 引数`subset`を指定することで同じ要素であるときに重複と判定するカラムを指定できる\n",
        "    - `df.duplicated(subset=\"Name\")`みたいな感じ\n",
        "- `df.drop_duplicates()` : 重複した行を削除する\n",
        "  - この辺のメソッドを使用すると重複した行をカウントできる"
      ],
      "metadata": {
        "id": "MxfCADGMFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | sort | uniq | wc"
      ],
      "metadata": {
        "id": "8EXYXKElDMpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "Column_1_set = set(df.iloc[:,0])\n",
        "len(Column_1_set)"
      ],
      "metadata": {
        "id": "H8RZ6MkJFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. 各行を3コラム目の数値の降順にソート\n",
        "各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．"
      ],
      "metadata": {
        "id": "OJ_CfkueFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sMC0YogFFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "df.sort_values([\"Population\"])"
      ],
      "metadata": {
        "id": "fDnTABk3Enmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる\n",
        "各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．\n",
        "\n",
        "- `uniq -c`でuniqの重複時に出現回数をカウントする\n",
        "- `sort -n`でソート時に数字として認識する\n",
        "  - コレをしないと辞書順で判定されるため [1743] が [94] より先に出てくることになる\n",
        "\n",
        "#### Pandasを使用した解答\n",
        "Pandasを利用するのであれば`pandas.Series.value_counts()`を使用することでユニークな要素のindexと個数を取得できる\n",
        "- デフォルトでは出現回数が多いものから順にソートされる\n",
        "  - 引数ascending=Trueとすると昇順にソートされ、引数sort=Falseとするとソートされない。"
      ],
      "metadata": {
        "id": "xCyzhPazFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | sort | uniq -c | sort -n -r | head -n 10"
      ],
      "metadata": {
        "id": "e5jxJB9_F2M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "df[\"Name\"].value_counts()"
      ],
      "metadata": {
        "id": "4S_PWR7RFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第3章: 正規表現\n",
        "\n",
        "Wikipediaの記事を以下のフォーマットで書き出したファイルjawiki-country.json.gzがある．\n",
        "- 1行に1記事の情報がJSON形式で格納される\n",
        "- 各行には記事名が”title”キーに，記事本文が”text”キーの辞書オブジェクトに格納され，そのオブジェクトがJSON形式で書き出される\n",
        "- ファイル全体はgzipで圧縮される\n",
        "\n",
        "らしいので例によってまずデータをローカルに保存します…。\n",
        "- `!wget https://nlp100.github.io/data/jawiki-country.json.gz`\n",
        "- `!gunzip jawiki-country.json.gz`\n",
        "\n",
        "とすると`jawiki-country.json`がローカルに配置される。\n",
        "> ちなみに、gzipを解凍(伸張)すると元の圧縮ファイルは自動で消去される。\n",
        "  そのため実質的には変換を行っているのと同等。"
      ],
      "metadata": {
        "id": "bPOa1uS0FuFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/jawiki-country.json.gz"
      ],
      "metadata": {
        "id": "ndIAHDnkJySw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip jawiki-country.json.gz"
      ],
      "metadata": {
        "id": "T2vF6vFhKR2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "VyJWZWXKzbz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. JSONデータの読み込み\n",
        "Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．\n",
        "**問題21-29では，ここで抽出した記事本文に対して実行**せよ．\n",
        "\n",
        "#### Pandasを使用したJSONの扱い\n",
        "- `pd.read_json(<filename>)`でJSON形式のファイルを読み込める\n",
        "  - JSON Lines形式の場合は引数を`lines=True`とする必要がある\n",
        "    - JSON LinesとよばれるJSONが改行で区切られたフォーマット(拡張子が`.jsonl`の時もある)\n",
        "  - `compression='infer'`と引数を設定すると**対応する圧縮方式が自動で選ばれて圧縮ファイルを読み込める**\n",
        "    - 対応する圧縮方式は{`.gz`,`.bz2`,`zip`,`xz`}など。\n",
        "\n",
        "> PandasってJSONファイルも読めんの！！\n",
        "<br>えっしかもgzファイルのまま読めんの！？！？？！？すご！？？！？！\n",
        "\n",
        "- pandas.dataframeはbool値をもつリストでカバーをすることが出来る\n",
        "  - `df = [A,B,C]`とあったときに、`X = [False,True,False]`のListXを使って\n",
        "    `df[X]`とすると、`df[X] -> B`となる\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXLpbW81FuFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzAmFuTOFuFo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "\n",
        "print(df[\"title\"]==\"イギリス\")\n",
        "print(df[df[\"title\"]==\"イギリス\"])\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "#print(uk_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(uk_text)"
      ],
      "metadata": {
        "id": "Aqn47zdm4rCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. カテゴリ名を含む行を抽出\n",
        "記事中でカテゴリ名を宣言している行を抽出せよ．\n",
        "\n",
        "textをみるとカテゴリ宣言は`Category:イギリス`のようにされているのでこの部分を含む行を抽出できるようにすれば良い\n",
        "\n",
        "#### Pythonを使用した文字列の検索(grep的処理)\n",
        "[https://note.nkmk.me/python-grep-like/]\n",
        "\n",
        "\n",
        "#### Python内で正規表現を扱う`re`モジュール\n",
        "[https://note.nkmk.me/python-re-match-search-findall-etc/]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yQsM0QzFuFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forによる表記\n",
        "\"\"\"\n",
        "for line in uk_text.split(\"\\n\"):\n",
        "  if \"[Category:\" in line:\n",
        "    print(line)\n",
        "\"\"\"\n",
        "# リスト内包表記 + if条件式\n",
        "Category_lines = [line for line in uk_text.split(\"\\n\") if \"[Category\" in line]\n",
        "print(Category_lines)"
      ],
      "metadata": {
        "id": "m0al_VEzFuFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. カテゴリ名の抽出\n",
        "記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．\n",
        "\n",
        "#### reモジュールで正規表現\n",
        "- `match()` : マッチオブジェクト\n",
        "  - 位置を取得 : 先頭`start()`, 末尾`end()`, 範囲`span()`\n",
        "  - 文字列を取得 : `group()`\n",
        "  - 各グループの文字列を取得 : `groups()`\n",
        "- `search()` : 文字列全てから検索し、マッチした場合にマッチオブジェクトを返す。\n",
        "  - ただし、マッチする部分が複数ある場合は最初のマッチのみが返されるため、1つずつ処理したい時以外は他のを使うことになる\n",
        "- `findall()` : マッチするすべての部分文字列をリストにして返す。\n",
        "- `finditer()` : マッチするすべての部分文字列のマッチオブジェクトをイテレータで返す。\n",
        "  > 多分この2種を使う。\n",
        "  - 正規表現の一部に()を使用すると、マッチングした際に()内の部分のみ[Group]を返すようになる。\n",
        "  - 括弧で囲む際に先頭に`?P<hoge>`と記述することで該当グループに任意の名前\"hoge\"をつけることが出来る。\n",
        "    - このグループ名はマッチングオブジェクトに対してgroup()などで名前を指定して取得できる様になる。\n",
        "  - \n",
        "\n",
        "\n",
        "\n",
        "#### エスケープシーケンスの無効化 (raw文字列)\n",
        "[https://note.nkmk.me/python-raw-string-escape/]\n",
        "正規表現の際は`\\(=バックスラッシュ)`を多用することが多い。これがエスケープシーケンスとして認識されるのを防ぐため、**raw文字列**として扱う。\n",
        "- 文字列リテラル`\"hoge\"`などを表記する際、その直前に`r`または`R`をつけるとそのままの値が文字列となる\n",
        "  - `\"a\\tb\\nA\\tB\"` → \"a[tab]b[改行]A[tab]B\" // 普通に入れるとエスケープシーケンスが展開される\n",
        "  - `a\\\\tb\\\\nA\\\\tB` → \"a\\tb\\nA\\tB\" // バックスラッシュをエスケープシーケンスで表記した場合\n",
        "  - `r\"a\\tb\\nA\\tB\"` → \"a\\tb\\nA\\tB\" // エスケープシーケンスを無効化したraw文字列\n",
        "\n",
        "#### マッチングの除外\n",
        "今回は「イギリス｜＊」のようにマッチングしているが拾わないで欲しい文字列も存在している。\n",
        "こういうときに`(?:hoge)`みたいな表記したら良い\n",
        "> らしいけどちょっとココ以外のソースが見つからんかったので詳しくはよくわかりません…\n",
        "  [https://qiita.com/yamaru/items/255d0c5dcb2d1d4ccc14#22-%E3%82%AB%E3%83%86%E3%82%B4%E3%83%AA%E5%90%8D%E3%81%AE%E6%8A%BD%E5%87%BA]\n",
        "\n",
        "- Pythonの公式Docに書いてあった！[https://docs.python.org/ja/3/library/re.html]\n",
        "\n",
        "> `(?:...)`\n",
        "普通の丸括弧の、キャプチャしない版です。丸括弧で囲まれた正規表現にマッチしますが、このグループがマッチした部分文字列は、マッチを実行したあとで回収することも、そのパターン中で以降参照することもできません 。"
      ],
      "metadata": {
        "id": "27f_mJLkFuFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# forによる表記\n",
        "Category_Matches = re.findall(r\"\\[Category:(.+?)(?:\\|.*)?\\]\",uk_text)\n",
        "\n",
        "for iter in Category_Matches:\n",
        "  print(iter)\n",
        "\n"
      ],
      "metadata": {
        "id": "rP6hJJB2FuFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. セクション構造\n",
        "記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ．\n",
        "\n",
        "> textを読む感じ、多分`==hoge==`だと1で`===hoge===`だと2みたいな感じなんだと思う。わからんけど。\n",
        "\n",
        "> 正規表現とにらめっこしつつ試行錯誤してたらうまくハマってキレイに取り出せたのでそのままlen()でレベル判定して終わり。"
      ],
      "metadata": {
        "id": "ByFpF3jyFuFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# forによる表記\n",
        "Section_Matches = re.findall(r\"(=+=)(?P<SectionName>[^=]+)(=+=)\",uk_text)\n",
        "\n",
        "for iter in Section_Matches:\n",
        "  #print(iter)\n",
        "  print(f\"SectionLevel : {iter[1]} = {len(iter[0])-1}\")"
      ],
      "metadata": {
        "id": "yAzfIE3NFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. ファイル参照の抽出\n",
        "記事から参照されているメディアファイルをすべて抜き出せ．\n",
        "\n",
        "> メディアファイルを探してみると\n",
        "  `[ファイル:Royal Coat of Arms of the United Kingdom.svg|85px|イギリスの国章]`みたいな感じなので\n",
        "  `\"[ファイル:(.*)|*|*]\"`みたいなかんじで取り出せそう\n",
        "\n",
        "- `|`は複数条件(OR)として機能するのでエスケープする必要がありました\n",
        "- 大カッコ [ ] で囲まれているものの、カッコ内の説明は割と不統一な感じなのでファイル名だけ取得できれば充分みたい"
      ],
      "metadata": {
        "id": "50TgE0duFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#Media_Matches = re.findall(r\"\\[(ファイル:)(.+)\\|(.+)\\|(.+?)\\]\",uk_text)\n",
        "Media_Matches = re.findall(r\"\\[(ファイル:)(.+?)\\|\",uk_text)\n",
        "\n",
        "for iter in Media_Matches:\n",
        "  #print(iter)\n",
        "  print(f\"Media File : {iter[1]}\")"
      ],
      "metadata": {
        "id": "DOD2R7vZFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. テンプレートの抽出\n",
        "記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．\n",
        "\n",
        "> 「基礎情報」を探してみると\n",
        "  `{{基礎情報 |hoge = fuga ... }}`となっていたので、\n",
        "  一旦基礎情報だけぶっこ抜いてきてその中でリスト作るようにしたほうが良いかも\n",
        "\n",
        "#### re.compile()\n",
        "今まで普通に`re.findall()`でやってきたけど、正規表現を記述してコンパイルした正規表現パターンのオブジェクトを作って、そのメソッドとして実行するのが割と一般的な使い方っぽい\n",
        "- `pattern = re.compile(<使いたい正規表現>)` : 正規表現パターンオブジェクトの作成\n",
        "  - `pattern.match(<String>)`\n",
        "  - `re.<任意のメソッド>(pattern, text)`\n",
        "  \n",
        "  のような使い方が可能\n",
        "\n",
        "\n",
        "\n",
        "#### 改行を含むマッチング\n",
        "- `re.S` = `re.DOTALL` : 使うとワイルドカード文字`.`が改行を含むあらゆる文字にマッチする\n",
        "  - 通常のドットは改行文字は含まれない\n",
        "- `re.M` = `re.MULTILINE` : 使うと複数行に対して検索を行う"
      ],
      "metadata": {
        "id": "24m8DBMKFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "#print(Basic_Info_Text) ## 基礎情報の部分のみ一旦テキスト抽出\n",
        "\n",
        "# p_Basic_Info = re.compile(r\"\\|(.+)[\\s*]=[\\s*](.+)\")\n",
        "# >> これだと公式国名みたいに途中で改行が入ってるやつが拾えない…\n",
        "p_Basic_Info = re.compile(r\"(?:\\|)(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)|(?=\\n))\",re.M+re.S)\n",
        "# > 2時間くらい格闘したけどわからなかったのでちょっとパスします…\n",
        "# >> 諦めてたら次の問題でコレの回答を使うみたいだったので悩みながら他の人の答え(たかぴーさんのブログ)をみたら(?=\\n)の前にスペースを入れるだけで解決した。なぜ？？？\n",
        "# >>> [|標語 : ...]となって標語だけうまく取り出せてない\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "print(Basic_Info_List)\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "## test ##\n",
        "Basic_Info_Dict[\"公式国名\"]\n",
        "for k,v in Basic_Info_Dict.items():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "1XqoqDsHFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 26. 強調マークアップの除去\n",
        "25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ\n",
        "- (参考: マークアップ早見表[ http://ja.wikipedia.org/wiki/Help:%E6%97%A9%E8%A6%8B%E8%A1%A8 ])\n",
        "\n",
        "> リンク先によると\n",
        "- `''他との区別''`\t他との区別\n",
        "- `'''強調'''`\t強調\n",
        "- `'''''斜体と強調'''''`\t斜体と強調\n",
        "\n",
        "> らしいので、'の2個以上のマッチングを調べて取り除けば良さそう\n",
        "\n",
        "\n",
        "> 色々元テキストから消そうと努力してたけど先に作ってる辞書で表示するときだけ`'`を弾けば良いのか……賢いな………"
      ],
      "metadata": {
        "id": "b_TWlK7XFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"\\|(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)| (?=\\n))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "# 強調を消す\n",
        "# Basic_Info_NoEmphasis = {i[0]:re.sub(r\"\\'{2,}\", \"\", i[1]) for i in Basic_Info_List}\n",
        "\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "for k,v in Basic_Info_Dict.items():\n",
        "  v = re.sub(r\"\\'{2,}\", \"\", v)\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "JhzVyAnbFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 27. 内部リンクの除去\n",
        "26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: マークアップ早見表）\n",
        "\n",
        "> マッチングした文字列の一部を置換結果として利用したい……。\n",
        "\n",
        "#### `re.sub()`を使用した置換にマッチング文字列の一部を使用する\n",
        "`re.sub()`では正規表現の中でキャプチャグループが含まれている場合、置換する文字列の中で `\\1, \\2, ...` を使用することでキャプチャグループでキャプチャされた文字列を参照することができます。\n",
        "- 参考リンク [ https://www.javadrive.jp/python/regex/index10.html ]\n",
        "\n",
        "> なぜかこれは **0ではなくて1始まり** なので注意\n",
        "\n",
        "色々頑張ったけど、なぜか\n",
        "  - 国章リンク =（[[イギリスの国章|国章]]）<br>\n",
        "\n",
        "だけが消えなかった。\n",
        "正規表現チェッカー[ https://weblabo.oscasierra.net/tools/regex/ ]で確認してもちゃんとヒットしてるのになぜ？？？"
      ],
      "metadata": {
        "id": "Zkk3HaqCFuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"\\|(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)| (?=\\n))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "# 強調を消す\n",
        "p_remove_Emphasis = re.compile(r\"\\'{2,}\",re.S)\n",
        "Basic_Info_NoEmphasis = {i[0]:p_remove_Emphasis.sub(\"\", i[1]) for i in Basic_Info_List}\n",
        "\n",
        "# 内部リンクを消す\n",
        "p_remove_Link = re.compile(r\"\\[\\[(?:[^|]*\\|)*?([^|]*?)\\]\\]\",re.S+re.M)\n",
        "# [[hoge]] -> hoge , [[hoge(#foo)|fuga]] -> fuga にしたい\n",
        "## [[(.+)]] , [[.+|(.+)]]\n",
        "Basic_Info_NoLink = {i[0]:p_remove_Link.sub(r\"\\1\", i[1]) for i in Basic_Info_NoEmphasis.items()}\n",
        "\n",
        "ans = dict(Basic_Info_NoLink)\n",
        "\n",
        "for k,v in ans.items():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "QJUucQvMFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 28. MediaWikiマークアップの除去\n",
        "27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．\n",
        "\n",
        "> 普通にマークアップを何処まで消して良いのかわからんからわからん……\n",
        "  とりあえず見てて気になるところだけやった"
      ],
      "metadata": {
        "id": "QTYd8NJ9FuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.*?$(.*?)^\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "def remove_markup(text):\n",
        "\n",
        "  # 強調を消す\n",
        "  p_remove_Emphasis = re.compile(r\"\\'{2,}\",re.S)\n",
        "  text = p_remove_Emphasis.sub(\"\", text)\n",
        "\n",
        "  # 内部リンクを消す\n",
        "  p_remove_Link = re.compile(r\"\\[\\[(?:[^|]*\\|)*?([^|]*?)\\]\\]\",re.S+re.M)\n",
        "  text = p_remove_Link.sub(r\"\\1\", text)\n",
        "\n",
        "  # 箇条書き消す\n",
        "  p_remove_kajou = re.compile(r\"^\\*\",re.S+re.M)\n",
        "  text = p_remove_kajou.sub(\"\", text)\n",
        "\n",
        "  # 外部リンク消す\n",
        "  p_remove_otherLink = re.compile(r\"[\\{\\[].*http.+[\\}\\]]\",re.S+re.M)\n",
        "  text = p_remove_otherLink.sub(\"\", text)\n",
        "\n",
        "  # HTMLタグ消す\n",
        "  p_remove_HTMLtag = re.compile(r\"<.+?>\",re.S+re.M)\n",
        "  text = p_remove_HTMLtag.sub(\"\", text)\n",
        "  \n",
        "  # {{langとか仮リンクとか|なんちゃら|かんちゃら}}←みたいなやつの最後だけ取り出す(よくわからん)\n",
        "  p_remove_Nazo = re.compile(r\"\\{\\{(?:.*\\|)*(.*?)\\}\\}\",re.S+re.M)\n",
        "  text = p_remove_Nazo.sub(r\"\\1\", text)\n",
        "\n",
        "  # 改行消す\n",
        "  p_remove_kaigyou = re.compile(r\"$|\\n\",re.S+re.M)\n",
        "  text = p_remove_kaigyou.sub(\"\", text)\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  \n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "ans = dict(Basic_Info_List)\n",
        "\n",
        "for k,v in ans.items():\n",
        "  print(f\"{k} : {remove_markup(v)}\")"
      ],
      "metadata": {
        "id": "sPM9IzeTFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 29. 国旗画像のURLを取得する\n",
        "テンプレートの内容を利用し，国旗画像のURLを取得せよ．\n",
        "（ヒント: MediaWiki APIのimageinfoを呼び出して，ファイル参照をURLに変換すればよい）\n",
        "\n",
        "> 急にAPIの話出てきた～～～何もわかんね～～～！\n",
        "\n"
      ],
      "metadata": {
        "id": "tg1CPd0BFuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.*?$(.*?)^\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "uk_flag_URL = Basic_Info_Dict[\"国旗画像\"]\n",
        "\n",
        "# ヒントのページ見たけどな～～～んもわからんかったので流石にココだけ他の方の答えのソースを無心でコピペしました…\n",
        "import requests\n",
        "uk_flag_URL = uk_flag_URL.replace(\" \",\"_\")\n",
        "url = 'https://commons.wikimedia.org/w/api.php?action=query&titles=File:' + uk_flag_URL + '&prop=imageinfo&iiprop=url&format=json'\n",
        "data = requests.get(url)\n",
        "\n",
        "print(re.search(r'\"url\":\"(.+?)\"',data.text).group(1))\n",
        "# 正直webAPI周りの箇所がマジで何やってるか何もわからん……恥ずかし……"
      ],
      "metadata": {
        "id": "PNyrDcUNFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第4章: 形態素解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "なお，問題37, 38, 39はmatplotlibもしくはGnuplotを用いるとよい．\n",
        "\n",
        "> らしいので例によってデータをローカルに(ry\n",
        "\n",
        "- `!wget https://nlp100.github.io/data/neko.txt`\n",
        "\n",
        "> 調べてようやく知ったけど`wget`コマンドで`-O`を指定すると常にファイル名を指定して保存できるので、名前が被ったときに「`hoge.tmp`と`hoge1.tmp`ができちゃった～～」みたいな事にならないですむらしい。"
      ],
      "metadata": {
        "id": "8t09a_dWGBrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/neko.txt -O neko.txt\n",
        "!ls"
      ],
      "metadata": {
        "id": "aiHHBe0FgYs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> いきなり当然の権利のようにmecabを使ってみろと言ってくるので…\n",
        "\n",
        "#### MeCabのインストール\n",
        "MeCabは普通に外部ライブラリなのでインストールする必要がある。\n",
        "\n",
        "- 一応念の為 `!pip show mecab`してみたけど出ませんでした\n",
        "  > `WARNING: Package(s) not found: mecab`\n",
        "  \n",
        "  - 後で気付いたけどパッケージ名は`mecab-python3`らしいです…\n",
        "\n",
        "\n",
        "- ColaboratoryでMeCabを使えようにする。 - Qiita\n",
        "  - [ https://qiita.com/pytry3g/items/897ae738b8fbd3ae7893 ]\n",
        "\n",
        "コチラの記事を参考にpipを使ってMeCabをインストールする。\n",
        "\n",
        "\n",
        "あ、でも今回の問題はMeCab使って形態素解析の結果を出す作業が要るので一旦コマンドラインでMeCabを叩いてみることにします。\n",
        "\n",
        "> この手のやつ、Google Colaboratoryだと落ちるたびに上から順にやっていかないといけないからめんどくさいなぁ。\n",
        "  などと…\n",
        "  "
      ],
      "metadata": {
        "id": "TKE39_IJMWvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loeNRp3eGBrk"
      },
      "outputs": [],
      "source": [
        "!apt install aptitude\n",
        "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.7\n",
        "\n",
        "# グラフ表示用\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "!pip show mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mecab neko.txt -o neko.txt.mecab\n",
        "!head neko.txt.mecab -n 20"
      ],
      "metadata": {
        "id": "TtpdVo1BnioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 30. 形態素解析結果の読み込み\n",
        "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
        "\n",
        "\n",
        "#### 形態素解析について\n",
        "さらっと「各形態素は～として表現せよ」って言われたけど何もわからんね…\n",
        "\n",
        "> MeCabが形態素解析結果として返す情報は次のような内容である：\n",
        "\n",
        "    表層形  品詞  品詞細分類1  品詞細分類2  品詞細分類3  活用形  活用型  原形  読み  発音\n",
        "\n",
        "> 「一文を形態素のリストとして表現せよ」とあるので\n",
        "  \n",
        "    [<surface(表層形)>, <base(基本形)>, <pos(品詞)>, <pos1(品詞細分類1)>]\n",
        "    をもとに、\n",
        "    list(dict[\"<形態素名(品詞とか)>\"])\n",
        "という感じで取り出せるようにすればいいっぽい\n",
        "\n",
        "……\n",
        "\n",
        "やれば良いことはわかったけど、ファイルの読み込み方とか全くわからん…。\n",
        "最初の問題なのでおとなしく写経しようかしら…\n",
        "\n",
        "> ちまちまやってたら出来た。\n",
        "  作成方針は以下の流れ\n",
        "\n",
        "1. (形態素解析の結果の)ファイルを行ごとに読み込む\n",
        "2. 形態素解析の結果を元に辞書を作る(とりあえず全部)\n",
        "  - このとき `\"\\n\"` や `\"\"(空白文字)` を省く\n",
        "3. 上記の処理を節毎(`\"EOS\"`で区切る)にする。\n",
        "4. 節毎に辞書をリストにして追加するようにする\n"
      ],
      "metadata": {
        "id": "WdfbPA5qGBrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[0:30]:\n",
        "    if line != \"EOS\\n\":\n",
        "      # line = 初めて\t副詞,一般,*,*,*,*,初めて,ハジメテ,ハジメテ ←こんなかんじ\n",
        "      # [表層形  品詞  品詞細分類1  品詞細分類2  品詞細分類3  活用形  活用型  原形  読み  発音]\n",
        "      ## ここから [表層形(0番目), 基本形(?????多分原型？だとしたら7番目), 品詞(1番目), 品詞細分類1(2番目)] を取り出す\n",
        "      word = line.split(\"\\t\")\n",
        "      # word = ['吾輩', '名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\\n'] ←こんなかんじ\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        # 改行文字、空白文字を省く\n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      # 節末で溜めた形態素解析の結果を「辞書のリスト」として追加\n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "IPuDD6i_PLyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 31. 動詞\n",
        "動詞の表層形をすべて抽出せよ．\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ\n"
      ],
      "metadata": {
        "id": "whtu8E8XGBrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if (m[\"pos\"]==\"動詞\") : print(m[\"surface\"]) "
      ],
      "metadata": {
        "id": "IIAtIF5mGBrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32. 動詞の基本形\n",
        "動詞の基本形をすべて抽出せよ．\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ2\n",
        "  31番の `\"surface\"` を `\"base\"` にするだけなのでマジで5秒とかで解ける\n"
      ],
      "metadata": {
        "id": "1yHL2j5KGBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if (m[\"pos\"]==\"動詞\") : print(m[\"base\"]) "
      ],
      "metadata": {
        "id": "q6ub1Uf3GBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33. 「AのB」\n",
        "2つの名詞が「の」で連結されている名詞句を抽出せよ．\n",
        "\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ3\n",
        "\n",
        "> …かと思ったけどそうでもなかった\n",
        "  とはいっても「の」の前後を取り出して名詞かどうか判定するだけ"
      ],
      "metadata": {
        "id": "F-dC7WJ7GBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def is_MeishiNoMeishi(sentence):\n",
        "  ans = []\n",
        "  for s in sentence:\n",
        "    for i in range(len(s)-1):\n",
        "      if s[i][\"surface\"]==\"の\":\n",
        "        if (s[i-1][\"pos\"]==\"名詞\") & (s[i+1][\"pos\"]==\"名詞\"):\n",
        "          ans.append(s[i-1][\"surface\"]+s[i][\"surface\"]+s[i+1][\"surface\"])\n",
        "  return ans\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = is_MeishiNoMeishi(sentence)\n",
        "print(len(ans))\n",
        "print(ans)\n"
      ],
      "metadata": {
        "id": "8OxWIj3GGBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 34. 名詞の連接\n",
        "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．\n",
        "\n",
        "> 最長一致…貪欲だな！(思考完)\n",
        "\n",
        "> …というわけにもいかんかった\n",
        "  連接なので1(その他→名詞→その他)のときは拾ってはいけないので面倒\n",
        "  でも普通に長さを保持する変数入れるだけで解決した…\n",
        "  あんまり汎用性の高い書き方じゃないから通用はしなさそうだけど…"
      ],
      "metadata": {
        "id": "gsh5EawtGBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def Meishi_renzoku(sentence):\n",
        "  ans = []\n",
        "  for s in sentence:\n",
        "    tmp = \"\"\n",
        "    length = 0\n",
        "    for m in s:\n",
        "      if(m[\"pos\"]==\"名詞\"):\n",
        "        tmp += m[\"surface\"]\n",
        "        length += 1\n",
        "      else:\n",
        "        if(length>=2):\n",
        "          ans.append(tmp)\n",
        "          print(tmp)\n",
        "        tmp = \"\"\n",
        "        length = 0\n",
        "  return ans\n",
        "\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = Meishi_renzoku(sentence)\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "Dv2BboHrGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 35. 単語の出現頻度\n",
        "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．\n",
        "\n",
        "#### リストからDataFrameの作成\n",
        "参考 [ https://www.delftstack.com/ja/howto/python-pandas/pandas-create-dataframe-from-list/ ]\n",
        "\n",
        "リストからDataFrameの作成ってできるんだ…\n",
        "いやまあそりゃできるか…ファイルの読み込みで使う機会が多いというだけで…\n",
        "\n",
        "> 出現頻度の計算は以前のノック(19番)でやっているのでそれを活用\n",
        "  普通に単語でDict作ってカウントしてもいいけどなんとなくコッチのほうが良いかなって…\n",
        "  表層形と基本形どっちでカウントするのが良いのかわからなかったけど、解答例見る感じ基本形(Base)でやっていたのでそっちを採用。\n",
        "  >> 言語処理だとコッチが一般的？\n",
        "\n",
        "> 句点と読点とか、カギカッコは除外しても良かったかもしれんわね…"
      ],
      "metadata": {
        "id": "pdyOrgv8GBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    #print(m.values())\n",
        "    ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "print(df.value_counts(\"base\")[:20])"
      ],
      "metadata": {
        "id": "N-2DUVBTGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 36. 頻度上位10語\n",
        "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．\n",
        "\n",
        "> なんかグラフのときはmatplotlibかGNUplotを使ったほうが良いみたいなの言ってたな\n",
        "\n",
        "軽く調べたらGnuplotは元々外部のフリーソフトらしいので、今回はPythonで汎用的に使えそうなmatplotlibの方を採用。\n",
        "\n",
        "> …と思ったが、調べていたらpandasのDataFrameには\n",
        "  `DataFrame.plot`というド直球なメソッドが存在しているらしくてすごいなあと思いました(小並感)\n",
        "\n"
      ],
      "metadata": {
        "id": "K2Z0Gp0RGBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    #print(m.values())\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = (df.value_counts(\"base\"))[0:10]\n",
        "print(top_words)\n",
        "\n",
        "# グラフ化\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "top_words.plot.bar()\n",
        "\n",
        "\"\"\"\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(top_words[:10])\n",
        "pyplot.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nBZLXxluGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 37. 「猫」と共起頻度の高い上位10語\n",
        "「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．\n",
        "\n",
        "> 「共起頻度」is なに？？？？\n",
        "\n",
        "#### 共起頻度\n",
        "あんまり詳しい解説はなかったんだけど…\n",
        "\n",
        "    共起 : コロケーション (Co-location)\n",
        "    ある単語同士が同一文中に出現する組み合わせ\n",
        "\n",
        "つまり、「共起頻度はある単語の組み合わせで同一文中に登場する頻度の高い組み合わせ」\n",
        "というのがざっとした理解。\n",
        "\n",
        "> なので今回は文章中に「猫」という単語を含んでいるものを取り出し、\n",
        "  そこから単語1つ1つをカウントするだけで良さそう。\n",
        "  \n",
        ">> 本来は「組み合わせ」なので[A-B]という組み合わせで取り出すべきだが、\n",
        "  今回は「猫」という単語を指定されているので1つずつ見てカウントしていくだけで良い"
      ],
      "metadata": {
        "id": "TIBgB5lkGBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  #print([word[\"base\"] for word in s])\n",
        "  if not \"猫\" in [word[\"base\"] for word in s]: continue\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "df_CatDrop = df[df[\"base\"]!=\"猫\"]\n",
        "\n",
        "top_words = (df_CatDrop.value_counts(\"base\"))\n",
        "print(top_words[0:10])\n",
        "\n",
        "# グラフ化\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "top_words[0:10].plot.bar()\n",
        "\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HslXE1SRGBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 38. ヒストグラム\n",
        "単語の出現頻度のヒストグラムを描け．ただし，横軸は出現頻度を表し，1から単語の出現頻度の最大値までの線形目盛とする．縦軸はx軸で示される出現頻度となった単語の異なり数（種類数）である．\n",
        "\n",
        "\n",
        "> Pandasからヒストグラムの書き方わからなかったからForで全探索しようかと思ったけど調べたら普通に関数一発で描けるらしいです\n",
        "> - 【histogram入門】pandasとmatplotlibでヒストグラムを描いてみた [ https://qiita.com/MuAuan/items/849df1fffb0727f0dd09 ]\n",
        "\n",
        "を参考に `df.hist(bins=100<区間の個数>)`してplotするだけでした…"
      ],
      "metadata": {
        "id": "NvdNnDEsGBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = pd.DataFrame(df.value_counts(\"base\"))\n",
        "print(type(top_words))\n",
        "\n",
        "\"\"\"\n",
        "for w in top_words.index:\n",
        "  print(w)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QztgN5yzGBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words.hist(bins=100)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "kaCHpx6rYd-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 39. Zipfの法則\n",
        "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．\n",
        "\n",
        "- ジップの法則(Wikipedia)\n",
        "  - https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%83%E3%83%97%E3%81%AE%E6%B3%95%E5%89%87\n",
        "- Pandasのplotの全引数を解説 - Rosyuku\n",
        "  - https://own-search-and-study.xyz/2016/08/03/pandas%E3%81%AEplot%E3%81%AE%E5%85%A8%E5%BC%95%E6%95%B0%E3%82%92%E4%BD%BF%E3%81%84%E3%81%93%E3%81%AA%E3%81%99/\n",
        "\n",
        "\n",
        ">「出現頻度順位」と「出現頻度」の必要な値は揃っているのであとは対数グラフにプロットするだけ。\n",
        "  調べたら`df.plot()`の引数で`(loglog=True)`とすると両対数グラフになるらしい。\n",
        "  そして`df.plot?`でヘルプが表示される。便利だねぇ"
      ],
      "metadata": {
        "id": "3uTbIWTWGBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = pd.DataFrame(df.value_counts(\"base\"))\n",
        "#print(top_words)\n",
        "\n",
        "\n",
        "top_words.plot(loglog=True)"
      ],
      "metadata": {
        "id": "3k762OO3GBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第5章: 係り受け解析\n",
        "\n",
        "日本語Wikipediaの「人工知能」に関する記事からテキスト部分を抜き出したファイルがai.ja.zipに収録されている． この文章をCaboChaやKNP等のツールを利用して係り受け解析を行い，その結果をai.ja.txt.parsedというファイルに保存せよ．このファイルを読み込み，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "- CaboCha/南瓜: Yet Another Japanese Dependency Structure Analyzer\n",
        "  - 公式ページ https://taku910.github.io/cabocha/\n",
        "  - 奈良先端技術大学院の工藤拓氏による日本語係り受け解析器"
      ],
      "metadata": {
        "id": "triCPO2JGCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 事前準備\n",
        "\n",
        "例によってまたデータがいるので拾って読み込んでくるまでをやる。\n",
        "必要に応じて適宜ライブラリの追加などもココでやる。\n",
        "\n",
        "↓の実行ボタンを押したら事前準備が完了するようになってる。はず。\n"
      ],
      "metadata": {
        "id": "7mrrvSw6Sjku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### やってること\n",
        "- `ai.ja.zip`の保存と解凍(データの準備)\n",
        "- ライブラリの導入\n",
        "  - MeCab (+MeCab-Python3)\n",
        "  - CaboCha \n",
        "    - 配布はGoogle Driveから [ https://drive.google.com/drive/folders/0B4y35FiV1wh7cGRCUUJHVTNJRnM ]\n",
        "    - 2022-03-23時点で最新はcabocha-0.69なのでそれを使用します\n",
        "  - CRF++ (CaboChaの依存パッケージとして必要)\n",
        "    - 配布はGoogle Driveから [ https://drive.google.com/drive/folders/0B4y35FiV1wh7fngteFhHQUN2Y1B5eUJBNHZUemJYQV9VWlBUb3JlX0xBdWVZTWtSbVBneU0 ]\n",
        "    - 2022-03-23時点で最新はCRF++-0.58なのでそれを使用します。(最終更新が2015年なので多分ずっとコレだと思うけど…)\n",
        "- グラフ描画用\n",
        "  - matplotlib の読み込み\n",
        "  - japanize_matplotlib の読み込み (日本語でグラフ作成)\n",
        "  - `%matplotlib inline` の実行(グラフをインラインで描画するように)\n",
        "- 係り受け解析\n",
        "  - 係り受け解析結果を`ai.ja.txt.parsed`として出力\n",
        "\n",
        "\n",
        "#### [参考]\n",
        "- Google Colab で MeCab と CaboCha を使う最強の方法 by @tomowarkar\n",
        "  - Qiita https://qiita.com/tomowarkar/items/b6a89145c06956618542\n",
        "\n",
        "- Google Colab で MeCab と CaboCha を使う。 by tomowarkar\n",
        "  - github Blog https://tomowarkar.github.io/blog/posts/colab_mecab/\n",
        "\n",
        "> ……で頑張ってたけどbashだとどうしてもエラーが出てうまくいかない！！\n",
        "\n",
        "- Colabratory に Cabocha をインストールする by @iimuz\n",
        "  - Qiita https://qiita.com/iimuz/items/30a7e02772ffd3445f3b \n",
        "- ▲心くじけず言語処理100本ノック＝＝5章下準備＝＝ by tbtech\n",
        "  - はてなブログ https://ds-blog.tbtech.co.jp/entry/2020/06/08/%E2%96%B2%E5%BF%83%E3%81%8F%E3%81%98%E3%81%91%E3%81%9A%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%EF%BC%9D%EF%BC%9D5%E7%AB%A0%E4%B8%8B%E6%BA%96%E5%82%99%EF%BC%9D%EF%BC%9D\n",
        "\n",
        "> MeCabは謎の記法してるけど、CRF++とCaboChaをPythonベースで導入してて此方の記事のおかげで無事に使えました…感謝🙏\n",
        "  ていうか環境構築周りだけはマジで言語処理ノック側でもサポートしてくれって思う、そんなとこで躓くのは本来の課題じゃないだろうに\n",
        "\n",
        "- curlやwgetで公開済みGoogle Driveデータをダウンロードする by @namakemono\n",
        "  - Qiita https://qiita.com/namakemono/items/c963e75e0af3f7eed732\n",
        "\n",
        "> Google Driveで大きいサイズのときに出るウイルスチェック出来ませんみたいな警告のせいであのクソ長いwget文になるみたい。\n",
        "  そのままDLすると `cabocha.tar.bz2 is not a bzip2 file` と表示されて!tarや!bzipで解凍できない。困る。\n",
        "  ていうかなんで他の人は普通にwgetで通るの？？\n",
        "\n",
        "結果として\n",
        "- `cabocha.tar.bz2`のDLまでは心くじけず～の部分、\n",
        "- tarの解凍～make～Pythonライブラリのインストールをtomowarkar氏のものでやったらうまくいきました。"
      ],
      "metadata": {
        "id": "tcDLyeh9XOu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なデータの準備\n",
        "!wget https://nlp100.github.io/data/ai.ja.zip -O ai.ja.zip\n",
        "!unzip -o ai.ja.zip\n",
        "!pwd\n",
        "!ls\n",
        "!head -10 ai.ja.txt"
      ],
      "metadata": {
        "id": "w7A8mPqDUfPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649ec006-4bd9-4475-cb9a-22d646739f69"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-28 08:43:35--  https://nlp100.github.io/data/ai.ja.zip\n",
            "Resolving nlp100.github.io (nlp100.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to nlp100.github.io (nlp100.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17516 (17K) [application/zip]\n",
            "Saving to: ‘ai.ja.zip’\n",
            "\n",
            "\rai.ja.zip             0%[                    ]       0  --.-KB/s               \rai.ja.zip           100%[===================>]  17.11K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-03-28 08:43:35 (29.4 MB/s) - ‘ai.ja.zip’ saved [17516/17516]\n",
            "\n",
            "Archive:  ai.ja.zip\n",
            "  inflating: ai.ja.txt               \n",
            "  inflating: readme.ai.ja.md         \n",
            "/content\n",
            "ai.ja.txt\t  ai.ja.zip\tcabocha.tar.bz2  crfpp.tar.gz\t  sample_data\n",
            "ai.ja.txt.parsed  cabocha-0.69\tCRF++-0.58\t readme.ai.ja.md\n",
            "人工知能\n",
            "\n",
            "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\n",
            "\n",
            "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\n",
            "人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある。1956年にダートマス会議でジョン・マッカーシーにより命名された。現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。\n",
            "\n",
            "プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている。人工的な知能の実現へのアプローチとしては、「ファジィ理論」や「ニューラルネットワーク」などのようなアプローチも知られているが、従来の人工知能である (Good Old Fashioned AI) との差は記述の記号的明示性にある。その後「サポートベクターマシン」が注目を集めた。また、自らの経験を元に学習を行う強化学習という手法もある。「この宇宙において、知性とは最も強力な形質である（レイ・カーツワイル）」という言葉通り、知性を機械的に表現し実装するということは極めて重要な作業である。\n",
            "\n",
            "2006年のディープラーニング（深層学習）の登場と2010年代以降のビッグデータの登場により、一過性の流行を超えて社会に浸透して行った。2016年から2017年にかけて、ディープラーニングを導入したAIが完全情報ゲームである囲碁などのトップ棋士、さらに不完全情報ゲームであるポーカーの世界トップクラスのプレイヤーも破り、麻雀では「Microsoft Suphx (Super Phoenix)」がAIとして初めて十段に到達するなど、時代の最先端技術となった。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリの導入\n",
        "!apt install aptitude\n",
        "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.7\n"
      ],
      "metadata": {
        "id": "kYJESPXxYs1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68838f9-9dc9-44e6-94ec-0f25769bec82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 1s (3,059 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-jumandic\n",
            "  mecab-jumandic-utf8 mecab-utils\n",
            "0 upgraded, 8 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 29.0 MB of archives.\n",
            "After this operation, 277 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.0 MB in 1s (39.2 MB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 156669 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../3-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../4-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../5-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../6-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../7-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3==0.7\n",
            "  Downloading mecab-python3-0.7.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 257 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp37-cp37m-linux_x86_64.whl size=156600 sha256=e8c2fff4a5f94ef64c1c6fba9356d29203f99400aaa5aaaeb16be5faa4141ea1\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/46/95/3748ec2c4936cb69ee4d248a85e862064ea1e84819344c5292\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CRF++のインストール\n",
        "import os\n",
        "filename_crfpp = 'crfpp.tar.gz'\n",
        "!wget \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "%cd CRF++-0.58\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "%cd ../\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib' "
      ],
      "metadata": {
        "id": "M9meLf7q1__t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36e2899-302e-42e1-ae3d-81f93198ab36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-28 05:48:39--  https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.210.100, 173.194.210.113, 173.194.210.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.210.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/alt54c85u68bpho56s0nm24q9895372a/1648446450000/13553212398903315502/*/0B4y35FiV1wh7QVR6VXJ5dWExSTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-03-28 05:48:43--  https://doc-08-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/alt54c85u68bpho56s0nm24q9895372a/1648446450000/13553212398903315502/*/0B4y35FiV1wh7QVR6VXJ5dWExSTQ?e=download\n",
            "Resolving doc-08-74-docs.googleusercontent.com (doc-08-74-docs.googleusercontent.com)... 172.253.123.132, 2607:f8b0:400c:c16::84\n",
            "Connecting to doc-08-74-docs.googleusercontent.com (doc-08-74-docs.googleusercontent.com)|172.253.123.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 790570 (772K) [application/x-gzip]\n",
            "Saving to: ‘crfpp.tar.gz’\n",
            "\n",
            "crfpp.tar.gz        100%[===================>] 772.04K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-03-28 05:48:44 (39.5 MB/s) - ‘crfpp.tar.gz’ saved [790570/790570]\n",
            "\n",
            "CRF++-0.58/\n",
            "CRF++-0.58/INSTALL\n",
            "CRF++-0.58/python/\n",
            "CRF++-0.58/python/test.py\n",
            "CRF++-0.58/python/README\n",
            "CRF++-0.58/python/CRFPP.py\n",
            "CRF++-0.58/python/setup.py\n",
            "CRF++-0.58/python/CRFPP_wrap.cxx\n",
            "CRF++-0.58/Makefile.in\n",
            "CRF++-0.58/encoder.cpp\n",
            "CRF++-0.58/node.h\n",
            "CRF++-0.58/param.cpp\n",
            "CRF++-0.58/swig/\n",
            "CRF++-0.58/swig/CRFPP_wrap.c\n",
            "CRF++-0.58/swig/version.h\n",
            "CRF++-0.58/swig/version.h.in\n",
            "CRF++-0.58/swig/Makefile\n",
            "CRF++-0.58/swig/CRFPP.i\n",
            "CRF++-0.58/config.h.in\n",
            "CRF++-0.58/feature_cache.cpp\n",
            "CRF++-0.58/config.guess\n",
            "CRF++-0.58/scoped_ptr.h\n",
            "CRF++-0.58/node.cpp\n",
            "CRF++-0.58/README\n",
            "CRF++-0.58/timer.h\n",
            "CRF++-0.58/feature_index.h\n",
            "CRF++-0.58/config.sub\n",
            "CRF++-0.58/ltmain.sh\n",
            "CRF++-0.58/common.h\n",
            "CRF++-0.58/configure\n",
            "CRF++-0.58/crf_learn.cpp\n",
            "CRF++-0.58/darts.h\n",
            "CRF++-0.58/winmain.h\n",
            "CRF++-0.58/doc/\n",
            "CRF++-0.58/doc/html/\n",
            "CRF++-0.58/doc/html/search/\n",
            "CRF++-0.58/doc/html/search/nomatches.html\n",
            "CRF++-0.58/doc/html/search/search_r.png\n",
            "CRF++-0.58/doc/html/search/search_l.png\n",
            "CRF++-0.58/doc/html/search/mag_sel.png\n",
            "CRF++-0.58/doc/html/search/search.css\n",
            "CRF++-0.58/doc/html/search/search_m.png\n",
            "CRF++-0.58/doc/html/search/close.png\n",
            "CRF++-0.58/doc/html/search/search.js\n",
            "CRF++-0.58/doc/html/nav_f.png\n",
            "CRF++-0.58/doc/html/crfpp_8h_source.html\n",
            "CRF++-0.58/doc/html/jquery.js\n",
            "CRF++-0.58/doc/html/nav_h.png\n",
            "CRF++-0.58/doc/html/bc_s.png\n",
            "CRF++-0.58/doc/html/index.html\n",
            "CRF++-0.58/doc/html/closed.png\n",
            "CRF++-0.58/doc/html/tab_h.png\n",
            "CRF++-0.58/doc/html/tab_a.png\n",
            "CRF++-0.58/doc/html/tab_b.png\n",
            "CRF++-0.58/doc/html/installdox\n",
            "CRF++-0.58/doc/html/doxygen.css\n",
            "CRF++-0.58/doc/html/open.png\n",
            "CRF++-0.58/doc/html/tab_s.png\n",
            "CRF++-0.58/doc/html/files.html\n",
            "CRF++-0.58/doc/html/doxygen.png\n",
            "CRF++-0.58/doc/html/tabs.css\n",
            "CRF++-0.58/doc/latex/\n",
            "CRF++-0.58/doc/latex/refman.tex\n",
            "CRF++-0.58/doc/latex/doxygen.sty\n",
            "CRF++-0.58/doc/latex/Makefile\n",
            "CRF++-0.58/doc/index.html\n",
            "CRF++-0.58/doc/default.css\n",
            "CRF++-0.58/doc/doxygen/\n",
            "CRF++-0.58/doc/doxygen/tab_l.gif\n",
            "CRF++-0.58/doc/doxygen/namespacemembers.html\n",
            "CRF++-0.58/doc/doxygen/nav_f.png\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h_source.html\n",
            "CRF++-0.58/doc/doxygen/namespaces.html\n",
            "CRF++-0.58/doc/doxygen/nav_h.png\n",
            "CRF++-0.58/doc/doxygen/namespaceCRFPP.html\n",
            "CRF++-0.58/doc/doxygen/globals.html\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h-source.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Tagger-members.html\n",
            "CRF++-0.58/doc/doxygen/tab_b.gif\n",
            "CRF++-0.58/doc/doxygen/functions.html\n",
            "CRF++-0.58/doc/doxygen/tab_r.gif\n",
            "CRF++-0.58/doc/doxygen/bc_s.png\n",
            "CRF++-0.58/doc/doxygen/namespacemembers_func.html\n",
            "CRF++-0.58/doc/doxygen/index.html\n",
            "CRF++-0.58/doc/doxygen/closed.png\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Model.html\n",
            "CRF++-0.58/doc/doxygen/tab_h.png\n",
            "CRF++-0.58/doc/doxygen/functions_func.html\n",
            "CRF++-0.58/doc/doxygen/tab_a.png\n",
            "CRF++-0.58/doc/doxygen/globals_defs.html\n",
            "CRF++-0.58/doc/doxygen/classes.html\n",
            "CRF++-0.58/doc/doxygen/tab_b.png\n",
            "CRF++-0.58/doc/doxygen/globals_type.html\n",
            "CRF++-0.58/doc/doxygen/doxygen.css\n",
            "CRF++-0.58/doc/doxygen/open.png\n",
            "CRF++-0.58/doc/doxygen/tab_s.png\n",
            "CRF++-0.58/doc/doxygen/globals_func.html\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Model-members.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Tagger.html\n",
            "CRF++-0.58/doc/doxygen/files.html\n",
            "CRF++-0.58/doc/doxygen/doxygen.png\n",
            "CRF++-0.58/doc/doxygen/tabs.css\n",
            "CRF++-0.58/doc/doxygen/annotated.html\n",
            "CRF++-0.58/doc/crfpp.cfg\n",
            "CRF++-0.58/crf_test.cpp\n",
            "CRF++-0.58/mmap.h\n",
            "CRF++-0.58/Makefile.msvc.in\n",
            "CRF++-0.58/ChangeLog\n",
            "CRF++-0.58/feature_index.cpp\n",
            "CRF++-0.58/COPYING\n",
            "CRF++-0.58/libcrfpp.cpp\n",
            "CRF++-0.58/NEWS\n",
            "CRF++-0.58/mkinstalldirs\n",
            "CRF++-0.58/freelist.h\n",
            "CRF++-0.58/AUTHORS\n",
            "CRF++-0.58/merge-models.pl\n",
            "CRF++-0.58/java/\n",
            "CRF++-0.58/java/test.java\n",
            "CRF++-0.58/java/README\n",
            "CRF++-0.58/java/.am\n",
            "CRF++-0.58/java/Makefile\n",
            "CRF++-0.58/java/org/\n",
            "CRF++-0.58/java/org/chasen/\n",
            "CRF++-0.58/java/org/chasen/crfpp/\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_float.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPPJNI.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_p_char.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPP.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_int.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPPConstants.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/Model.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/Tagger.java\n",
            "CRF++-0.58/java/CRFPP_wrap.cxx\n",
            "CRF++-0.58/encoder.h\n",
            "CRF++-0.58/crfpp.h\n",
            "CRF++-0.58/perl/\n",
            "CRF++-0.58/perl/test.pl\n",
            "CRF++-0.58/perl/README\n",
            "CRF++-0.58/perl/Makefile.PL\n",
            "CRF++-0.58/perl/CRFPP_wrap.cxx\n",
            "CRF++-0.58/perl/CRFPP.pm\n",
            "CRF++-0.58/aclocal.m4\n",
            "CRF++-0.58/lbfgs.cpp\n",
            "CRF++-0.58/lbfgs.h\n",
            "CRF++-0.58/install-sh\n",
            "CRF++-0.58/tagger.cpp\n",
            "CRF++-0.58/param.h\n",
            "CRF++-0.58/missing\n",
            "CRF++-0.58/feature_cache.h\n",
            "CRF++-0.58/feature.cpp\n",
            "CRF++-0.58/depcomp\n",
            "CRF++-0.58/stream_wrapper.h\n",
            "CRF++-0.58/sdk/\n",
            "CRF++-0.58/sdk/example.cpp\n",
            "CRF++-0.58/ruby/\n",
            "CRF++-0.58/ruby/extconf.rb\n",
            "CRF++-0.58/ruby/CRFPP_wrap.cpp\n",
            "CRF++-0.58/ruby/README\n",
            "CRF++-0.58/ruby/test.rb\n",
            "CRF++-0.58/configure.in\n",
            "CRF++-0.58/path.cpp\n",
            "CRF++-0.58/path.h\n",
            "CRF++-0.58/tagger.h\n",
            "CRF++-0.58/thread.h\n",
            "CRF++-0.58/example/\n",
            "CRF++-0.58/example/JapaneseNE/\n",
            "CRF++-0.58/example/JapaneseNE/train.data\n",
            "CRF++-0.58/example/JapaneseNE/test.data\n",
            "CRF++-0.58/example/JapaneseNE/exec.sh\n",
            "CRF++-0.58/example/JapaneseNE/template\n",
            "CRF++-0.58/example/chunking/\n",
            "CRF++-0.58/example/chunking/train.data\n",
            "CRF++-0.58/example/chunking/test.data\n",
            "CRF++-0.58/example/chunking/exec.sh\n",
            "CRF++-0.58/example/chunking/template\n",
            "CRF++-0.58/example/seg/\n",
            "CRF++-0.58/example/seg/train.data\n",
            "CRF++-0.58/example/seg/test.data\n",
            "CRF++-0.58/example/seg/exec.sh\n",
            "CRF++-0.58/example/seg/template\n",
            "CRF++-0.58/example/basenp/\n",
            "CRF++-0.58/example/basenp/train.data\n",
            "CRF++-0.58/example/basenp/test.data\n",
            "CRF++-0.58/example/basenp/exec.sh\n",
            "CRF++-0.58/example/basenp/template\n",
            "CRF++-0.58/Makefile.am\n",
            "/content/CRF++-0.58\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... gcc3\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... gcc3\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking for library containing strerror... none required\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... no\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7077: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... no\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for ANSI C header files... (cached) no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking math.h usability... yes\n",
            "checking math.h presence... yes\n",
            "checking for math.h... yes\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for size_t... yes\n",
            "checking for pow in -lm... yes\n",
            "checking for exp in -lm... yes\n",
            "checking for log in -lm... yes\n",
            "checking for pthread_create in -lpthread... yes\n",
            "checking for pthread_join in -lpthread... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking whether make is GNU Make... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports stl <sstream> (required)... yes\n",
            "checking if g++ supports stl <stdexcept> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports dynamic_cast<> (required)... yes\n",
            "checking if g++ supports exception handler (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ supports _SC_NPROCESSORS_CONF (optional)... yes\n",
            "checking if g++ environment provides all required features... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating Makefile.msvc\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "make  all-am\n",
            "make[1]: Entering directory '/content/CRF++-0.58'\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o libcrfpp.lo libcrfpp.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c libcrfpp.cpp  -fPIC -DPIC -o .libs/libcrfpp.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c libcrfpp.cpp -o libcrfpp.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o lbfgs.lo lbfgs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c lbfgs.cpp  -fPIC -DPIC -o .libs/lbfgs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c lbfgs.cpp -o lbfgs.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o encoder.lo encoder.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c encoder.cpp  -fPIC -DPIC -o .libs/encoder.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c encoder.cpp -o encoder.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature.lo feature.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature.cpp  -fPIC -DPIC -o .libs/feature.o\n",
            "In file included from \u001b[01m\u001b[Ktagger.h:14:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfeature.cpp:12\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:34:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget CRFPP::{anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature.cpp -o feature.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature_cache.lo feature_cache.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_cache.cpp  -fPIC -DPIC -o .libs/feature_cache.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_cache.cpp -o feature_cache.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature_index.lo feature_index.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_index.cpp  -fPIC -DPIC -o .libs/feature_index.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_index.cpp -o feature_index.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o node.lo node.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c node.cpp  -fPIC -DPIC -o .libs/node.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c node.cpp -o node.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o path.lo path.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c path.cpp  -fPIC -DPIC -o .libs/path.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c path.cpp -o path.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o tagger.lo tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c tagger.cpp  -fPIC -DPIC -o .libs/tagger.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c tagger.cpp -o tagger.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o libcrfpp.la -rpath /usr/local/lib libcrfpp.lo lbfgs.lo param.lo encoder.lo feature.lo feature_cache.lo feature_index.lo node.lo path.lo tagger.lo  -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/libcrfpp.o .libs/lbfgs.o .libs/param.o .libs/encoder.o .libs/feature.o .libs/feature_cache.o .libs/feature_index.o .libs/node.o .libs/path.o .libs/tagger.o   -lpthread -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libcrfpp.so.0 -o .libs/libcrfpp.so.0.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcrfpp.so.0\" && ln -s \"libcrfpp.so.0.0.0\" \"libcrfpp.so.0\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcrfpp.so\" && ln -s \"libcrfpp.so.0.0.0\" \"libcrfpp.so\")\n",
            "libtool: link: ar cru .libs/libcrfpp.a  libcrfpp.o lbfgs.o param.o encoder.o feature.o feature_cache.o feature_index.o node.o path.o tagger.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libcrfpp.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libcrfpp.la\" && ln -s \"../libcrfpp.la\" \"libcrfpp.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o crf_learn.o crf_learn.cpp\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o crf_learn crf_learn.o libcrfpp.la -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++ -O3 -Wall -o .libs/crf_learn crf_learn.o  ./.libs/libcrfpp.so -lpthread -lm\n",
            "g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o crf_test.o crf_test.cpp\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o crf_test crf_test.o libcrfpp.la  -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++ -O3 -Wall -o .libs/crf_test crf_test.o  ./.libs/libcrfpp.so -lpthread -lm\n",
            "make[1]: Leaving directory '/content/CRF++-0.58'\n",
            "make[1]: Entering directory '/content/CRF++-0.58'\n",
            "test -z \"/usr/local/lib\" || /bin/mkdir -p \"/usr/local/lib\"\n",
            " /bin/bash ./libtool   --mode=install /usr/bin/install -c   libcrfpp.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.so.0.0.0 /usr/local/lib/libcrfpp.so.0.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcrfpp.so.0.0.0 libcrfpp.so.0 || { rm -f libcrfpp.so.0 && ln -s libcrfpp.so.0.0.0 libcrfpp.so.0; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcrfpp.so.0.0.0 libcrfpp.so || { rm -f libcrfpp.so && ln -s libcrfpp.so.0.0.0 libcrfpp.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.lai /usr/local/lib/libcrfpp.la\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.a /usr/local/lib/libcrfpp.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libcrfpp.a\n",
            "libtool: install: ranlib /usr/local/lib/libcrfpp.a\n",
            "libtool: finish: PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            "  /bin/bash ./libtool   --mode=install /usr/bin/install -c crf_learn crf_test '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/crf_learn /usr/local/bin/crf_learn\n",
            "libtool: install: /usr/bin/install -c .libs/crf_test /usr/local/bin/crf_test\n",
            "test -z \"/usr/local/include\" || /bin/mkdir -p \"/usr/local/include\"\n",
            " /usr/bin/install -c -m 644 crfpp.h '/usr/local/include'\n",
            "make[1]: Leaving directory '/content/CRF++-0.58'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CaboChaのインストール\n",
        "## > @yamaru氏の言語処理100本ノック第5章記事より引用\n",
        "FILE_ID = \"0B4y35FiV1wh7SDd1Q1dUQkZQaUU\"\n",
        "FILE_NAME = \"cabocha.tar.bz2\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt\n",
        "!tar -xvf cabocha.tar.bz2\n",
        "%cd cabocha-0.69\n",
        "\n",
        "!./configure -with-charset=utf-8 && make && make check && make install && ldconfig\n",
        "\n",
        "%cd ~/../content"
      ],
      "metadata": {
        "id": "Kx7FR3SzOZZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5569d269-d6b5-4c24-b199-a952f8bbf9f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-28 05:49:31--  https://docs.google.com/uc?export=download&confirm=t&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.31.102, 74.125.31.113, 74.125.31.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.31.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gguivp5pqjd3tmsh4fudv690mmfh2n4l/1648446525000/13553212398903315502/*/0B4y35FiV1wh7SDd1Q1dUQkZQaUU?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-03-28 05:49:31--  https://doc-04-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gguivp5pqjd3tmsh4fudv690mmfh2n4l/1648446525000/13553212398903315502/*/0B4y35FiV1wh7SDd1Q1dUQkZQaUU?e=download\n",
            "Resolving doc-04-74-docs.googleusercontent.com (doc-04-74-docs.googleusercontent.com)... 172.253.123.132, 2607:f8b0:400c:c16::84\n",
            "Connecting to doc-04-74-docs.googleusercontent.com (doc-04-74-docs.googleusercontent.com)|172.253.123.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84638995 (81M) [application/x-bzip2]\n",
            "Saving to: ‘cabocha.tar.bz2’\n",
            "\n",
            "cabocha.tar.bz2     100%[===================>]  80.72M   156MB/s    in 0.5s    \n",
            "\n",
            "2022-03-28 05:49:32 (156 MB/s) - ‘cabocha.tar.bz2’ saved [84638995/84638995]\n",
            "\n",
            "cabocha-0.69/\n",
            "cabocha-0.69/cabocha-config.in\n",
            "cabocha-0.69/compile\n",
            "cabocha-0.69/swig/\n",
            "cabocha-0.69/swig/version.h.in\n",
            "cabocha-0.69/swig/Makefile\n",
            "cabocha-0.69/swig/version.h\n",
            "cabocha-0.69/swig/CaboCha.i\n",
            "cabocha-0.69/missing\n",
            "cabocha-0.69/java/\n",
            "cabocha-0.69/java/test.java\n",
            "cabocha-0.69/java/Makefile\n",
            "cabocha-0.69/java/org/\n",
            "cabocha-0.69/java/org/chasen/\n",
            "cabocha-0.69/java/org/chasen/cabocha/\n",
            "cabocha-0.69/java/org/chasen/cabocha/FormatType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/OutputLayerType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Token.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboChaConstants.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/ParserType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/ParsingAlgorithm.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Chunk.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/InputLayerType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboCha.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboChaJNI.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/PossetType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Tree.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CharsetType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Parser.java\n",
            "cabocha-0.69/java/CaboCha_wrap.cxx\n",
            "cabocha-0.69/ltmain.sh\n",
            "cabocha-0.69/config.guess\n",
            "cabocha-0.69/man/\n",
            "cabocha-0.69/man/Makefile.in\n",
            "cabocha-0.69/man/cabocha.1\n",
            "cabocha-0.69/man/Makefile.am\n",
            "cabocha-0.69/BSD\n",
            "cabocha-0.69/python/\n",
            "cabocha-0.69/python/test.py\n",
            "cabocha-0.69/python/CaboCha.py\n",
            "cabocha-0.69/python/CaboCha_wrap.cxx\n",
            "cabocha-0.69/python/setup.py\n",
            "cabocha-0.69/AUTHORS\n",
            "cabocha-0.69/ruby/\n",
            "cabocha-0.69/ruby/CaboCha_wrap.cpp\n",
            "cabocha-0.69/ruby/extconf.rb\n",
            "cabocha-0.69/ruby/test.rb\n",
            "cabocha-0.69/Makefile.in\n",
            "cabocha-0.69/NEWS\n",
            "cabocha-0.69/install-sh\n",
            "cabocha-0.69/cabocha.iss.in\n",
            "cabocha-0.69/ChangeLog\n",
            "cabocha-0.69/configure\n",
            "cabocha-0.69/src/\n",
            "cabocha-0.69/src/string_buffer.cpp\n",
            "cabocha-0.69/src/tree_allocator.cpp\n",
            "cabocha-0.69/src/dep.h\n",
            "cabocha-0.69/src/dep_learner.cpp\n",
            "cabocha-0.69/src/tree_allocator.h\n",
            "cabocha-0.69/src/svm.h\n",
            "cabocha-0.69/src/svm.cpp\n",
            "cabocha-0.69/src/ucstable.h\n",
            "cabocha-0.69/src/utils.h\n",
            "cabocha-0.69/src/selector.cpp\n",
            "cabocha-0.69/src/chunk_learner.cpp\n",
            "cabocha-0.69/src/string_buffer.h\n",
            "cabocha-0.69/src/ucs.cpp\n",
            "cabocha-0.69/src/ne.cpp\n",
            "cabocha-0.69/src/eval.cpp\n",
            "cabocha-0.69/src/cabocha.cpp\n",
            "cabocha-0.69/src/Makefile.in\n",
            "cabocha-0.69/src/scoped_ptr.h\n",
            "cabocha-0.69/src/chunker.h\n",
            "cabocha-0.69/src/normalizer.rule\n",
            "cabocha-0.69/src/common.h\n",
            "cabocha-0.69/src/normalizer_rule.sh\n",
            "cabocha-0.69/src/darts.h\n",
            "cabocha-0.69/src/learner.cpp\n",
            "cabocha-0.69/src/cabocha.h\n",
            "cabocha-0.69/src/morph.h\n",
            "cabocha-0.69/src/svm_learn.cpp\n",
            "cabocha-0.69/src/Makefile.msvc.in\n",
            "cabocha-0.69/src/timer.h\n",
            "cabocha-0.69/src/chunker.cpp\n",
            "cabocha-0.69/src/utils.cpp\n",
            "cabocha-0.69/src/param.h\n",
            "cabocha-0.69/src/winmain.h\n",
            "cabocha-0.69/src/normalizer.h\n",
            "cabocha-0.69/src/param.cpp\n",
            "cabocha-0.69/src/parser.cpp\n",
            "cabocha-0.69/src/ne.h\n",
            "cabocha-0.69/src/normalizer_rule.h\n",
            "cabocha-0.69/src/svm_learn.h\n",
            "cabocha-0.69/src/ucs.h\n",
            "cabocha-0.69/src/cabocha-model-index.cpp\n",
            "cabocha-0.69/src/mmap.h\n",
            "cabocha-0.69/src/analyzer.h\n",
            "cabocha-0.69/src/make.bat\n",
            "cabocha-0.69/src/tree.cpp\n",
            "cabocha-0.69/src/char_category.h\n",
            "cabocha-0.69/src/Makefile.am\n",
            "cabocha-0.69/src/dep.cpp\n",
            "cabocha-0.69/src/morph.cpp\n",
            "cabocha-0.69/src/selector_pat.h\n",
            "cabocha-0.69/src/cabocha-system-eval.cpp\n",
            "cabocha-0.69/src/cabocha-learn.cpp\n",
            "cabocha-0.69/src/stream_wrapper.h\n",
            "cabocha-0.69/src/selector.h\n",
            "cabocha-0.69/src/libcabocha.cpp\n",
            "cabocha-0.69/src/normalizer.cpp\n",
            "cabocha-0.69/src/freelist.h\n",
            "cabocha-0.69/perl/\n",
            "cabocha-0.69/perl/test.pl\n",
            "cabocha-0.69/perl/Makefile.PL\n",
            "cabocha-0.69/perl/CaboCha_wrap.o\n",
            "cabocha-0.69/perl/CaboCha.bs\n",
            "cabocha-0.69/perl/blib/\n",
            "cabocha-0.69/perl/blib/bin/\n",
            "cabocha-0.69/perl/blib/bin/.exists\n",
            "cabocha-0.69/perl/blib/arch/\n",
            "cabocha-0.69/perl/blib/arch/.exists\n",
            "cabocha-0.69/perl/blib/arch/auto/\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/.exists\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/CaboCha.so\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/CaboCha.bs\n",
            "cabocha-0.69/perl/blib/lib/\n",
            "cabocha-0.69/perl/blib/lib/.exists\n",
            "cabocha-0.69/perl/blib/lib/auto/\n",
            "cabocha-0.69/perl/blib/lib/auto/CaboCha/\n",
            "cabocha-0.69/perl/blib/lib/auto/CaboCha/.exists\n",
            "cabocha-0.69/perl/blib/lib/CaboCha.pm\n",
            "cabocha-0.69/perl/blib/man1/\n",
            "cabocha-0.69/perl/blib/man1/.exists\n",
            "cabocha-0.69/perl/blib/script/\n",
            "cabocha-0.69/perl/blib/script/.exists\n",
            "cabocha-0.69/perl/blib/man3/\n",
            "cabocha-0.69/perl/blib/man3/.exists\n",
            "cabocha-0.69/perl/CaboCha_wrap.cxx\n",
            "cabocha-0.69/perl/pm_to_blib\n",
            "cabocha-0.69/perl/CaboCha.pm\n",
            "cabocha-0.69/perl/MYMETA.yml\n",
            "cabocha-0.69/config.rpath\n",
            "cabocha-0.69/TODO\n",
            "cabocha-0.69/configure.in\n",
            "cabocha-0.69/config.sub\n",
            "cabocha-0.69/LGPL\n",
            "cabocha-0.69/tools/\n",
            "cabocha-0.69/tools/kc2cabocha.pl\n",
            "cabocha-0.69/tools/irex2cabocha.pl\n",
            "cabocha-0.69/tools/chasen2mecab.pl\n",
            "cabocha-0.69/tools/kc2juman.pl\n",
            "cabocha-0.69/tools/KyotoCorpus.pm\n",
            "cabocha-0.69/tools/KNBC2KC.pl\n",
            "cabocha-0.69/cabocharc.in\n",
            "cabocha-0.69/INSTALL\n",
            "cabocha-0.69/aclocal.m4\n",
            "cabocha-0.69/README\n",
            "cabocha-0.69/config.h.in\n",
            "cabocha-0.69/COPYING\n",
            "cabocha-0.69/example/\n",
            "cabocha-0.69/example/example2.cpp\n",
            "cabocha-0.69/example/example.c\n",
            "cabocha-0.69/Makefile.am\n",
            "cabocha-0.69/model/\n",
            "cabocha-0.69/model/dep.ipa.txt\n",
            "cabocha-0.69/model/ne.juman.txt\n",
            "cabocha-0.69/model/dep.juman.txt\n",
            "cabocha-0.69/model/Makefile.in\n",
            "cabocha-0.69/model/dep.unidic.txt\n",
            "cabocha-0.69/model/chunk.ipa.txt\n",
            "cabocha-0.69/model/chunk.unidic.txt\n",
            "cabocha-0.69/model/ne.ipa.txt\n",
            "cabocha-0.69/model/ne.unidic.txt\n",
            "cabocha-0.69/model/chunk.juman.txt\n",
            "cabocha-0.69/model/Makefile.am\n",
            "cabocha-0.69/doc/\n",
            "cabocha-0.69/doc/README.txt\n",
            "cabocha-0.69/doc/doxygen/\n",
            "cabocha-0.69/doc/doxygen/classes.html\n",
            "cabocha-0.69/doc/doxygen/ftv2plastnode.png\n",
            "cabocha-0.69/doc/doxygen/nav_g.png\n",
            "cabocha-0.69/doc/doxygen/files.html\n",
            "cabocha-0.69/doc/doxygen/tab_b.gif\n",
            "cabocha-0.69/doc/doxygen/nav_h.png\n",
            "cabocha-0.69/doc/doxygen/namespaceCaboCha.html\n",
            "cabocha-0.69/doc/doxygen/functions_vars.html\n",
            "cabocha-0.69/doc/doxygen/tab_s.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_eval.html\n",
            "cabocha-0.69/doc/doxygen/ftv2pnode.png\n",
            "cabocha-0.69/doc/doxygen/cabocha_8h.html\n",
            "cabocha-0.69/doc/doxygen/open.png\n",
            "cabocha-0.69/doc/doxygen/globals_func.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__token__t.html\n",
            "cabocha-0.69/doc/doxygen/doxygen.css\n",
            "cabocha-0.69/doc/doxygen/ftv2node.png\n",
            "cabocha-0.69/doc/doxygen/functions_func.html\n",
            "cabocha-0.69/doc/doxygen/ftv2mnode.png\n",
            "cabocha-0.69/doc/doxygen/ftv2doc.png\n",
            "cabocha-0.69/doc/doxygen/globals_enum.html\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Tree.html\n",
            "cabocha-0.69/doc/doxygen/functions.html\n",
            "cabocha-0.69/doc/doxygen/ftv2folderopen.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers.html\n",
            "cabocha-0.69/doc/doxygen/globals.html\n",
            "cabocha-0.69/doc/doxygen/ftv2link.png\n",
            "cabocha-0.69/doc/doxygen/ftv2folderclosed.png\n",
            "cabocha-0.69/doc/doxygen/structcabocha__token__t-members.html\n",
            "cabocha-0.69/doc/doxygen/bdwn.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_func.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__chunk__t.html\n",
            "cabocha-0.69/doc/doxygen/bc_s.png\n",
            "cabocha-0.69/doc/doxygen/cabocha_8h_source.html\n",
            "cabocha-0.69/doc/doxygen/globals_eval.html\n",
            "cabocha-0.69/doc/doxygen/ftv2mo.png\n",
            "cabocha-0.69/doc/doxygen/doxygen.png\n",
            "cabocha-0.69/doc/doxygen/index.html\n",
            "cabocha-0.69/doc/doxygen/tab_b.png\n",
            "cabocha-0.69/doc/doxygen/closed.png\n",
            "cabocha-0.69/doc/doxygen/nav_f.png\n",
            "cabocha-0.69/doc/doxygen/ftv2lastnode.png\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Tree-members.html\n",
            "cabocha-0.69/doc/doxygen/tabs.css\n",
            "cabocha-0.69/doc/doxygen/ftv2vertline.png\n",
            "cabocha-0.69/doc/doxygen/ftv2cl.png\n",
            "cabocha-0.69/doc/doxygen/tab_h.png\n",
            "cabocha-0.69/doc/doxygen/globals_type.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__chunk__t-members.html\n",
            "cabocha-0.69/doc/doxygen/globals_defs.html\n",
            "cabocha-0.69/doc/doxygen/annotated.html\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_type.html\n",
            "cabocha-0.69/doc/doxygen/tab_l.gif\n",
            "cabocha-0.69/doc/doxygen/tab_a.png\n",
            "cabocha-0.69/doc/doxygen/sync_off.png\n",
            "cabocha-0.69/doc/doxygen/ftv2ns.png\n",
            "cabocha-0.69/doc/doxygen/tab_r.gif\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Parser-members.html\n",
            "cabocha-0.69/doc/doxygen/ftv2splitbar.png\n",
            "cabocha-0.69/doc/doxygen/ftv2mlastnode.png\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Parser.html\n",
            "cabocha-0.69/doc/doxygen/namespaces.html\n",
            "cabocha-0.69/doc/doxygen/sync_on.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_enum.html\n",
            "cabocha-0.69/doc/doxygen/dir_68267d1309a1af8e8297ef4c3efbcdba.html\n",
            "cabocha-0.69/doc/doxygen/dynsections.js\n",
            "cabocha-0.69/doc/doxygen/ftv2blank.png\n",
            "cabocha-0.69/doc/cabocha.cfg\n",
            "/content/cabocha-0.69\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports nested variables... yes\n",
            "checking whether to enable maintainer-specific portions of Makefiles... no\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking whether gcc understands -c and -o together... yes\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... none\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... none\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... dlltool\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7604: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for library containing strerror... none required\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for shared library run path origin... done\n",
            "checking for iconv... yes\n",
            "checking for working iconv... yes\n",
            "checking for iconv declaration... \n",
            "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
            "checking for ANSI C header files... (cached) yes\n",
            "checking for an ANSI C-conforming const... yes\n",
            "checking whether byte ordering is bigendian... no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking dirent.h usability... yes\n",
            "checking dirent.h presence... yes\n",
            "checking for dirent.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking io.h usability... no\n",
            "checking io.h presence... no\n",
            "checking for io.h... no\n",
            "checking windows.h usability... no\n",
            "checking windows.h presence... no\n",
            "checking for windows.h... no\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for off_t... yes\n",
            "checking for size_t... yes\n",
            "checking size of char... 1\n",
            "checking size of short... 2\n",
            "checking size of int... 4\n",
            "checking size of long... 8\n",
            "checking size of long long... 8\n",
            "checking size of size_t... 8\n",
            "checking for size_t... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for getenv... yes\n",
            "checking for opendir... yes\n",
            "checking for snprintf... yes\n",
            "checking for mecab-config... /usr/bin/mecab-config\n",
            "checking whether iconv supports EUC-JP-MS and CP932... checking for main in -lstdc++... yes\n",
            "checking for crfpp_new in -lcrfpp... yes\n",
            "checking for mecab_new in -lmecab... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <list> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <queue> (required)... yes\n",
            "checking if g++ supports stl <functional> (required)... yes\n",
            "checking if g++ supports stl <algorithm> (required)... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <strstream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports dynamic_cast<> (required)... yes\n",
            "checking if g++ supports reinterpret_cast<> (required)... yes\n",
            "checking if g++ supports exception handler (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ environment provides all required features... yes\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating src/Makefile\n",
            "config.status: creating src/Makefile.msvc\n",
            "config.status: creating model/Makefile\n",
            "config.status: creating man/Makefile\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating cabocha-config\n",
            "config.status: creating cabocharc\n",
            "config.status: creating cabocha.iss\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "config.status: executing default commands\n",
            "make  all-recursive\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "Making all in src\n",
            "make[2]: Entering directory '/content/cabocha-0.69/src'\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o chunk_learner.lo chunk_learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunk_learner.cpp  -fPIC -DPIC -o .libs/chunk_learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunk_learner.cpp -o chunk_learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o chunker.lo chunker.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunker.cpp  -fPIC -DPIC -o .libs/chunker.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunker.cpp -o chunker.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o dep.lo dep.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep.cpp  -fPIC -DPIC -o .libs/dep.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep.cpp -o dep.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o dep_learner.lo dep_learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep_learner.cpp  -fPIC -DPIC -o .libs/dep_learner.o\n",
            "In file included from \u001b[01m\u001b[Kdep_learner.cpp:17:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep_learner.cpp -o dep_learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o eval.lo eval.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c eval.cpp  -fPIC -DPIC -o .libs/eval.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c eval.cpp -o eval.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o learner.lo learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c learner.cpp  -fPIC -DPIC -o .libs/learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c learner.cpp -o learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o libcabocha.lo libcabocha.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c libcabocha.cpp  -fPIC -DPIC -o .libs/libcabocha.o\n",
            "In file included from \u001b[01m\u001b[Klibcabocha.cpp:18:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c libcabocha.cpp -o libcabocha.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o morph.lo morph.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c morph.cpp  -fPIC -DPIC -o .libs/morph.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c morph.cpp -o morph.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o ne.lo ne.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ne.cpp  -fPIC -DPIC -o .libs/ne.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ne.cpp -o ne.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o normalizer.lo normalizer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c normalizer.cpp  -fPIC -DPIC -o .libs/normalizer.o\n",
            "\u001b[01m\u001b[Knormalizer.cpp:\u001b[m\u001b[K In static member function '\u001b[01m\u001b[Kstatic void CaboCha::Normalizer::normalize(int, const char*, size_t, std::__cxx11::string*)\u001b[m\u001b[K':\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c normalizer.cpp -o normalizer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o parser.lo parser.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c parser.cpp  -fPIC -DPIC -o .libs/parser.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c parser.cpp -o parser.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o selector.lo selector.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c selector.cpp  -fPIC -DPIC -o .libs/selector.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c selector.cpp -o selector.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o tree_allocator.lo tree_allocator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree_allocator.cpp  -fPIC -DPIC -o .libs/tree_allocator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree_allocator.cpp -o tree_allocator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o string_buffer.lo string_buffer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c string_buffer.cpp  -fPIC -DPIC -o .libs/string_buffer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c string_buffer.cpp -o string_buffer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o svm.lo svm.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm.cpp  -fPIC -DPIC -o .libs/svm.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm.cpp -o svm.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o svm_learn.lo svm_learn.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm_learn.cpp  -fPIC -DPIC -o .libs/svm_learn.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm_learn.cpp -o svm_learn.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o tree.lo tree.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree.cpp  -fPIC -DPIC -o .libs/tree.o\n",
            "In file included from \u001b[01m\u001b[Kstring_buffer.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Ktree.cpp:14\u001b[m\u001b[K:\n",
            "utils.h: In instantiation of '\u001b[01m\u001b[Ksize_t CaboCha::tokenizeCSV(char*, Iterator, size_t) [with Iterator = char**; size_t = long unsigned int]\u001b[m\u001b[K':\n",
            "\u001b[01m\u001b[Ktree.cpp:480:57:\u001b[m\u001b[K   required from here\n",
            "\u001b[01m\u001b[Kutils.h:127:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable '\u001b[01m\u001b[Kinquote\u001b[m\u001b[K' set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     bool \u001b[01;35m\u001b[Kinquote\u001b[m\u001b[K = false;\n",
            "          \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree.cpp -o tree.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o ucs.lo ucs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ucs.cpp  -fPIC -DPIC -o .libs/ucs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ucs.cpp -o ucs.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o utils.lo utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c utils.cpp  -fPIC -DPIC -o .libs/utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c utils.cpp -o utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall -no-undefined -version-info 5:0:0  -o libcabocha.la -rpath /usr/local/lib chunk_learner.lo chunker.lo dep.lo dep_learner.lo eval.lo learner.lo libcabocha.lo morph.lo ne.lo normalizer.lo param.lo parser.lo selector.lo tree_allocator.lo string_buffer.lo svm.lo svm_learn.lo tree.lo ucs.lo utils.lo  -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/chunk_learner.o .libs/chunker.o .libs/dep.o .libs/dep_learner.o .libs/eval.o .libs/learner.o .libs/libcabocha.o .libs/morph.o .libs/ne.o .libs/normalizer.o .libs/param.o .libs/parser.o .libs/selector.o .libs/tree_allocator.o .libs/string_buffer.o .libs/svm.o .libs/svm_learn.o .libs/tree.o .libs/ucs.o .libs/utils.o   /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libcabocha.so.5 -o .libs/libcabocha.so.5.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcabocha.so.5\" && ln -s \"libcabocha.so.5.0.0\" \"libcabocha.so.5\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcabocha.so\" && ln -s \"libcabocha.so.5.0.0\" \"libcabocha.so\")\n",
            "libtool: link: ar cru .libs/libcabocha.a  chunk_learner.o chunker.o dep.o dep_learner.o eval.o learner.o libcabocha.o morph.o ne.o normalizer.o param.o parser.o selector.o tree_allocator.o string_buffer.o svm.o svm_learn.o tree.o ucs.o utils.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libcabocha.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libcabocha.la\" && ln -s \"../libcabocha.la\" \"libcabocha.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha.o cabocha.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha cabocha.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha cabocha.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-model-index.o cabocha-model-index.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-model-index cabocha-model-index.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-model-index cabocha-model-index.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-learn.o cabocha-learn.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-learn cabocha-learn.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-learn cabocha-learn.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-system-eval.o cabocha-system-eval.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-system-eval cabocha-system-eval.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-system-eval cabocha-system-eval.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making all in model\n",
            "make[2]: Entering directory '/content/cabocha-0.69/model'\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.ipa.txt chunk.ipa.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.juman.txt chunk.juman.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.unidic.txt chunk.unidic.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.ipa.txt dep.ipa.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2340864\n",
            "trie         size : 22100992\n",
            "feature size      : 122541\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 113308\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 1.98193e-07\n",
            "Done!\n",
            "18.88 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.juman.txt dep.juman.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2892800\n",
            "trie         size : 22047744\n",
            "feature size      : 149674\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 78200\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 2.27711e-07\n",
            "Done!\n",
            "18.83 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.unidic.txt dep.unidic.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2159616\n",
            "trie         size : 19891200\n",
            "feature size      : 121120\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 92412\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 2.27189e-07\n",
            "Done!\n",
            "17.08 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.ipa.txt ne.ipa.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.juman.txt ne.juman.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.unidic.txt ne.unidic.model\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making all in man\n",
            "make[2]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[2]: Entering directory '/content/cabocha-0.69'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "Making check in src\n",
            "make[1]: Entering directory '/content/cabocha-0.69/src'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making check in model\n",
            "make[1]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making check in man\n",
            "make[1]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "Making install in src\n",
            "make[1]: Entering directory '/content/cabocha-0.69/src'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/src'\n",
            " /bin/mkdir -p '/usr/local/lib'\n",
            " /bin/bash ../libtool   --mode=install /usr/bin/install -c   libcabocha.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.so.5.0.0 /usr/local/lib/libcabocha.so.5.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcabocha.so.5.0.0 libcabocha.so.5 || { rm -f libcabocha.so.5 && ln -s libcabocha.so.5.0.0 libcabocha.so.5; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcabocha.so.5.0.0 libcabocha.so || { rm -f libcabocha.so && ln -s libcabocha.so.5.0.0 libcabocha.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.lai /usr/local/lib/libcabocha.la\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.a /usr/local/lib/libcabocha.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libcabocha.a\n",
            "libtool: install: ranlib /usr/local/lib/libcabocha.a\n",
            "libtool: finish: PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            " /bin/mkdir -p '/usr/local/bin'\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c cabocha '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha /usr/local/bin/cabocha\n",
            " /bin/mkdir -p '/usr/local/libexec/cabocha'\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c cabocha-model-index cabocha-learn cabocha-system-eval '/usr/local/libexec/cabocha'\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-model-index /usr/local/libexec/cabocha/cabocha-model-index\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-learn /usr/local/libexec/cabocha/cabocha-learn\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-system-eval /usr/local/libexec/cabocha/cabocha-system-eval\n",
            " /bin/mkdir -p '/usr/local/include'\n",
            " /usr/bin/install -c -m 644 cabocha.h '/usr/local/include'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/src'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making install in model\n",
            "make[1]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/lib/cabocha/model'\n",
            " /usr/bin/install -c -m 644 chunk.ipa.model chunk.juman.model chunk.unidic.model dep.ipa.model dep.juman.model dep.unidic.model ne.ipa.model ne.juman.model ne.unidic.model '/usr/local/lib/cabocha/model'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/model'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making install in man\n",
            "make[1]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/share/man/man1'\n",
            " /usr/bin/install -c -m 644 cabocha.1 '/usr/local/share/man/man1'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "make[2]: Entering directory '/content/cabocha-0.69'\n",
            " /bin/mkdir -p '/usr/local/bin'\n",
            " /usr/bin/install -c cabocha-config '/usr/local/bin'\n",
            " /bin/mkdir -p '/usr/local/etc'\n",
            " /usr/bin/install -c -m 644 cabocharc '/usr/local/etc'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cabocha-pythonのインストール\n",
        "%cd ~/../content/cabocha-0.69/python\n",
        "!python setup.py build_ext\n",
        "!python setup.py install\n",
        "!pwd\n",
        "!ls\n",
        "%cd ~/../content"
      ],
      "metadata": {
        "id": "1ojGkcV_PyUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8f033b-29f1-4d0b-b259-8395b1d6c74c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cabocha-0.69/python\n",
            "running build_ext\n",
            "building '_CaboCha' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/include -I/usr/include/python3.7m -c CaboCha_wrap.cxx -o build/temp.linux-x86_64-3.7/CaboCha_wrap.o\n",
            "warning: no library file corresponding to '-L/usr/lib/x86_64inux-gnu' found (skipping)\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/CaboCha_wrap.o -L/usr/local/lib -lcabocha -lcrfpp -lmecab -lmecab -lstdc++ -o build/lib.linux-x86_64-3.7/_CaboCha.cpython-37m-x86_64-linux-gnu.so\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "copying CaboCha.py -> build/lib.linux-x86_64-3.7\n",
            "running build_ext\n",
            "running install_lib\n",
            "copying build/lib.linux-x86_64-3.7/CaboCha.py -> /usr/local/lib/python3.7/dist-packages\n",
            "copying build/lib.linux-x86_64-3.7/_CaboCha.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/CaboCha.py to CaboCha.cpython-37.pyc\n",
            "running install_egg_info\n",
            "Writing /usr/local/lib/python3.7/dist-packages/cabocha_python-0.69.egg-info\n",
            "/content/cabocha-0.69/python\n",
            "build  CaboCha.py  CaboCha_wrap.cxx  setup.py  test.py\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語をmatplotlibで表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# グラフ表示用\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "9-bEGSsEY0XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b1f9c9-e897-4f84-b61e-2164fdc09d38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from japanize-matplotlib) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->japanize-matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->japanize-matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120275 sha256=77e79fdce4d368ae7fc5b885149375327d371ff7fe7bd5294af296fcdfd9c621\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/97/6b/e9e0cde099cc40f972b8dd23367308f7705ae06cd6d4714658\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 導入確認\n",
        "!mecab -v\n",
        "!pip show mecab-python3\n",
        "!cabocha -v\n",
        "!pip show cabocha-python"
      ],
      "metadata": {
        "id": "Pm42fDyZcnt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d5bb99-6557-49f5-ab89-82cd6e472c94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mecab of 0.996\n",
            "\n",
            "Name: mecab-python3\n",
            "Version: 0.7\n",
            "Summary: python wrapper for mecab: Morphological Analysis engine\n",
            "Home-page: https://github.com/SamuraiT/mecab-python3\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "cabocha of 0.69\n",
            "Name: cabocha-python\n",
            "Version: 0.69\n",
            "Summary: UNKNOWN\n",
            "Home-page: UNKNOWN\n",
            "Author: UNKNOWN\n",
            "Author-email: UNKNOWN\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MeCab TEST CODE ##\n",
        "\n",
        "import MeCab\n",
        "tagger = MeCab.Tagger(\"-Odump\")\n",
        "print(tagger.parse(\"隣の客はよく柿食う客だ。\"))\n",
        "print(tagger.parse(\"今日は一日中家にいることにしている。\"))"
      ],
      "metadata": {
        "id": "chRTBEz96TvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1e7dc5-862d-46d0-b59a-ad360400cfa6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0\n",
            "1 隣 名詞,一般,*,*,*,*,隣,トナリ,トナリ 0 3 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 6894\n",
            "4 の 助詞,連体化,*,*,*,*,の,ノ,ノ 3 6 368 368 24 6 0 1 0.000000 0.000000 0.000000 7268\n",
            "9 客 名詞,一般,*,*,*,*,客,キャク,キャク 6 9 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 10360\n",
            "11 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 9 12 261 261 16 6 0 1 0.000000 0.000000 0.000000 10380\n",
            "22 よく 副詞,一般,*,*,*,*,よく,ヨク,ヨク 12 18 1281 1281 34 6 0 1 0.000000 0.000000 0.000000 12183\n",
            "33 柿 名詞,一般,*,*,*,*,柿,カキ,カキ 18 21 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 18979\n",
            "37 食う 動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ 21 27 817 817 31 2 0 1 0.000000 0.000000 0.000000 24452\n",
            "45 客 名詞,一般,*,*,*,*,客,キャク,キャク 27 30 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 28626\n",
            "48 だ 助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ 30 33 453 453 25 6 0 1 0.000000 0.000000 0.000000 30648\n",
            "49 。 記号,句点,*,*,*,*,。,。,。 33 36 8 8 7 3 0 1 0.000000 0.000000 0.000000 26508\n",
            "51 EOS BOS/EOS,*,*,*,*,*,*,*,* 36 36 0 0 0 0 3 1 0.000000 0.000000 0.000000 24972\n",
            "\n",
            "0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0\n",
            "7 今日 名詞,副詞可能,*,*,*,*,今日,キョウ,キョー 0 6 1314 1314 67 2 0 1 0.000000 0.000000 0.000000 3947\n",
            "20 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 6 9 261 261 16 6 0 1 0.000000 0.000000 0.000000 4822\n",
            "28 一日中 名詞,一般,*,*,*,*,一日中,イチニチジュウ,イチニチジュー 9 18 1285 1285 38 8 0 1 0.000000 0.000000 0.000000 11799\n",
            "64 家 名詞,接尾,一般,*,*,*,家,カ,カ 18 21 1298 1298 51 2 0 1 0.000000 0.000000 0.000000 14853\n",
            "67 に 助詞,格助詞,一般,*,*,*,に,ニ,ニ 21 24 151 151 13 6 0 1 0.000000 0.000000 0.000000 14880\n",
            "80 いる 動詞,自立,*,*,一段,基本形,いる,イル,イル 24 30 619 619 31 6 0 1 0.000000 0.000000 0.000000 19162\n",
            "91 こと 名詞,非自立,一般,*,*,*,こと,コト,コト 30 36 1310 1310 63 6 0 1 0.000000 0.000000 0.000000 20026\n",
            "101 に 助詞,格助詞,一般,*,*,*,に,ニ,ニ 36 39 151 151 13 6 0 1 0.000000 0.000000 0.000000 19975\n",
            "112 し 動詞,自立,*,*,サ変・スル,連用形,する,シ,シ 39 42 610 610 31 6 0 1 0.000000 0.000000 0.000000 23328\n",
            "113 て 助詞,接続助詞,*,*,*,*,て,テ,テ 42 45 307 307 18 6 0 1 0.000000 0.000000 0.000000 21835\n",
            "128 いる 動詞,非自立,*,*,一段,基本形,いる,イル,イル 45 51 919 919 33 6 0 1 0.000000 0.000000 0.000000 22384\n",
            "134 。 記号,句点,*,*,*,*,。,。,。 51 54 8 8 7 3 0 1 0.000000 0.000000 0.000000 19114\n",
            "136 EOS BOS/EOS,*,*,*,*,*,*,*,* 54 54 0 0 0 0 3 1 0.000000 0.000000 0.000000 17578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CaboCha TEST CODE ##\n",
        "\n",
        "import CaboCha\n",
        "cp = CaboCha.Parser()\n",
        "sentence = '猫は道路を渡る犬を見た。'\n",
        "print(cp.parseToString(sentence))"
      ],
      "metadata": {
        "id": "M1QuOi6K1c-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b8a502-7547-458b-a710-32f9490e545f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  猫は-------D\n",
            "  道路を-D   |\n",
            "      渡る-D |\n",
            "        犬を-D\n",
            "        見た。\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cabocha -f1 ai.ja.txt -o ai.ja.txt.parsed\n",
        "!head -25 ai.ja.txt.parsed"
      ],
      "metadata": {
        "id": "c49V6FTf4qVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c599d5df-76a5-4daa-87e8-ded7c22f2ae1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* 0 -1D 1/1 0.000000\n",
            "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
            "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
            "EOS\n",
            "EOS\n",
            "* 0 17D 1/1 0.388993\n",
            "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
            "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
            "* 1 17D 2/3 0.613549\n",
            "（\t記号,括弧開,*,*,*,*,（,（,（\n",
            "じん\t名詞,一般,*,*,*,*,じん,ジン,ジン\n",
            "こうち\t名詞,一般,*,*,*,*,こうち,コウチ,コーチ\n",
            "のう\t助詞,終助詞,*,*,*,*,のう,ノウ,ノー\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "* 2 3D 0/0 0.758984\n",
            "AI\t名詞,一般,*,*,*,*,*\n",
            "* 3 17D 1/5 0.517898\n",
            "〈\t記号,括弧開,*,*,*,*,〈,〈,〈\n",
            "エーアイ\t名詞,固有名詞,一般,*,*,*,*\n",
            "〉\t記号,括弧閉,*,*,*,*,〉,〉,〉\n",
            "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
            "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CaboChaの出力形式\n",
        "cabochaを使うと以下のような形式で出力される\n",
        "\n",
        "    * 0 1D 0/1 2.206035\n",
        "    隣\t名詞,一般,*,*,*,*,隣,トナリ,トナリ\n",
        "    の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
        "    * 1 5D 0/1 -0.593304\n",
        "    客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
        "    は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
        "    * 2 4D 0/0 0.538813\n",
        "    よく\t副詞,一般,*,*,*,*,よく,ヨク,ヨク\n",
        "    * 3 4D 0/0 1.985106\n",
        "    柿\t名詞,一般,*,*,*,*,柿,カキ,カキ\n",
        "    * 4 5D 0/0 -0.593304\n",
        "    食う\t動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ\n",
        "    * 5 -1D 0/1 0.000000\n",
        "    客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
        "    だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
        "    EOS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhmGwHRUGhid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"隣の客はよく柿食う客だ\" | cabocha -f1 "
      ],
      "metadata": {
        "id": "oHuTIxZEIfpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これは各文節毎に区切ってあり、以下のように読む\n",
        "\n",
        "- 1行目\n",
        "  1. *\n",
        "  2. 文節番号\n",
        "  3. 係り先の文節番号(係り先なし:-1)\n",
        "  4. 主辞の形態素番号/機能語の形態素番号\n",
        "  5. 係り関係のスコア(大きい方が係りやすい)\n",
        "- 2行目\n",
        "  1. 表層形 （Tab区切り）\n",
        "  2. 品詞\n",
        "  3. 品詞細分類1\n",
        "  4. 品詞細分類2\n",
        "  5. 品詞細分類3\n",
        "  6. 活用形\n",
        "  7. 活用型\n",
        "  8. 原形\n",
        "  9. 読み\n",
        "  10. 発音\n",
        "\n",
        "#### [参考]\n",
        "- CaboChaで始める係り受け解析 by @nezuq\n",
        "  - Qiita https://qiita.com/nezuq/items/f481f07fc0576b38e81d"
      ],
      "metadata": {
        "id": "jW31L0z5JH_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 40. 係り受け解析結果の読み込み（形態素）\n",
        "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．\n",
        "\n"
      ],
      "metadata": {
        "id": "efvr7lJWGCvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MsUvj8-OGCvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bc4e28-91ab-4b5a-e0b6-63046e45a5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'surface': '人工', 'base': '人工', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '知能', 'base': '知能', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '人工', 'base': '人工', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '知能', 'base': '知能', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '（', 'base': '（', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': 'じん', 'base': 'じん', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': 'こうち', 'base': 'こうち', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': 'のう', 'base': 'のう', 'pos': '助詞', 'pos1': '終助詞'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': 'AI', 'base': '*\\n', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '〈', 'base': '〈', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': 'エーアイ', 'base': '*\\n', 'pos': '名詞', 'pos1': '固有名詞'}\n",
            "{'surface': '〉', 'base': '〉', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': '）', 'base': '）', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': 'と', 'base': 'と', 'pos': '助詞', 'pos1': '格助詞'}\n",
            "{'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': '「', 'base': '「', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '『', 'base': '『', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '計算', 'base': '計算', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
            "{'surface': '（', 'base': '（', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '）', 'base': '）', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': '』', 'base': '』', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': 'という', 'base': 'という', 'pos': '助詞', 'pos1': '格助詞'}\n"
          ]
        }
      ],
      "source": [
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, morph):\n",
        "    self.surface = morph[\"surface\"]  # 表層形\n",
        "    self.base = morph[\"base\"]  # 基本形\n",
        "    self.pos = morph[\"pos\"]  # 品詞\n",
        "    self.pos1 = morph[\"pos1\"]  # 品詞細分類1\n",
        "\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(morphemes[0]) : word[0], # 表層形\n",
        "    str(morphemes[1]) : morpheme[6], # 基本形\n",
        "    str(morphemes[2]) : morpheme[0], # 品詞\n",
        "    str(morphemes[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "\n",
        "morph_list = []\n",
        "sentence = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        sentence.append(Morph(make_mapping(word)))\n",
        "\n",
        "\n",
        "for m in sentence[:25]:\n",
        "  print(vars(m))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
        "\n",
        "> 1行ずつ読んで、Morphを作って、EOSか文節情報が来たら直前の文節を格納…ン？？？？どうやんの？？？？\n",
        "  → 1行ずつ読むのではなくて、`EOS\\n` で分割してブロックごとに処理すると以下のメリットが有る\n",
        "  - EOSは出てこない\n",
        "  - 文節ごとに処理するので行読みしても文節頭の係り受け情報が保持できる\n",
        "\n",
        "> 3文節目「」"
      ],
      "metadata": {
        "id": "NVOr6AErGCvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 形態素\n",
        "  def __init__(self, morphs, dst):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = dst  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sentence = []  # 文毎に纏めるList \n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  #print(blocks)\n",
        "  \n",
        "  for block in blocks[0:5]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    #print(f\"block:{block}\")\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        kakari = line.split(\" \")\n",
        "        if len(morphemes)>0:\n",
        "          sentence.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\")))\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        #print(word)\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    #print(f\"morphemes:{morphemes}\")\n",
        "    if len(morphemes)>0:\n",
        "      sentence.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\")))\n",
        "    morphemes = []\n",
        "    kakari = []\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    print(type(sentence))\n",
        "    for i,c in enumerate(sentence):\n",
        "      if c.dst == -1:\n",
        "        continue\n",
        "      sentence[int(c.dst)].srcs.append(int(c.dst))\n",
        "      print(f\"{i} = 形態素: {[m.surface for m in c.morphs]}, 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for m in sentence[:25]:\n",
        "  print(vars(m))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4G8JMAmzGCvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "66db5041-44fe-4151-85f7-7451b405937d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "0 = 形態素: ['人工', '知能'], 係り先: -1, 係り元: [-1]\n",
            "<class 'list'>\n",
            "0 = 形態素: ['人工', '知能'], 係り先: -1, 係り元: [-1, -1]\n",
            "<class 'list'>\n",
            "0 = 形態素: ['人工', '知能'], 係り先: -1, 係り元: [-1, -1]\n",
            "1 = 形態素: ['人工', '知能'], 係り先: 17, 係り元: []\n",
            "2 = 形態素: ['（', 'じん', 'こうち', 'のう', '、', '、'], 係り先: 3, 係り元: []\n",
            "3 = 形態素: ['AI'], 係り先: 17, 係り元: [3]\n",
            "4 = 形態素: ['〈', 'エーアイ', '〉', '）', 'と', 'は', '、'], 係り先: 5, 係り元: []\n",
            "5 = 形態素: ['「', '『', '計算'], 係り先: 9, 係り元: [5]\n",
            "6 = 形態素: ['（', '）', '』', 'という'], 係り先: 9, 係り元: []\n",
            "7 = 形態素: ['概念', 'と'], 係り先: 8, 係り元: []\n",
            "8 = 形態素: ['『', 'コンピュータ'], 係り先: 9, 係り元: [8]\n",
            "9 = 形態素: ['（', '）', '』', 'という'], 係り先: 10, 係り元: [9, 9, 9]\n",
            "10 = 形態素: ['道具', 'を'], 係り先: 12, 係り元: [10]\n",
            "11 = 形態素: ['用い', 'て'], 係り先: 12, 係り元: []\n",
            "12 = 形態素: ['『', '知能', '』', 'を'], 係り先: 13, 係り元: [12, 12]\n",
            "13 = 形態素: ['研究', 'する'], 係り先: 14, 係り元: [13]\n",
            "14 = 形態素: ['計算', '機', '科学'], 係り先: 15, 係り元: [14]\n",
            "15 = 形態素: ['（', '）', 'の'], 係り先: 16, 係り元: [15]\n",
            "16 = 形態素: ['一', '分野', '」', 'を'], 係り先: 17, 係り元: [16]\n",
            "17 = 形態素: ['指す'], 係り先: 34, 係り元: [17, 17, 17]\n",
            "18 = 形態素: ['語', '。'], 係り先: 20, 係り元: []\n",
            "19 = 形態素: ['「', '言語', 'の'], 係り先: 20, 係り元: []\n",
            "20 = 形態素: ['理解', 'や'], 係り先: 21, 係り元: [20, 20]\n",
            "21 = 形態素: ['推論', '、'], 係り先: 22, 係り元: [21]\n",
            "22 = 形態素: ['問題', '解決', 'など', 'の'], 係り先: 24, 係り元: [22]\n",
            "23 = 形態素: ['知的', '行動', 'を'], 係り先: 24, 係り元: []\n",
            "24 = 形態素: ['人間', 'に'], 係り先: 26, 係り元: [24, 24]\n",
            "25 = 形態素: ['代わっ', 'て'], 係り先: 26, 係り元: []\n",
            "26 = 形態素: ['コンピューター', 'に'], 係り先: 27, 係り元: [26, 26]\n",
            "27 = 形態素: ['行わ', 'せる'], 係り先: 34, 係り元: [27]\n",
            "28 = 形態素: ['技術', '」', '、', 'または', '、'], 係り先: 29, 係り元: []\n",
            "29 = 形態素: ['「', '計算', '機'], 係り先: 31, 係り元: [29]\n",
            "30 = 形態素: ['（', 'コンピュータ', '）', 'による'], 係り先: 31, 係り元: []\n",
            "31 = 形態素: ['知的', 'な'], 係り先: 33, 係り元: [31, 31]\n",
            "32 = 形態素: ['情報処理', 'システム', 'の'], 係り先: 33, 係り元: []\n",
            "33 = 形態素: ['設計', 'や'], 係り先: 34, 係り元: [33, 33]\n",
            "34 = 形態素: ['実現', 'に関する'], 係り先: 35, 係り元: [34, 34, 34]\n",
            "35 = 形態素: ['研究', '分野', '」', 'と', 'も'], 係り先: -1, 係り元: [35]\n",
            "36 = 形態素: ['さ', 'れる', '。'], 係り先: -1, 係り元: [-1, -1, -1]\n",
            "<class 'list'>\n",
            "0 = 形態素: ['人工', '知能'], 係り先: -1, 係り元: [-1, -1]\n",
            "1 = 形態素: ['人工', '知能'], 係り先: 17, 係り元: []\n",
            "2 = 形態素: ['（', 'じん', 'こうち', 'のう', '、', '、'], 係り先: 3, 係り元: []\n",
            "3 = 形態素: ['AI'], 係り先: 17, 係り元: [3, 3]\n",
            "4 = 形態素: ['〈', 'エーアイ', '〉', '）', 'と', 'は', '、'], 係り先: 5, 係り元: []\n",
            "5 = 形態素: ['「', '『', '計算'], 係り先: 9, 係り元: [5, 5]\n",
            "6 = 形態素: ['（', '）', '』', 'という'], 係り先: 9, 係り元: []\n",
            "7 = 形態素: ['概念', 'と'], 係り先: 8, 係り元: []\n",
            "8 = 形態素: ['『', 'コンピュータ'], 係り先: 9, 係り元: [8, 8]\n",
            "9 = 形態素: ['（', '）', '』', 'という'], 係り先: 10, 係り元: [9, 9, 9, 9, 9, 9]\n",
            "10 = 形態素: ['道具', 'を'], 係り先: 12, 係り元: [10, 10]\n",
            "11 = 形態素: ['用い', 'て'], 係り先: 12, 係り元: []\n",
            "12 = 形態素: ['『', '知能', '』', 'を'], 係り先: 13, 係り元: [12, 12, 12, 12]\n",
            "13 = 形態素: ['研究', 'する'], 係り先: 14, 係り元: [13, 13]\n",
            "14 = 形態素: ['計算', '機', '科学'], 係り先: 15, 係り元: [14, 14]\n",
            "15 = 形態素: ['（', '）', 'の'], 係り先: 16, 係り元: [15, 15]\n",
            "16 = 形態素: ['一', '分野', '」', 'を'], 係り先: 17, 係り元: [16, 16]\n",
            "17 = 形態素: ['指す'], 係り先: 34, 係り元: [17, 17, 17, 17, 17, 17]\n",
            "18 = 形態素: ['語', '。'], 係り先: 20, 係り元: []\n",
            "19 = 形態素: ['「', '言語', 'の'], 係り先: 20, 係り元: []\n",
            "20 = 形態素: ['理解', 'や'], 係り先: 21, 係り元: [20, 20, 20, 20]\n",
            "21 = 形態素: ['推論', '、'], 係り先: 22, 係り元: [21, 21]\n",
            "22 = 形態素: ['問題', '解決', 'など', 'の'], 係り先: 24, 係り元: [22, 22]\n",
            "23 = 形態素: ['知的', '行動', 'を'], 係り先: 24, 係り元: []\n",
            "24 = 形態素: ['人間', 'に'], 係り先: 26, 係り元: [24, 24, 24, 24]\n",
            "25 = 形態素: ['代わっ', 'て'], 係り先: 26, 係り元: []\n",
            "26 = 形態素: ['コンピューター', 'に'], 係り先: 27, 係り元: [26, 26, 26, 26]\n",
            "27 = 形態素: ['行わ', 'せる'], 係り先: 34, 係り元: [27, 27]\n",
            "28 = 形態素: ['技術', '」', '、', 'または', '、'], 係り先: 29, 係り元: []\n",
            "29 = 形態素: ['「', '計算', '機'], 係り先: 31, 係り元: [29, 29]\n",
            "30 = 形態素: ['（', 'コンピュータ', '）', 'による'], 係り先: 31, 係り元: []\n",
            "31 = 形態素: ['知的', 'な'], 係り先: 33, 係り元: [31, 31, 31, 31]\n",
            "32 = 形態素: ['情報処理', 'システム', 'の'], 係り先: 33, 係り元: []\n",
            "33 = 形態素: ['設計', 'や'], 係り先: 34, 係り元: [33, 33, 33, 33]\n",
            "34 = 形態素: ['実現', 'に関する'], 係り先: 35, 係り元: [34, 34, 34, 34, 34, 34]\n",
            "35 = 形態素: ['研究', '分野', '」', 'と', 'も'], 係り先: -1, 係り元: [35, 35]\n",
            "36 = 形態素: ['さ', 'れる', '。'], 係り先: -1, 係り元: [-1, -1, -1, -1, -1, -1]\n",
            "<class 'list'>\n",
            "0 = 形態素: ['人工', '知能'], 係り先: -1, 係り元: [-1, -1]\n",
            "1 = 形態素: ['人工', '知能'], 係り先: 17, 係り元: []\n",
            "2 = 形態素: ['（', 'じん', 'こうち', 'のう', '、', '、'], 係り先: 3, 係り元: []\n",
            "3 = 形態素: ['AI'], 係り先: 17, 係り元: [3, 3, 3]\n",
            "4 = 形態素: ['〈', 'エーアイ', '〉', '）', 'と', 'は', '、'], 係り先: 5, 係り元: []\n",
            "5 = 形態素: ['「', '『', '計算'], 係り先: 9, 係り元: [5, 5, 5]\n",
            "6 = 形態素: ['（', '）', '』', 'という'], 係り先: 9, 係り元: []\n",
            "7 = 形態素: ['概念', 'と'], 係り先: 8, 係り元: []\n",
            "8 = 形態素: ['『', 'コンピュータ'], 係り先: 9, 係り元: [8, 8, 8]\n",
            "9 = 形態素: ['（', '）', '』', 'という'], 係り先: 10, 係り元: [9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
            "10 = 形態素: ['道具', 'を'], 係り先: 12, 係り元: [10, 10, 10]\n",
            "11 = 形態素: ['用い', 'て'], 係り先: 12, 係り元: []\n",
            "12 = 形態素: ['『', '知能', '』', 'を'], 係り先: 13, 係り元: [12, 12, 12, 12, 12, 12]\n",
            "13 = 形態素: ['研究', 'する'], 係り先: 14, 係り元: [13, 13, 13]\n",
            "14 = 形態素: ['計算', '機', '科学'], 係り先: 15, 係り元: [14, 14, 14]\n",
            "15 = 形態素: ['（', '）', 'の'], 係り先: 16, 係り元: [15, 15, 15]\n",
            "16 = 形態素: ['一', '分野', '」', 'を'], 係り先: 17, 係り元: [16, 16, 16]\n",
            "17 = 形態素: ['指す'], 係り先: 34, 係り元: [17, 17, 17, 17, 17, 17, 17, 17, 17]\n",
            "18 = 形態素: ['語', '。'], 係り先: 20, 係り元: []\n",
            "19 = 形態素: ['「', '言語', 'の'], 係り先: 20, 係り元: []\n",
            "20 = 形態素: ['理解', 'や'], 係り先: 21, 係り元: [20, 20, 20, 20, 20, 20]\n",
            "21 = 形態素: ['推論', '、'], 係り先: 22, 係り元: [21, 21, 21]\n",
            "22 = 形態素: ['問題', '解決', 'など', 'の'], 係り先: 24, 係り元: [22, 22, 22]\n",
            "23 = 形態素: ['知的', '行動', 'を'], 係り先: 24, 係り元: []\n",
            "24 = 形態素: ['人間', 'に'], 係り先: 26, 係り元: [24, 24, 24, 24, 24, 24]\n",
            "25 = 形態素: ['代わっ', 'て'], 係り先: 26, 係り元: []\n",
            "26 = 形態素: ['コンピューター', 'に'], 係り先: 27, 係り元: [26, 26, 26, 26, 26, 26]\n",
            "27 = 形態素: ['行わ', 'せる'], 係り先: 34, 係り元: [27, 27, 27]\n",
            "28 = 形態素: ['技術', '」', '、', 'または', '、'], 係り先: 29, 係り元: []\n",
            "29 = 形態素: ['「', '計算', '機'], 係り先: 31, 係り元: [29, 29, 29]\n",
            "30 = 形態素: ['（', 'コンピュータ', '）', 'による'], 係り先: 31, 係り元: []\n",
            "31 = 形態素: ['知的', 'な'], 係り先: 33, 係り元: [31, 31, 31, 31, 31, 31]\n",
            "32 = 形態素: ['情報処理', 'システム', 'の'], 係り先: 33, 係り元: []\n",
            "33 = 形態素: ['設計', 'や'], 係り先: 34, 係り元: [33, 33, 33, 33, 33, 33]\n",
            "34 = 形態素: ['実現', 'に関する'], 係り先: 35, 係り元: [34, 34, 34, 34, 34, 34, 34, 34, 34]\n",
            "35 = 形態素: ['研究', '分野', '」', 'と', 'も'], 係り先: -1, 係り元: [35, 35, 35]\n",
            "36 = 形態素: ['さ', 'れる', '。'], 係り先: -1, 係り元: [-1, -1, -1, -1, -1, -1]\n",
            "37 = 形態素: ['『', '日本', '大', '百科全書', '(', 'ニッポニカ', ')』', 'の'], 係り先: 5, 係り元: []\n",
            "38 = 形態素: ['解説', 'で', '、'], 係り先: 3, 係り元: []\n",
            "39 = 形態素: ['情報', '工学', '者', '・', '通信', '工学', '者', 'の'], 係り先: 5, 係り元: []\n",
            "40 = 形態素: ['佐藤', '理', '史', 'は'], 係り先: 5, 係り元: []\n",
            "41 = 形態素: ['次', 'の', 'よう', 'に'], 係り先: -1, 係り元: []\n",
            "42 = 形態素: ['述べ', 'て', 'いる', '。'], 係り先: -1, 係り元: [-1, -1, -1, -1, -1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor m in sentence[:25]:\\n  print(vars(m))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 42. 係り元と係り先の文節の表示\n",
        "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
      ],
      "metadata": {
        "id": "EkCDSwTMGCvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G_OM9atUGCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
      ],
      "metadata": {
        "id": "tki4hy54GCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9o102UtqGCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44. 係り受け木の可視化\n",
        "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．"
      ],
      "metadata": {
        "id": "tZR-DWmxGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3NJui4rdGCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 45. 動詞の格パターンの抽出\n",
        "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
        "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
        "- 述語に係る助詞を格とする\n",
        "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "    作り出す\tで は を\n",
        "\n",
        "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
        "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
      ],
      "metadata": {
        "id": "KsTUgQ7qGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "grwlaBb9GCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 46. 動詞の格フレーム情報の抽出\n",
        "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
        "\n",
        "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
        "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "    作り出す\tで は を\t会議で ジョンマッカーシーは 用語を"
      ],
      "metadata": {
        "id": "ZY2_LUvmGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e7oStrybGCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 47. 機能動詞構文のマイニング\n",
        "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
        "\n",
        "「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
        "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
        "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
        "\n",
        "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
        "\n",
        "    学習を行う\tに を\t元に 経験を"
      ],
      "metadata": {
        "id": "0dFw4MJIGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EgJZi8auGCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 48. 名詞から根へのパスの抽出\n",
        "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
        "- 各文節は（表層形の）形態素列で表現する\n",
        "- パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    ジョンマッカーシーは -> 作り出した\n",
        "    AIに関する -> 最初の -> 会議で -> 作り出した\n",
        "    最初の -> 会議で -> 作り出した\n",
        "    会議で -> 作り出した\n",
        "    人工知能という -> 用語を -> 作り出した\n",
        "    用語を -> 作り出した\n",
        "\n",
        "KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    ジョンマッカーシーは -> 作り出した\n",
        "    ＡＩに -> 関する -> 会議で -> 作り出した\n",
        "    会議で -> 作り出した\n",
        "    人工知能と -> いう -> 用語を -> 作り出した\n",
        "    用語を -> 作り出した"
      ],
      "metadata": {
        "id": "ASM7F7ZMGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XfoDNc3IGCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 49. 名詞間の係り受けパスの抽出\n",
        "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj`(i<j)`のとき，係り受けパスは以下の仕様を満たすものとする．\n",
        "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
        "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
        "\n",
        "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
        "\n",
        "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
        "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
        "    Xは | Yの -> 会議で | 作り出した\n",
        "    Xは | Yで | 作り出した\n",
        "    Xは | Yという -> 用語を | 作り出した\n",
        "    Xは | Yを | 作り出した\n",
        "    Xに関する -> Yの\n",
        "    Xに関する -> 最初の -> Yで\n",
        "    Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
        "    Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
        "    Xの -> Yで\n",
        "    Xの -> 会議で | Yという -> 用語を | 作り出した\n",
        "    Xの -> 会議で | Yを | 作り出した\n",
        "    Xで | Yという -> 用語を | 作り出した\n",
        "    Xで | Yを | 作り出した\n",
        "    Xという -> Yを\n",
        "\n",
        "KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    Xは | Yに -> 関する -> 会議で | 作り出した。\n",
        "    Xは | Yで | 作り出した。\n",
        "    Xは | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xは | Yを | 作り出した。\n",
        "    Xに -> 関する -> Yで\n",
        "    Xに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xに -> 関する -> 会議で | Yを | 作り出した。\n",
        "    Xで | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xで | Yを | 作り出した。\n",
        "    Xと -> いう -> Yを"
      ],
      "metadata": {
        "id": "Ely3RRkBGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gsNXyAThGCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第6章: 機械学習"
      ],
      "metadata": {
        "id": "cNWf4DGTGUnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "zH78h42hGUnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxeItMqtGUn0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "jGlc3MXtGUn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VFTzIkC3GUn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "D6bg-elpGUn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d2jCPkqqGUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "DyUYiplYGUn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "py3mHPPMGUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "WBGDyhppGUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RohKtHJUGUn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "3W8gVsOqGUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HaEtVYZJGUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "hgOJHSCXGUn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cdan2_sdGUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "0QXqzLhTGUn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I81B5QhTGUn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Kmc1CZljGUn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FHuNDnDRGUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "JDxWM0EnGUn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QsC7jvbgGUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第7章: 単語ベクトル"
      ],
      "metadata": {
        "id": "fFWgl4_HGUn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "OVbyTLWpGUn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZ2_h-nGUn7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "5XMltajHGUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bgObeRJZGUn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "6f9guhGvGUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8rmuH7_YGUn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Qti4fMtgGUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2yn69ypIGUn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "0ZM9MJ8XGUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JooczH7ZGUn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "78YJXkQCGUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ar4cHATNGUn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "rEuas98LGUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BjyVGREXGUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "emZ2W5TrGUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6NzushwwGUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Yvs0rDyzGUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZYWCcB3QGUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "WfwSmXGcGUoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3ZqFnAP9GUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第8章: ニューラルネット"
      ],
      "metadata": {
        "id": "NMOIVGPMGUoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "nUHYvHeRGUoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgvBFenNGUoB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "wguxJnK7GUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PKcYF6hqGUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "jCJCzpXnGUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XdTFCjbkGUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "pdkstkG4GUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y_KUWM71GUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "IHANJMStGUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9mLCYkR8GUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "l892pPbAGUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LcBewpIiGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "udN-yYPuGUoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vPCNLDTzGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "aY6Qb7Z-GUoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0-IBSIsfGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "4sdE0PmCGUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3VQMfEdOGUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "ujNB-Nt6GUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8YTVV0N0GUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第9章: RNNとCNN"
      ],
      "metadata": {
        "id": "A_fskrYuGDe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "uVFVICBTGDe3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCe-W2miGDe4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "j_ek77AlGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RS4Aon27GDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "BpfRkFReGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d02RDDCSGDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "x9Zr__PdGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QRGzx3P3GDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "HTbmQJbsGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MwLJUY7rGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "I26_r7nhGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wu-ZxkLBGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "RTrizMr7GDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JFMLkF8fGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "v49GPvdkGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YU9CoWcTGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "RvlqPNHfGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CH0Jxw1sGDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "S_n5DroHGDe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9O2SdvbNGDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第10章: 機械翻訳"
      ],
      "metadata": {
        "id": "HxRHhP-yGd-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "EPolx2ELGd-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4X5oTb2Gd-T"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "JpIxmETQGd-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oz0QHcCAGd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "GsEmSl2yGd-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_iNZMWz3Gd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "M9x9C5g-Gd-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OI3QSandGd-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "TJY2NdzDGd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TzFDDn8WGd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "na9B1uP3Gd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gRpFpJbbGd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "VgUT4sUGGd-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wbx-9ATdGd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "f_NdGNIrGd-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V4XQOeVtGd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "E4A_wEhxGd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHj95k5AGd-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "c-wQyjZOGd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4wFHPnPmGd-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}