{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "言語処理100本ノック2020[nlp2020].ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ExnBfpTnY-hJ",
        "qJbCeI_9Etpb",
        "N8LrAkjcDWMW",
        "TiHqMcclED6B",
        "zZ06yBtKEIJx",
        "8jkcGrsWEL_n",
        "SkHpEeLmEObx",
        "Imyqw0ffEQIv",
        "nIOikAjiESFx",
        "WhYuAmdaEUq9",
        "wLGsCLQpEWdC",
        "XYef5GAZEX9r",
        "oeaShrnkFXUu",
        "oreVQ8KdFXU0",
        "Ju_6iik5FXU0",
        "IH4RiaQbFXU1",
        "3tC88THbFXU1",
        "_nBP6zSgFXU1",
        "gH__H51uFXU1",
        "B-n_nvttFXU2",
        "MxfCADGMFXU2",
        "OJ_CfkueFXU2",
        "xCyzhPazFXU2",
        "bPOa1uS0FuFZ",
        "bXLpbW81FuFo",
        "9yQsM0QzFuFo",
        "27f_mJLkFuFp",
        "ByFpF3jyFuFp",
        "50TgE0duFuFq",
        "24m8DBMKFuFq",
        "b_TWlK7XFuFq",
        "Zkk3HaqCFuFr",
        "QTYd8NJ9FuFr",
        "tg1CPd0BFuFr",
        "8t09a_dWGBrU",
        "TKE39_IJMWvh",
        "WdfbPA5qGBrj",
        "whtu8E8XGBrk",
        "1yHL2j5KGBrl",
        "F-dC7WJ7GBrl",
        "gsh5EawtGBrl",
        "pdyOrgv8GBrm",
        "K2Z0Gp0RGBrm",
        "TIBgB5lkGBrm",
        "NvdNnDEsGBrn",
        "3uTbIWTWGBrn",
        "7mrrvSw6Sjku",
        "WhmGwHRUGhid",
        "jW31L0z5JH_D",
        "efvr7lJWGCvA",
        "NVOr6AErGCvA",
        "EkCDSwTMGCvA",
        "tki4hy54GCvB",
        "tZR-DWmxGCvB",
        "H_gcQUPXiCFJ",
        "1Q1TeJ6M_ZY-",
        "KsTUgQ7qGCvB",
        "Ely3RRkBGCvC",
        "cNWf4DGTGUnz",
        "fFWgl4_HGUn7",
        "NMOIVGPMGUoA",
        "A_fskrYuGDe3",
        "HxRHhP-yGd-S"
      ],
      "authorship_tag": "ABX9TyM8BQNTL0SlDJVYbzGufvuu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z1LF/nlp100-2020/blob/main/%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF2020%5Bnlp2020%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 言語処理100本ノック 2020\n",
        "\n",
        "- 問題リンク[ https://nlp100.github.io/ja/ ]\n",
        "- GitHub [ https://github.com/nlp100/nlp100.github.io ]\n",
        "- 参考リンク\n",
        "  - 文字列処理に関する情報 | note.nkmk.me [ https://note.nkmk.me/string/ ] \n",
        "    >Pythonやるときはこのサイトにお世話にならないと何もできないので…\n",
        "\n",
        "    \n",
        "      "
      ],
      "metadata": {
        "id": "BAYv_FCnErn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 解答例 "
      ],
      "metadata": {
        "id": "ExnBfpTnY-hJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Links\n",
        "\n",
        "- 【言語処理100本ノック 2020】Pythonによる解答例まとめ by @yanmaru\n",
        "  - [Qiita](https://qiita.com/yamaru/items/0cac24710626333bd693)\n",
        "  > 解答例とはいえいきなり解答が書いてあって、その後に参考リンクが記されている\n",
        "    自分で悩んで解いた後に参考程度に見るのが良さそう\n",
        "    \n",
        "- 「言語処理100本ノック 2020」をPythonで解く by u++(upura)\n",
        "  - [はてなブログ](https://upura.hatenablog.com/entry/2020/04/14/024948)\n",
        "  - [GitHub](https://github.com/upura/nlp100v2020)\n",
        "  > ブログの方では解説や参考記事も載っているので見やすい。\n",
        "\n",
        "- Python-機械学習-自然言語処理-言語処理100本ノック 2020\n",
        "  - [ブログ](https://www.takapy.work/archive/category/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%202020)\n",
        "  > 個人ブログ。解説とかはなく単純に動くコードが乗っている備忘録的なもの。\n",
        "\n",
        "- 100本ノックシリーズ by tomowarkar\n",
        "  - [GitHub blog](https://tomowarkar.github.io/blog/posts/100series/)\n",
        "  > 第5章のCaboChaの導入で見つけたブログ。\n",
        "    言語処理だけじゃなくデータサイエンス100本ノックもやってるみたい。\n",
        "\n",
        "\n",
        "- Pythonで自然言語処理100本ノック 2020を解いたついでに死ぬほど詳しく解説を書いていく[第1章 準備運動] by @python_kenichi\n",
        "  - [Qiita](https://qiita.com/python_kenichi/items/b1fcecc4274511e4c26e)\n",
        "  > 第1章のみだが丁寧に考え方や必要な知識を解説してくれている。続きがないのが惜しまれる。"
      ],
      "metadata": {
        "id": "7n5fEAYeZMbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第1章: 準備運動"
      ],
      "metadata": {
        "id": "qJbCeI_9Etpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 00. 文字列の逆順\n",
        "文字列\"stressed\"の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．\n",
        "\n"
      ],
      "metadata": {
        "id": "N8LrAkjcDWMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8LNmNEUDVdc"
      },
      "outputs": [],
      "source": [
        "str = \"stressed\"\n",
        "ans = str[::-1]\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01. 「パタトクカシーー」\n",
        "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．\n"
      ],
      "metadata": {
        "id": "TiHqMcclED6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str=\"パタトクカシーー\"\n",
        "print(str[::2])"
      ],
      "metadata": {
        "id": "ymEKDM4oEbw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
        "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．\n",
        "\n"
      ],
      "metadata": {
        "id": "zZ06yBtKEIJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = \"パトカー\"\n",
        "str2 = \"タクシー\"\n",
        "ans = \"\".join([(str1[i]+str2[i]) for i in range(4)])\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "PE0gdpBVEHzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 03. 円周率\n",
        "\"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．\n",
        "\n",
        "- `str.split(\"X\")` = 文字Xを基準に分割を行う\n",
        "  - デフォの引数はスペースなので別にスペース入れる必要はなかったかも\n",
        "- `str.strip(\"X\")` = 文字列の先頭・末尾にある文字Xを除去する\n",
        "\n",
        "> 最初普通にsplitだけで「できたやん！」と思ったけど出力されたリストが\n",
        "  [3,1,4,1,6...]←！？となっていてカンマの存在に気づくなど"
      ],
      "metadata": {
        "id": "8jkcGrsWEL_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
        "sep_str = str.split()\n",
        "[len(s.strip(\",. \")) for s in sep_str]"
      ],
      "metadata": {
        "id": "6mcyVFiOEOHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04. 元素記号\n",
        "\"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．\n",
        "\n",
        "- `str.strip`を使わなくても`str.replace`で良かった…てかコッチのほうが汎用性高いし…\n",
        "- len(str)=intはイテラブルなオブジェクトではないのでちゃんとrange()する。\n",
        "- `値1 if [条件式1] else 値2 if [条件式2] else 値3`\n",
        "  - というふうにするとPythonで三項演算子が実装できる。\n",
        "  - 条件式1がTrueのときは値1…という感じ。\n",
        "- 辞書は`{}`を使って定義。\n",
        "  - `Dict[Key]=Value`で辞書DictのKeyの項目に値Valueを追加する\n",
        "\n",
        "> Pythonでも三項演算子のようなものが使えるらしい。リスト内包表記といい、後置といい、ifの置き方が自由すぎる\n",
        "\n",
        "> 連想配列、どの言語でも表記が思い出せなくてうまく使えないがち。\n",
        "\n",
        "#### enumerate\n",
        "`enumerate(Object)`とすると`[インデックス番号,要素]`が戻ってくる\n",
        "- コレを使うと簡単に書ける(追記コードブロック)"
      ],
      "metadata": {
        "id": "SkHpEeLmEObx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
        "numList = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
        "str = str.replace((\",\"or\".\"),\"\").split(\" \")\n",
        "ans = {}\n",
        "\n",
        "for i in range(len(str)):\n",
        "  str_head = str[i][0] if (i+1 in numList) else str[i][0:2]\n",
        "  #print(str_head)\n",
        "  ans[str_head] = i\n",
        "\n",
        "ans"
      ],
      "metadata": {
        "id": "D6B4gJsVEPqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_str = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
        "numList = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
        "str = raw_str.replace((\",\"or\".\"),\"\").split(\" \")\n",
        "ans = []\n",
        "\n",
        "for i, word in enumerate(str):\n",
        "  str_head = word[0] if (i+1 in numList) else word[:2] \n",
        "  ans.append([str_head,i])\n",
        "\n",
        "dict(ans)"
      ],
      "metadata": {
        "id": "MtrIrjFy1AJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 05. n-gram\n",
        "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，\"I am an NLPer\"という文から単語bi-gram，文字bi-gramを得よ．\n",
        "\n",
        "> まず「n-gram」ってなんやねん！！！\n",
        "  参考 → N-gramの作り方 @kazmaw | Qiita [https://qiita.com/kazmaw/items/4df328cba6429ec210fb]\n",
        "  n-gramは文字列などにおける連続したn個のまとまりでグループ化したもの\n",
        "\n",
        "- n個連続で拾う際にカウントは「最後の文字が含まれるまで」なので範囲は`len(X)-n+1`となることに注意\n",
        "- for文で一旦書いてから(できそうなら)内包表記に変換するとやりやすいかも(主観)\n",
        "\n",
        "> n-gramがわかれば大したことはしていない\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Imyqw0ffEQIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N_gram(n,X): # オブジェクトXのn-gramを返す関数\n",
        "  ## nがオブジェクト全体の長さよりも大きい場合はとりあえず全部返すようにしようか…わかんないけど…\n",
        "  if len(X) < n:\n",
        "    return [X]\n",
        "\n",
        "  \"\"\"\n",
        "  # for文による表記\n",
        "  ret=[]\n",
        "  for i in range(len(X)-n+1):\n",
        "    ret.append(X[i:i+n])\n",
        "  \"\"\"\n",
        "  ret = [X[i:i+n] for i in range(len(X)-n+1)]\n",
        "  \n",
        "  return ret\n",
        "\n",
        "raw_text = \"I am an NLPer\"\n",
        "words = raw_text.split(\" \")\n",
        "\n",
        "print(\"--- 文字 n-gram ---\")\n",
        "print(N_gram(1,raw_text))\n",
        "print(N_gram(2,raw_text))\n",
        "print(N_gram(3,raw_text))\n",
        "\n",
        "\n",
        "print(\"--- 単語 n-gram ---\")\n",
        "print(N_gram(1,words))\n",
        "print(N_gram(2,words))\n",
        "print(N_gram(3,words))\n"
      ],
      "metadata": {
        "id": "2zS9UmqWEcpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 06. 集合\n",
        "\"paraparaparadise\"と\"paragraph\"に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．\n",
        "\n",
        "- `set`型オブジェクト [ https://note.nkmk.me/python-set/ ]\n",
        "  - set型は重複しない要素の集合オブジェクト。\n",
        "  - 定義の際は`{}`で生成できる\n",
        "    - 空のsetを作りたいときは単に`s={}`とするとdict型になってしまうので、`s=set()`と明示的にコンストラクタを呼んで定義する必要がある\n",
        "  - 内部には異なる型のオブジェクトを(変更不能なイミュータブルな型であれば)含むことが出来る。\n",
        "    - 変数やlistのような更新可能な(ミュータブルな)オブジェクトは不可能\n",
        "  - set()を利用してリストやタプルから重複要素を取り除けるが、順序は保持されない。\n",
        "\n",
        "  - 関数など\n",
        "    - `set_A | set_B` または `set_A.union(set_B)` でAとBの **和集合** を得られる\n",
        "    - `set_A & set_B` または `set_A.intersection(set_B)` でAとBの **積集合** を得られる\n",
        "    - `set_A - set_B` または `set_A.difference(set_B)` でAとBの **差集合** を得られる (AがBより大きい場合)\n",
        "    - `set_A ^ set_B` または `set_A.symmetric_difference(set_B)` でAとBの **対称差集合** を得られる (XOR=どちらか片方にのみ含まれる要素の集合)\n",
        "    - `set_A.isdisjoint(set_B)` でAとBが **互いに素かどうか** を判定できる\n",
        "\n",
        "- print内で使えるformat関数\n",
        "  - `print(\"hoge : {0}, fuga : {1}\".format(ans1, ans2)` みたいに書くと勝手にansが代入できる。\n",
        "  - 文字列と変数を同時にprintしたいときに便利\n",
        "  - 毎回使おうとするけど表記が思い出せないやつ……。\n",
        "\n",
        "- ｆ文字列 [Python v3.6以降]\n",
        "  - `f\"hoge : {ans1}, fuga : {ans2}\"`で上記のformat記法と同じ結果が出せるやつ。\n",
        "  - 書き方としてはコッチのほうがかなりわかりやすい"
      ],
      "metadata": {
        "id": "nIOikAjiESFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N_gram(n,X): # オブジェクトXのn-gramを返す関数\n",
        "  if len(X) < n:\n",
        "    return [X]\n",
        "\n",
        "  ret = [X[i:i+n] for i in range(len(X)-n+1)]\n",
        "  return ret\n",
        "\n",
        "text1 = \"paraparaparadise\"\n",
        "text2 = \"paragraph\"\n",
        "\n",
        "# 文字 bi-gram の集合\n",
        "X = N_gram(2,text1)\n",
        "setX = set(X)\n",
        "Y = N_gram(2,text2)\n",
        "setY = set(Y)\n",
        "\n",
        "\n",
        "print(\"X | Y : {0}\".format(setX | setY))\n",
        "\n",
        "print(\"X & Y : {0}\".format(setX & setY))\n",
        "\n",
        "print(\"X - Y : {0}\".format(setX - setY))\n",
        "\n",
        "\n",
        "print('\"se\" in X : {0}'.format(\"se\" in setX))\n",
        "\n",
        "print('\"se\" in Y : {0}'.format(\"se\" in setY))"
      ],
      "metadata": {
        "id": "0DA_btFBERzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 07. テンプレートによる文生成\n",
        "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=\"気温\", z=22.4として，実行結果を確認せよ．\n",
        "\n",
        "> 偶然前の問題でformat関数を解説してしまったので一瞬で解けて終わった…"
      ],
      "metadata": {
        "id": "WhYuAmdaEUq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def templete(x,y,z):\n",
        "  return \"{0}時の{1}は{2}\".format(x,y,z)\n",
        "\n",
        "print(templete(12,\"気温\",22.4))"
      ],
      "metadata": {
        "id": "YFFP2kiQEdNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 08. 暗号文\n",
        "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
        "\n",
        "* 英小文字ならば(219 - 文字コード)の文字に置換\n",
        "* その他の文字はそのまま出力\n",
        "\n",
        "この関数を用い，英語のメッセージを暗号化・復号化せよ．\n",
        "\n",
        "#### 大文字/小文字の判定\n",
        "- `str.islower()`文字列strが全て小文字のときにTrueを返す\n",
        "  - 全角文字であっても大文字小文字の区別があれば判定される\n",
        "  - 全て数字や日本語の様に大文字と小文字の区別がない文字種の時はfalse\n",
        "\n",
        "- 同様に`str.isupper()`は文字列strが全て大文字のときにTrueを返す\n",
        "- `str.istitle()`は文字列strが最初の文字だけ大文字、他はすべて小文字のときにTrueを返す\n",
        "\n",
        "#### 文字コードの変換\n",
        "- `ord('文字') = asciiコード`\n",
        "- `chr(数値) = 文字`\n",
        "\n",
        "\n",
        "> どっちの作業も無知だたので…、"
      ],
      "metadata": {
        "id": "wLGsCLQpEWdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cipher(text):\n",
        "  ans = \"\"\n",
        "  for c in text:\n",
        "    ans += chr(219-ord(c)) if(c.islower()) else c\n",
        "  return ans\n",
        "\n",
        "message = \"This Text is 2022年 Marchに 作られた 暗号です !!\"\n",
        "print(cipher(message))\n",
        "## out > Tsrh Tvcg rh 2022年 Mzixsに 作られた 暗号です !!\n",
        "\n",
        "## 219-ord(c)で暗号化したものは、219-(219-ord(c))=ord(c)なので同じ関数で複合可能\n",
        "encrypted_message = \"Tsrh Tvcg rh 2022年 Mzixsに 作られた 暗号です !!\"\n",
        "print(cipher(encrypted_message))"
      ],
      "metadata": {
        "id": "iJ71003AEWMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 09. Typoglycemia\n",
        "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば\"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"）を与え，その実行結果を確認せよ．\n",
        "\n",
        "#### ランダムな並び替え [ https://note.nkmk.me/python-random-shuffle/ ]\n",
        "- `import random`でrandomモジュールを使用可能\n",
        "- `random.shuffle(list)`で元のリストをシャッフルする。\n",
        "- `random.sample(list)`で元のリストは変更せず、シャッフルされた新たなリストを作成する。\n",
        "  - 文字列やタプルは **変更不能(イミュータブル)な** オブジェクトなので`shuffle`を使うとエラーになる！注意\n",
        "  - 適用するときは`sample`の方を使おう\n",
        "- listしかシャッフルできないのでstrもリストの形式にネスト[list]する必要がある。\n",
        "\n",
        "- 文字列の結合は`\"<Space>\".join(list)`ですね……(第2問)"
      ],
      "metadata": {
        "id": "XYef5GAZEX9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def Typoglycemia(str):\n",
        "  if len(str)<=4:\n",
        "    return str\n",
        "  else:\n",
        "    shuffle = random.sample(str[1:-1], len(str[1:-1]))\n",
        "    return \"\".join([str[0]] + shuffle + [str[-1]])\n",
        "\n",
        "text = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
        "ans = [Typoglycemia(words) for words in text.split(\" \")]\n",
        "ans = \" \".join(ans)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "IVwxEd1FEYPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第2章: UNIXコマンド\n",
        "\n",
        "popular-names.txtは，アメリカで生まれた赤ちゃんの「名前」「性別」「人数」「年」をタブ区切り形式で格納したファイルである．以下の処理を行うプログラムを作成し，popular-names.txtを入力ファイルとして実行せよ．さらに，同様の処理をUNIXコマンドでも実行し，プログラムの実行結果を確認せよ．\n",
        "\n",
        "#### Linuxコマンドの使用\n",
        "Google Colaboratory上ではコマンド部に`!`をPrefixとしてLinuxコマンドが使用できる\n",
        "\n",
        "#### データのダウンロード\n",
        "Google Colaboratory上で指定データを使用するにはwgetコマンドを使用する\n",
        "> `!wget https://nlp100.github.io/data/popular-names.txt`\n",
        "<br>これを実行するとカレントディレクトリに対象のテキストファイルを置くことが出来る"
      ],
      "metadata": {
        "id": "oeaShrnkFXUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/popular-names.txt"
      ],
      "metadata": {
        "id": "n9AOmWoLT64c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cC8xCmK74J4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. 行数のカウント\n",
        "行数をカウントせよ．確認にはwcコマンドを用いよ．\n",
        "\n",
        "#### データの読み込み\n",
        "- pandasを利用 [ https://note.nkmk.me/python-pandas-read-csv-tsv/ ]\n",
        "  ##### 読み込み時メモ\n",
        "  - `pd.read_csv`は区切り文字が`,`で固定されているだけで`pd.read_table`と根本的に同じ。\n",
        "    > `pd.read_csv(hoge)` == `pd.read_table(hoge, sep=\",\")`\n",
        "  - read時に`names`オプションでカラム名を名付ける\n",
        "\n",
        "#### 確認\n",
        "- Linuxの`wc`コマンドは`$wc <filename>`で使用する\n",
        "  - 順に[行数, 単語数(空白で区切られた文字列の数), ファイル容量(byte)]で結果が表示される\n",
        "\n",
        "> Google Colaboratoryでローカルなデータ扱うの不利じゃない！？と思ったけど普通にできるんだ。<br>\n",
        "  Linuxのコマンドも普通に使えるっぽいですね"
      ],
      "metadata": {
        "id": "oreVQ8KdFXU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXfRnG9YFXU0"
      },
      "outputs": [],
      "source": [
        "# データがディレクトリに存在するか確認\n",
        "!ls\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print(df)\n",
        "\n",
        "print(f\"行数 : {len(df)}\\n\")\n",
        "\n",
        "!wc popular-names.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. タブをスペースに置換\n",
        "タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．\n",
        "\n",
        "#### ~~「確認」~~→置換に使用するコマンド\n",
        "- sedコマンド : *__S__tream __ED__itor* `sed [option] <filename>`\n",
        "  - sedは指定したファイルをコマンドに従って処理し、標準出力へと出力する。のでパイプとして使うのが一般的。\n",
        "  - オプション`-e`で処理内容を設定(省略時はオプション以外の第一引数が処理内容)\n",
        "    - `$ cat ... | sed -e ... -e ...`のように連続した処理も可能\n",
        "  - `-s` : 正規表現を使った置換を行う。\n",
        "- trコマンド : _**TR**anslate/**TR**ansliterate_ `tr [置換前の文字セット] [置換後の文字セット] <in_filename> <out>`\n",
        "  - `$ tr 012 abc`とすると [0→a,1→b,2→c] という置換が行われる。\n",
        "    - 1対1対応なので常に長さを揃える必要がある\n",
        "  -\n",
        "- expandコマンド `expand [option] <filename>`\n",
        "  - タブを空白に置き換えるコマンド\n",
        "    - `-t`オプションでタブを幾つの空白文字に置き換えるか設定する。デフォルトは8個分。\n",
        "  - 逆に、unexpandコマンド `unexpand [option] <filename>`は空白をタブに置換する\n",
        "\n",
        "\n",
        "#### 本当に確認用のコマンド\n",
        "- `head <filename>`\n",
        "  - 指定したファイルを頭から表示する。デフォルトでは先頭10行が表示される。\n",
        "  - `-c`で文字数、`-n`で行数を指定する\n",
        "\n",
        "> なぜかsedだけTabの幅がSpaceでも残ってしまった…どうすれば消えてくれるんだ…まあべつにええか…"
      ],
      "metadata": {
        "id": "Ju_6iik5FXU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -e \"s/<tab>/ /g\" popular-names.txt | head -v -n 5\n",
        "!cat popular-names.txt | tr \"\\t\" \" \"  | head -v -n 5\n",
        "!expand popular-names.txt -t 1 | head -v -n 5"
      ],
      "metadata": {
        "id": "J7jprPLSFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. 1列目をcol1.txtに，2列目をcol2.txtに保存\n",
        "各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．\n",
        "\n",
        "- cutコマンド : `cut [option] <filename>`\n",
        "  - ファイルを読み込んで、それぞれの行から指定した部分だけを切り出すコマンド。\n",
        "  - 切り出す部分の指定は`-[範囲指定オプション] <数字>`で指定する\n",
        "    - `-b`でバイト単位、`-c`で文字数単位、`-f`でフィールド(区切り文字毎)\n",
        "    > `cut -f 1,7 --delim=\":\" hoge.txt` -> hoge.txtの\":\"で区切られた部分の1番目と7番目の列を拾ってくる。\n",
        "  - `-d`,`--delimiter` オプションでどの文字を区切りとするか指定できる。デフォルトではタブが区切り文字になっている。\n",
        "\n",
        "#### pandasを使用する場合\n",
        "pandasではデフォルトで列切り出しが可能\n",
        "- `df[n]`でn列目を拾ってこれる\n",
        "  - カラム名を指定していたらその文字列で指定する必要がある\n",
        "- `df.to_csv(<filename>, index=False)`とかやれば書き出しができる\n",
        "  - 行番号を消すオプションは`index=False`"
      ],
      "metadata": {
        "id": "IH4RiaQbFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | head -n 5\n",
        "!cut -f 1 popular-names.txt > col1.txt\n",
        "!cut -f 2 popular-names.txt | head -n 5\n",
        "!cut -f 2 popular-names.txt > col2.txt\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print(df[\"Name\"][:5])\n",
        "print(df[\"Sex\"][:5])\n"
      ],
      "metadata": {
        "id": "WkpEVhdTFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. col1.txtとcol2.txtをマージ\n",
        "12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．\n",
        "\n",
        "- Pasteコマンド : `$ paste <file_1> <file_2>`\n",
        "  - 複数のファイルを行で連結するコマンド。\n",
        "  - `-d`オプションを使うと連結時の区切り文字を変更できる。デフォルトではタブ。\n",
        "\n",
        "#### pandasを使用する場合\n",
        "特定の行・列を抽出する場合はlocが結局無難\n",
        "- `df.loc[行 , 列]` : 指定した行と列を拾ってくる。\n",
        "  - 行・列は複数のlist表記も可能。\n",
        "- `df.iloc[行 , 列]` : 「数字で」指定した行と列を拾ってくる。\n",
        "  - locの指定がindex名やcolumn名なのに対してこちらは番号で指定できる"
      ],
      "metadata": {
        "id": "3tC88THbFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!paste col1.txt col2.txt | head -n 5\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "print( df.iloc[0:5,0:2] )"
      ],
      "metadata": {
        "id": "CnuWxJRTFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. 先頭からN行を出力\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．\n",
        "\n",
        "#### headコマンド\n",
        "ファイルの上からn行を表示する。\n",
        "> もう今まで確認で散々使っとるやんけ……………！！！！\n",
        "\n",
        "\n",
        "#### Pythonでコマンドライン引数を受け取る\n",
        "`import sys`モジュール内の`argv`を使用する。\n",
        "- `args = sys.argv`で呼び出し時にコマンドラインによって与えた引数がargsに格納される\n",
        "- `args[0]`は実行ファイル名(`hoge.py`)になる\n"
      ],
      "metadata": {
        "id": "_nBP6zSgFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head popular-names.txt -n 5\n",
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowHead(df,n):\n",
        "  print(df.head(n))\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "\n",
        "ShowHead(df,args[1]) if len(args)==2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "8VFMVOOIFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. 末尾のN行を出力\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．\n",
        "\n",
        "> 末尾を参照するTailになっただけでやることは1個前と完全に同じ。"
      ],
      "metadata": {
        "id": "gH__H51uFXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tail popular-names.txt -n 5\n",
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowTail(df,n):\n",
        "  print(df.tail(n))\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "ShowTail(df, 3)\n",
        "ShowTail(df,args[1]) if len(args)==2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "1j0SXdREFXU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. ファイルをN分割する\n",
        "自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．\n",
        "\n",
        "#### splitコマンド\n",
        "`!split -n 5 popular-names.txt`みたいな感じにすれば分割できるけど、分割したらファイルが勝手に出力されるので面倒なことになってしまう。Pandasだけでええか…\n",
        "\n",
        "\n",
        "> これもやってることは「分割の計算」→「切り出し」なので、表示さえできればやるだけ"
      ],
      "metadata": {
        "id": "B-n_nvttFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!split -n 5 popular-names.txt | head -n 5\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "def ShowHeadAndTail(df,n=2): ## 全部表示すると流石に面倒なので上下のn行だけ表示するようにして確認\n",
        "  print(df.head(n))\n",
        "  print(\" ……… \")\n",
        "  print(df.tail(n))\n",
        "\n",
        "def Separate(df,n):\n",
        "  size = int(len(df)/n) # 先に区切りの数を決めておくとキャスティングが統一されてラク\n",
        "\n",
        "  for i in range(n):\n",
        "    ShowHeadAndTail(df[i*size:(i+1)*size], 2)\n",
        "    print(\"----- -----\")\n",
        "\n",
        "import sys\n",
        "args = sys.argv\n",
        "args[1] = 5 if len(args)>=2 else args[1]\n",
        "Separate(df,args[1]) if len(args)>=2 else print(\"引数エラー\")\n",
        "## 一応引数受け取れるようにしたけど、Google Colaboratoryでは一旦マウントしないとだめらしいので放置します。"
      ],
      "metadata": {
        "id": "428FqOK8FXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. １列目の文字列の異なり\n",
        "1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはcut, sort, uniqコマンドを用いよ．\n",
        "\n",
        "#### uniq コマンド\n",
        "重複している行を取り除くコマンド。 [ https://atmarkit.itmedia.co.jp/ait/articles/1611/14/news021.html ]\n",
        "- ただし、**uniqコマンドは隣り合った行しか比較しない**ので、先にsortコマンドを使って並べ替えるのが一般的らしい\n",
        "\n",
        "ちなみに、sortコマンドに`-u`という重複行を取り除くオプションがあるらしいのであんまり意味ないな…\n",
        "\n",
        "\n",
        "#### Pythonを使用した解答\n",
        "> 先の問題で「set」を使用した重複の排除があったので思わずそっちを使ったが、Pandasくんに便利な機能がある\n",
        "\n",
        "- `df.duplicated()` : 重複した行を抽出する\n",
        "  - 戻り値は重複した行がTrueとなるBool値の行。デフォルトでは全ての列要素が一致したときに同じ行とみなされる。\n",
        "  - 引数`subset`を指定することで同じ要素であるときに重複と判定するカラムを指定できる\n",
        "    - `df.duplicated(subset=\"Name\")`みたいな感じ\n",
        "- `df.drop_duplicates()` : 重複した行を削除する\n",
        "  - この辺のメソッドを使用すると重複した行をカウントできる"
      ],
      "metadata": {
        "id": "MxfCADGMFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | sort | uniq | wc"
      ],
      "metadata": {
        "id": "8EXYXKElDMpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "Column_1_set = set(df.iloc[:,0])\n",
        "len(Column_1_set)"
      ],
      "metadata": {
        "id": "H8RZ6MkJFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. 各行を3コラム目の数値の降順にソート\n",
        "各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．"
      ],
      "metadata": {
        "id": "OJ_CfkueFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sMC0YogFFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "df.sort_values([\"Population\"])"
      ],
      "metadata": {
        "id": "fDnTABk3Enmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる\n",
        "各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．\n",
        "\n",
        "- `uniq -c`でuniqの重複時に出現回数をカウントする\n",
        "- `sort -n`でソート時に数字として認識する\n",
        "  - コレをしないと辞書順で判定されるため [1743] が [94] より先に出てくることになる\n",
        "\n",
        "#### Pandasを使用した解答\n",
        "Pandasを利用するのであれば`pandas.Series.value_counts()`を使用することでユニークな要素のindexと個数を取得できる\n",
        "- デフォルトでは出現回数が多いものから順にソートされる\n",
        "  - 引数ascending=Trueとすると昇順にソートされ、引数sort=Falseとするとソートされない。"
      ],
      "metadata": {
        "id": "xCyzhPazFXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -f 1 popular-names.txt | sort | uniq -c | sort -n -r | head -n 10"
      ],
      "metadata": {
        "id": "e5jxJB9_F2M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### By Use Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_table( \\\n",
        "          \"popular-names.txt\", \\\n",
        "          sep=\"\\t\", \\\n",
        "          header=None, \\\n",
        "          names=[\"Name\", \"Sex\", \"Population\", \"Year\"] )\n",
        "\n",
        "df[\"Name\"].value_counts()"
      ],
      "metadata": {
        "id": "4S_PWR7RFXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第3章: 正規表現\n",
        "\n",
        "Wikipediaの記事を以下のフォーマットで書き出したファイルjawiki-country.json.gzがある．\n",
        "- 1行に1記事の情報がJSON形式で格納される\n",
        "- 各行には記事名が”title”キーに，記事本文が”text”キーの辞書オブジェクトに格納され，そのオブジェクトがJSON形式で書き出される\n",
        "- ファイル全体はgzipで圧縮される\n",
        "\n",
        "らしいので例によってまずデータをローカルに保存します…。\n",
        "- `!wget https://nlp100.github.io/data/jawiki-country.json.gz`\n",
        "- `!gunzip jawiki-country.json.gz`\n",
        "\n",
        "とすると`jawiki-country.json`がローカルに配置される。\n",
        "> ちなみに、gzipを解凍(伸張)すると元の圧縮ファイルは自動で消去される。\n",
        "  そのため実質的には変換を行っているのと同等。"
      ],
      "metadata": {
        "id": "bPOa1uS0FuFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/jawiki-country.json.gz"
      ],
      "metadata": {
        "id": "ndIAHDnkJySw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip jawiki-country.json.gz"
      ],
      "metadata": {
        "id": "T2vF6vFhKR2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "VyJWZWXKzbz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. JSONデータの読み込み\n",
        "Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．\n",
        "**問題21-29では，ここで抽出した記事本文に対して実行**せよ．\n",
        "\n",
        "#### Pandasを使用したJSONの扱い\n",
        "- `pd.read_json(<filename>)`でJSON形式のファイルを読み込める\n",
        "  - JSON Lines形式の場合は引数を`lines=True`とする必要がある\n",
        "    - JSON LinesとよばれるJSONが改行で区切られたフォーマット(拡張子が`.jsonl`の時もある)\n",
        "  - `compression='infer'`と引数を設定すると**対応する圧縮方式が自動で選ばれて圧縮ファイルを読み込める**\n",
        "    - 対応する圧縮方式は{`.gz`,`.bz2`,`zip`,`xz`}など。\n",
        "\n",
        "> PandasってJSONファイルも読めんの！！\n",
        "<br>えっしかもgzファイルのまま読めんの！？！？？！？すご！？？！？！\n",
        "\n",
        "- pandas.dataframeはbool値をもつリストでカバーをすることが出来る\n",
        "  - `df = [A,B,C]`とあったときに、`X = [False,True,False]`のListXを使って\n",
        "    `df[X]`とすると、`df[X] -> B`となる\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXLpbW81FuFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzAmFuTOFuFo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "\n",
        "print(df[\"title\"]==\"イギリス\")\n",
        "print(df[df[\"title\"]==\"イギリス\"])\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "#print(uk_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(uk_text)"
      ],
      "metadata": {
        "id": "Aqn47zdm4rCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. カテゴリ名を含む行を抽出\n",
        "記事中でカテゴリ名を宣言している行を抽出せよ．\n",
        "\n",
        "textをみるとカテゴリ宣言は`Category:イギリス`のようにされているのでこの部分を含む行を抽出できるようにすれば良い\n",
        "\n",
        "#### Pythonを使用した文字列の検索(grep的処理)\n",
        "[https://note.nkmk.me/python-grep-like/]\n",
        "\n",
        "\n",
        "#### Python内で正規表現を扱う`re`モジュール\n",
        "[https://note.nkmk.me/python-re-match-search-findall-etc/]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yQsM0QzFuFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forによる表記\n",
        "\"\"\"\n",
        "for line in uk_text.split(\"\\n\"):\n",
        "  if \"[Category:\" in line:\n",
        "    print(line)\n",
        "\"\"\"\n",
        "# リスト内包表記 + if条件式\n",
        "Category_lines = [line for line in uk_text.split(\"\\n\") if \"[Category\" in line]\n",
        "print(Category_lines)"
      ],
      "metadata": {
        "id": "m0al_VEzFuFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. カテゴリ名の抽出\n",
        "記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．\n",
        "\n",
        "#### reモジュールで正規表現\n",
        "- `match()` : マッチオブジェクト\n",
        "  - 位置を取得 : 先頭`start()`, 末尾`end()`, 範囲`span()`\n",
        "  - 文字列を取得 : `group()`\n",
        "  - 各グループの文字列を取得 : `groups()`\n",
        "- `search()` : 文字列全てから検索し、マッチした場合にマッチオブジェクトを返す。\n",
        "  - ただし、マッチする部分が複数ある場合は最初のマッチのみが返されるため、1つずつ処理したい時以外は他のを使うことになる\n",
        "- `findall()` : マッチするすべての部分文字列をリストにして返す。\n",
        "- `finditer()` : マッチするすべての部分文字列のマッチオブジェクトをイテレータで返す。\n",
        "  > 多分この2種を使う。\n",
        "  - 正規表現の一部に()を使用すると、マッチングした際に()内の部分のみ[Group]を返すようになる。\n",
        "  - 括弧で囲む際に先頭に`?P<hoge>`と記述することで該当グループに任意の名前\"hoge\"をつけることが出来る。\n",
        "    - このグループ名はマッチングオブジェクトに対してgroup()などで名前を指定して取得できる様になる。\n",
        "  - \n",
        "\n",
        "\n",
        "\n",
        "#### エスケープシーケンスの無効化 (raw文字列)\n",
        "[https://note.nkmk.me/python-raw-string-escape/]\n",
        "正規表現の際は`\\(=バックスラッシュ)`を多用することが多い。これがエスケープシーケンスとして認識されるのを防ぐため、**raw文字列**として扱う。\n",
        "- 文字列リテラル`\"hoge\"`などを表記する際、その直前に`r`または`R`をつけるとそのままの値が文字列となる\n",
        "  - `\"a\\tb\\nA\\tB\"` → \"a[tab]b[改行]A[tab]B\" // 普通に入れるとエスケープシーケンスが展開される\n",
        "  - `a\\\\tb\\\\nA\\\\tB` → \"a\\tb\\nA\\tB\" // バックスラッシュをエスケープシーケンスで表記した場合\n",
        "  - `r\"a\\tb\\nA\\tB\"` → \"a\\tb\\nA\\tB\" // エスケープシーケンスを無効化したraw文字列\n",
        "\n",
        "#### マッチングの除外\n",
        "今回は「イギリス｜＊」のようにマッチングしているが拾わないで欲しい文字列も存在している。\n",
        "こういうときに`(?:hoge)`みたいな表記したら良い\n",
        "> らしいけどちょっとココ以外のソースが見つからんかったので詳しくはよくわかりません…\n",
        "  [https://qiita.com/yamaru/items/255d0c5dcb2d1d4ccc14#22-%E3%82%AB%E3%83%86%E3%82%B4%E3%83%AA%E5%90%8D%E3%81%AE%E6%8A%BD%E5%87%BA]\n",
        "\n",
        "- Pythonの公式Docに書いてあった！[https://docs.python.org/ja/3/library/re.html]\n",
        "\n",
        "> `(?:...)`\n",
        "普通の丸括弧の、キャプチャしない版です。丸括弧で囲まれた正規表現にマッチしますが、このグループがマッチした部分文字列は、マッチを実行したあとで回収することも、そのパターン中で以降参照することもできません 。"
      ],
      "metadata": {
        "id": "27f_mJLkFuFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# forによる表記\n",
        "Category_Matches = re.findall(r\"\\[Category:(.+?)(?:\\|.*)?\\]\",uk_text)\n",
        "\n",
        "for iter in Category_Matches:\n",
        "  print(iter)\n",
        "\n"
      ],
      "metadata": {
        "id": "rP6hJJB2FuFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. セクション構造\n",
        "記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ．\n",
        "\n",
        "> textを読む感じ、多分`==hoge==`だと1で`===hoge===`だと2みたいな感じなんだと思う。わからんけど。\n",
        "\n",
        "> 正規表現とにらめっこしつつ試行錯誤してたらうまくハマってキレイに取り出せたのでそのままlen()でレベル判定して終わり。"
      ],
      "metadata": {
        "id": "ByFpF3jyFuFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# forによる表記\n",
        "Section_Matches = re.findall(r\"(=+=)(?P<SectionName>[^=]+)(=+=)\",uk_text)\n",
        "\n",
        "for iter in Section_Matches:\n",
        "  #print(iter)\n",
        "  print(f\"SectionLevel : {iter[1]} = {len(iter[0])-1}\")"
      ],
      "metadata": {
        "id": "yAzfIE3NFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. ファイル参照の抽出\n",
        "記事から参照されているメディアファイルをすべて抜き出せ．\n",
        "\n",
        "> メディアファイルを探してみると\n",
        "  `[ファイル:Royal Coat of Arms of the United Kingdom.svg|85px|イギリスの国章]`みたいな感じなので\n",
        "  `\"[ファイル:(.*)|*|*]\"`みたいなかんじで取り出せそう\n",
        "\n",
        "- `|`は複数条件(OR)として機能するのでエスケープする必要がありました\n",
        "- 大カッコ [ ] で囲まれているものの、カッコ内の説明は割と不統一な感じなのでファイル名だけ取得できれば充分みたい"
      ],
      "metadata": {
        "id": "50TgE0duFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#Media_Matches = re.findall(r\"\\[(ファイル:)(.+)\\|(.+)\\|(.+?)\\]\",uk_text)\n",
        "Media_Matches = re.findall(r\"\\[(ファイル:)(.+?)\\|\",uk_text)\n",
        "\n",
        "for iter in Media_Matches:\n",
        "  #print(iter)\n",
        "  print(f\"Media File : {iter[1]}\")"
      ],
      "metadata": {
        "id": "DOD2R7vZFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. テンプレートの抽出\n",
        "記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．\n",
        "\n",
        "> 「基礎情報」を探してみると\n",
        "  `{{基礎情報 |hoge = fuga ... }}`となっていたので、\n",
        "  一旦基礎情報だけぶっこ抜いてきてその中でリスト作るようにしたほうが良いかも\n",
        "\n",
        "#### re.compile()\n",
        "今まで普通に`re.findall()`でやってきたけど、正規表現を記述してコンパイルした正規表現パターンのオブジェクトを作って、そのメソッドとして実行するのが割と一般的な使い方っぽい\n",
        "- `pattern = re.compile(<使いたい正規表現>)` : 正規表現パターンオブジェクトの作成\n",
        "  - `pattern.match(<String>)`\n",
        "  - `re.<任意のメソッド>(pattern, text)`\n",
        "  \n",
        "  のような使い方が可能\n",
        "\n",
        "\n",
        "\n",
        "#### 改行を含むマッチング\n",
        "- `re.S` = `re.DOTALL` : 使うとワイルドカード文字`.`が改行を含むあらゆる文字にマッチする\n",
        "  - 通常のドットは改行文字は含まれない\n",
        "- `re.M` = `re.MULTILINE` : 使うと複数行に対して検索を行う"
      ],
      "metadata": {
        "id": "24m8DBMKFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "#print(Basic_Info_Text) ## 基礎情報の部分のみ一旦テキスト抽出\n",
        "\n",
        "# p_Basic_Info = re.compile(r\"\\|(.+)[\\s*]=[\\s*](.+)\")\n",
        "# >> これだと公式国名みたいに途中で改行が入ってるやつが拾えない…\n",
        "p_Basic_Info = re.compile(r\"(?:\\|)(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)|(?=\\n))\",re.M+re.S)\n",
        "# > 2時間くらい格闘したけどわからなかったのでちょっとパスします…\n",
        "# >> 諦めてたら次の問題でコレの回答を使うみたいだったので悩みながら他の人の答え(たかぴーさんのブログ)をみたら(?=\\n)の前にスペースを入れるだけで解決した。なぜ？？？\n",
        "# >>> [|標語 : ...]となって標語だけうまく取り出せてない\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "print(Basic_Info_List)\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "## test ##\n",
        "Basic_Info_Dict[\"公式国名\"]\n",
        "for k,v in Basic_Info_Dict.items():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "1XqoqDsHFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 26. 強調マークアップの除去\n",
        "25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ\n",
        "- (参考: マークアップ早見表[ http://ja.wikipedia.org/wiki/Help:%E6%97%A9%E8%A6%8B%E8%A1%A8 ])\n",
        "\n",
        "> リンク先によると\n",
        "- `''他との区別''`\t他との区別\n",
        "- `'''強調'''`\t強調\n",
        "- `'''''斜体と強調'''''`\t斜体と強調\n",
        "\n",
        "> らしいので、'の2個以上のマッチングを調べて取り除けば良さそう\n",
        "\n",
        "\n",
        "> 色々元テキストから消そうと努力してたけど先に作ってる辞書で表示するときだけ`'`を弾けば良いのか……賢いな………"
      ],
      "metadata": {
        "id": "b_TWlK7XFuFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"\\|(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)| (?=\\n))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "# 強調を消す\n",
        "# Basic_Info_NoEmphasis = {i[0]:re.sub(r\"\\'{2,}\", \"\", i[1]) for i in Basic_Info_List}\n",
        "\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "for k,v in Basic_Info_Dict.items():\n",
        "  v = re.sub(r\"\\'{2,}\", \"\", v)\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "JhzVyAnbFuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 27. 内部リンクの除去\n",
        "26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: マークアップ早見表）\n",
        "\n",
        "> マッチングした文字列の一部を置換結果として利用したい……。\n",
        "\n",
        "#### `re.sub()`を使用した置換にマッチング文字列の一部を使用する\n",
        "`re.sub()`では正規表現の中でキャプチャグループが含まれている場合、置換する文字列の中で `\\1, \\2, ...` を使用することでキャプチャグループでキャプチャされた文字列を参照することができます。\n",
        "- 参考リンク [ https://www.javadrive.jp/python/regex/index10.html ]\n",
        "\n",
        "> なぜかこれは **0ではなくて1始まり** なので注意\n",
        "\n",
        "色々頑張ったけど、なぜか\n",
        "  - 国章リンク =（[[イギリスの国章|国章]]）<br>\n",
        "\n",
        "だけが消えなかった。\n",
        "正規表現チェッカー[ https://weblabo.oscasierra.net/tools/regex/ ]で確認してもちゃんとヒットしてるのになぜ？？？"
      ],
      "metadata": {
        "id": "Zkk3HaqCFuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.+?\\n\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"\\|(.+?)[\\s*]=[\\s*](.+?)(?:(?=\\n\\|)| (?=\\n))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "# 強調を消す\n",
        "p_remove_Emphasis = re.compile(r\"\\'{2,}\",re.S)\n",
        "Basic_Info_NoEmphasis = {i[0]:p_remove_Emphasis.sub(\"\", i[1]) for i in Basic_Info_List}\n",
        "\n",
        "# 内部リンクを消す\n",
        "p_remove_Link = re.compile(r\"\\[\\[(?:[^|]*\\|)*?([^|]*?)\\]\\]\",re.S+re.M)\n",
        "# [[hoge]] -> hoge , [[hoge(#foo)|fuga]] -> fuga にしたい\n",
        "## [[(.+)]] , [[.+|(.+)]]\n",
        "Basic_Info_NoLink = {i[0]:p_remove_Link.sub(r\"\\1\", i[1]) for i in Basic_Info_NoEmphasis.items()}\n",
        "\n",
        "ans = dict(Basic_Info_NoLink)\n",
        "\n",
        "for k,v in ans.items():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "QJUucQvMFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 28. MediaWikiマークアップの除去\n",
        "27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．\n",
        "\n",
        "> 普通にマークアップを何処まで消して良いのかわからんからわからん……\n",
        "  とりあえず見てて気になるところだけやった"
      ],
      "metadata": {
        "id": "QTYd8NJ9FuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.*?$(.*?)^\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "\n",
        "def remove_markup(text):\n",
        "\n",
        "  # 強調を消す\n",
        "  p_remove_Emphasis = re.compile(r\"\\'{2,}\",re.S)\n",
        "  text = p_remove_Emphasis.sub(\"\", text)\n",
        "\n",
        "  # 内部リンクを消す\n",
        "  p_remove_Link = re.compile(r\"\\[\\[(?:[^|]*\\|)*?([^|]*?)\\]\\]\",re.S+re.M)\n",
        "  text = p_remove_Link.sub(r\"\\1\", text)\n",
        "\n",
        "  # 箇条書き消す\n",
        "  p_remove_kajou = re.compile(r\"^\\*\",re.S+re.M)\n",
        "  text = p_remove_kajou.sub(\"\", text)\n",
        "\n",
        "  # 外部リンク消す\n",
        "  p_remove_otherLink = re.compile(r\"[\\{\\[].*http.+[\\}\\]]\",re.S+re.M)\n",
        "  text = p_remove_otherLink.sub(\"\", text)\n",
        "\n",
        "  # HTMLタグ消す\n",
        "  p_remove_HTMLtag = re.compile(r\"<.+?>\",re.S+re.M)\n",
        "  text = p_remove_HTMLtag.sub(\"\", text)\n",
        "  \n",
        "  # {{langとか仮リンクとか|なんちゃら|かんちゃら}}←みたいなやつの最後だけ取り出す(よくわからん)\n",
        "  p_remove_Nazo = re.compile(r\"\\{\\{(?:.*\\|)*(.*?)\\}\\}\",re.S+re.M)\n",
        "  text = p_remove_Nazo.sub(r\"\\1\", text)\n",
        "\n",
        "  # 改行消す\n",
        "  p_remove_kaigyou = re.compile(r\"$|\\n\",re.S+re.M)\n",
        "  text = p_remove_kaigyou.sub(\"\", text)\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  \n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "ans = dict(Basic_Info_List)\n",
        "\n",
        "for k,v in ans.items():\n",
        "  print(f\"{k} : {remove_markup(v)}\")"
      ],
      "metadata": {
        "id": "sPM9IzeTFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 29. 国旗画像のURLを取得する\n",
        "テンプレートの内容を利用し，国旗画像のURLを取得せよ．\n",
        "（ヒント: MediaWiki APIのimageinfoを呼び出して，ファイル参照をURLに変換すればよい）\n",
        "\n",
        "> 急にAPIの話出てきた～～～何もわかんね～～～！\n",
        "\n"
      ],
      "metadata": {
        "id": "tg1CPd0BFuFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"jawiki-country.json.gz\", lines=True, compression=\"infer\")\n",
        "uk_text = df[df[\"title\"]==\"イギリス\"][\"text\"].values[0]\n",
        "\n",
        "Basic_Info_Text = re.findall(r\"\\{\\{基礎情報.*?$(.*?)^\\}\\}\", uk_text, re.MULTILINE+re.DOTALL)\n",
        "\n",
        "p_Basic_Info = re.compile(r\"^\\|(.+?)\\s*=\\s*(.+?)(?:(?=\\n\\|)|(?=\\n$))\",re.M+re.S)\n",
        "Basic_Info_List = re.findall(p_Basic_Info, Basic_Info_Text[0])\n",
        "Basic_Info_Dict = dict(Basic_Info_List)\n",
        "\n",
        "uk_flag_URL = Basic_Info_Dict[\"国旗画像\"]\n",
        "\n",
        "# ヒントのページ見たけどな～～～んもわからんかったので流石にココだけ他の方の答えのソースを無心でコピペしました…\n",
        "import requests\n",
        "uk_flag_URL = uk_flag_URL.replace(\" \",\"_\")\n",
        "url = 'https://commons.wikimedia.org/w/api.php?action=query&titles=File:' + uk_flag_URL + '&prop=imageinfo&iiprop=url&format=json'\n",
        "data = requests.get(url)\n",
        "\n",
        "print(re.search(r'\"url\":\"(.+?)\"',data.text).group(1))\n",
        "# 正直webAPI周りの箇所がマジで何やってるか何もわからん……恥ずかし……"
      ],
      "metadata": {
        "id": "PNyrDcUNFuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第4章: 形態素解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "なお，問題37, 38, 39はmatplotlibもしくはGnuplotを用いるとよい．\n",
        "\n",
        "> らしいので例によってデータをローカルに(ry\n",
        "\n",
        "- `!wget https://nlp100.github.io/data/neko.txt`\n",
        "\n",
        "> 調べてようやく知ったけど`wget`コマンドで`-O`を指定すると常にファイル名を指定して保存できるので、名前が被ったときに「`hoge.tmp`と`hoge1.tmp`ができちゃった～～」みたいな事にならないですむらしい。"
      ],
      "metadata": {
        "id": "8t09a_dWGBrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/neko.txt -O neko.txt\n",
        "!ls"
      ],
      "metadata": {
        "id": "aiHHBe0FgYs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> いきなり当然の権利のようにmecabを使ってみろと言ってくるので…\n",
        "\n",
        "#### MeCabのインストール\n",
        "MeCabは普通に外部ライブラリなのでインストールする必要がある。\n",
        "\n",
        "- 一応念の為 `!pip show mecab`してみたけど出ませんでした\n",
        "  > `WARNING: Package(s) not found: mecab`\n",
        "  \n",
        "  - 後で気付いたけどパッケージ名は`mecab-python3`らしいです…\n",
        "\n",
        "\n",
        "- ColaboratoryでMeCabを使えようにする。 - Qiita\n",
        "  - [ https://qiita.com/pytry3g/items/897ae738b8fbd3ae7893 ]\n",
        "\n",
        "コチラの記事を参考にpipを使ってMeCabをインストールする。\n",
        "\n",
        "\n",
        "あ、でも今回の問題はMeCab使って形態素解析の結果を出す作業が要るので一旦コマンドラインでMeCabを叩いてみることにします。\n",
        "\n",
        "> この手のやつ、Google Colaboratoryだと落ちるたびに上から順にやっていかないといけないからめんどくさいなぁ。\n",
        "  などと…\n",
        "  "
      ],
      "metadata": {
        "id": "TKE39_IJMWvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loeNRp3eGBrk"
      },
      "outputs": [],
      "source": [
        "!apt install aptitude\n",
        "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.7\n",
        "\n",
        "# グラフ表示用\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "!pip show mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mecab neko.txt -o neko.txt.mecab\n",
        "!head neko.txt.mecab -n 20"
      ],
      "metadata": {
        "id": "TtpdVo1BnioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 30. 形態素解析結果の読み込み\n",
        "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
        "\n",
        "\n",
        "#### 形態素解析について\n",
        "さらっと「各形態素は～として表現せよ」って言われたけど何もわからんね…\n",
        "\n",
        "> MeCabが形態素解析結果として返す情報は次のような内容である：\n",
        "\n",
        "    表層形  品詞  品詞細分類1  品詞細分類2  品詞細分類3  活用形  活用型  原形  読み  発音\n",
        "\n",
        "> 「一文を形態素のリストとして表現せよ」とあるので\n",
        "  \n",
        "    [<surface(表層形)>, <base(基本形)>, <pos(品詞)>, <pos1(品詞細分類1)>]\n",
        "    をもとに、\n",
        "    list(dict[\"<形態素名(品詞とか)>\"])\n",
        "という感じで取り出せるようにすればいいっぽい\n",
        "\n",
        "……\n",
        "\n",
        "やれば良いことはわかったけど、ファイルの読み込み方とか全くわからん…。\n",
        "最初の問題なのでおとなしく写経しようかしら…\n",
        "\n",
        "> ちまちまやってたら出来た。\n",
        "  作成方針は以下の流れ\n",
        "\n",
        "1. (形態素解析の結果の)ファイルを行ごとに読み込む\n",
        "2. 形態素解析の結果を元に辞書を作る(とりあえず全部)\n",
        "  - このとき `\"\\n\"` や `\"\"(空白文字)` を省く\n",
        "3. 上記の処理を節毎(`\"EOS\"`で区切る)にする。\n",
        "4. 節毎に辞書をリストにして追加するようにする\n"
      ],
      "metadata": {
        "id": "WdfbPA5qGBrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[0:30]:\n",
        "    if line != \"EOS\\n\":\n",
        "      # line = 初めて\t副詞,一般,*,*,*,*,初めて,ハジメテ,ハジメテ ←こんなかんじ\n",
        "      # [表層形  品詞  品詞細分類1  品詞細分類2  品詞細分類3  活用形  活用型  原形  読み  発音]\n",
        "      ## ここから [表層形(0番目), 基本形(?????多分原型？だとしたら7番目), 品詞(1番目), 品詞細分類1(2番目)] を取り出す\n",
        "      word = line.split(\"\\t\")\n",
        "      # word = ['吾輩', '名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\\n'] ←こんなかんじ\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        # 改行文字、空白文字を省く\n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      # 節末で溜めた形態素解析の結果を「辞書のリスト」として追加\n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "IPuDD6i_PLyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 31. 動詞\n",
        "動詞の表層形をすべて抽出せよ．\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ\n"
      ],
      "metadata": {
        "id": "whtu8E8XGBrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if (m[\"pos\"]==\"動詞\") : print(m[\"surface\"]) "
      ],
      "metadata": {
        "id": "IIAtIF5mGBrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32. 動詞の基本形\n",
        "動詞の基本形をすべて抽出せよ．\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ2\n",
        "  31番の `\"surface\"` を `\"base\"` にするだけなのでマジで5秒とかで解ける\n"
      ],
      "metadata": {
        "id": "1yHL2j5KGBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if (m[\"pos\"]==\"動詞\") : print(m[\"base\"]) "
      ],
      "metadata": {
        "id": "q6ub1Uf3GBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33. 「AのB」\n",
        "2つの名詞が「の」で連結されている名詞句を抽出せよ．\n",
        "\n",
        "\n",
        "> 30番のセンテンス辞書ができていれば後は走査するだけ3\n",
        "\n",
        "> …かと思ったけどそうでもなかった\n",
        "  とはいっても「の」の前後を取り出して名詞かどうか判定するだけ"
      ],
      "metadata": {
        "id": "F-dC7WJ7GBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def is_MeishiNoMeishi(sentence):\n",
        "  ans = []\n",
        "  for s in sentence:\n",
        "    for i in range(len(s)-1):\n",
        "      if s[i][\"surface\"]==\"の\":\n",
        "        if (s[i-1][\"pos\"]==\"名詞\") & (s[i+1][\"pos\"]==\"名詞\"):\n",
        "          ans.append(s[i-1][\"surface\"]+s[i][\"surface\"]+s[i+1][\"surface\"])\n",
        "  return ans\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = is_MeishiNoMeishi(sentence)\n",
        "print(len(ans))\n",
        "print(ans)\n"
      ],
      "metadata": {
        "id": "8OxWIj3GGBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 34. 名詞の連接\n",
        "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．\n",
        "\n",
        "> 最長一致…貪欲だな！(思考完)\n",
        "\n",
        "> …というわけにもいかんかった\n",
        "  連接なので1(その他→名詞→その他)のときは拾ってはいけないので面倒\n",
        "  でも普通に長さを保持する変数入れるだけで解決した…\n",
        "  あんまり汎用性の高い書き方じゃないから通用はしなさそうだけど…"
      ],
      "metadata": {
        "id": "gsh5EawtGBrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def Meishi_renzoku(sentence):\n",
        "  ans = []\n",
        "  for s in sentence:\n",
        "    tmp = \"\"\n",
        "    length = 0\n",
        "    for m in s:\n",
        "      if(m[\"pos\"]==\"名詞\"):\n",
        "        tmp += m[\"surface\"]\n",
        "        length += 1\n",
        "      else:\n",
        "        if(length>=2):\n",
        "          ans.append(tmp)\n",
        "          print(tmp)\n",
        "        tmp = \"\"\n",
        "        length = 0\n",
        "  return ans\n",
        "\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = Meishi_renzoku(sentence)\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "Dv2BboHrGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 35. 単語の出現頻度\n",
        "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．\n",
        "\n",
        "#### リストからDataFrameの作成\n",
        "参考 [ https://www.delftstack.com/ja/howto/python-pandas/pandas-create-dataframe-from-list/ ]\n",
        "\n",
        "リストからDataFrameの作成ってできるんだ…\n",
        "いやまあそりゃできるか…ファイルの読み込みで使う機会が多いというだけで…\n",
        "\n",
        "> 出現頻度の計算は以前のノック(19番)でやっているのでそれを活用\n",
        "  普通に単語でDict作ってカウントしてもいいけどなんとなくコッチのほうが良いかなって…\n",
        "  表層形と基本形どっちでカウントするのが良いのかわからなかったけど、解答例見る感じ基本形(Base)でやっていたのでそっちを採用。\n",
        "  >> 言語処理だとコッチが一般的？\n",
        "\n",
        "> 句点と読点とか、カギカッコは除外しても良かったかもしれんわね…"
      ],
      "metadata": {
        "id": "pdyOrgv8GBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    #print(m.values())\n",
        "    ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "print(df.value_counts(\"base\")[:20])"
      ],
      "metadata": {
        "id": "N-2DUVBTGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 36. 頻度上位10語\n",
        "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．\n",
        "\n",
        "> なんかグラフのときはmatplotlibかGNUplotを使ったほうが良いみたいなの言ってたな\n",
        "\n",
        "軽く調べたらGnuplotは元々外部のフリーソフトらしいので、今回はPythonで汎用的に使えそうなmatplotlibの方を採用。\n",
        "\n",
        "> …と思ったが、調べていたらpandasのDataFrameには\n",
        "  `DataFrame.plot`というド直球なメソッドが存在しているらしくてすごいなあと思いました(小並感)\n",
        "\n"
      ],
      "metadata": {
        "id": "K2Z0Gp0RGBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    #print(m.values())\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = (df.value_counts(\"base\"))[0:10]\n",
        "print(top_words)\n",
        "\n",
        "# グラフ化\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "top_words.plot.bar()\n",
        "\n",
        "\"\"\"\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(top_words[:10])\n",
        "pyplot.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nBZLXxluGBrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 37. 「猫」と共起頻度の高い上位10語\n",
        "「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．\n",
        "\n",
        "> 「共起頻度」is なに？？？？\n",
        "\n",
        "#### 共起頻度\n",
        "あんまり詳しい解説はなかったんだけど…\n",
        "\n",
        "    共起 : コロケーション (Co-location)\n",
        "    ある単語同士が同一文中に出現する組み合わせ\n",
        "\n",
        "つまり、「共起頻度はある単語の組み合わせで同一文中に登場する頻度の高い組み合わせ」\n",
        "というのがざっとした理解。\n",
        "\n",
        "> なので今回は文章中に「猫」という単語を含んでいるものを取り出し、\n",
        "  そこから単語1つ1つをカウントするだけで良さそう。\n",
        "  \n",
        ">> 本来は「組み合わせ」なので[A-B]という組み合わせで取り出すべきだが、\n",
        "  今回は「猫」という単語を指定されているので1つずつ見てカウントしていくだけで良い"
      ],
      "metadata": {
        "id": "TIBgB5lkGBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  #print([word[\"base\"] for word in s])\n",
        "  if not \"猫\" in [word[\"base\"] for word in s]: continue\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "df_CatDrop = df[df[\"base\"]!=\"猫\"]\n",
        "\n",
        "top_words = (df_CatDrop.value_counts(\"base\"))\n",
        "print(top_words[0:10])\n",
        "\n",
        "# グラフ化\n",
        "%matplotlib inline\n",
        "## 日本語を表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "top_words[0:10].plot.bar()\n",
        "\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HslXE1SRGBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 38. ヒストグラム\n",
        "単語の出現頻度のヒストグラムを描け．ただし，横軸は出現頻度を表し，1から単語の出現頻度の最大値までの線形目盛とする．縦軸はx軸で示される出現頻度となった単語の異なり数（種類数）である．\n",
        "\n",
        "\n",
        "> Pandasからヒストグラムの書き方わからなかったからForで全探索しようかと思ったけど調べたら普通に関数一発で描けるらしいです\n",
        "> - 【histogram入門】pandasとmatplotlibでヒストグラムを描いてみた [ https://qiita.com/MuAuan/items/849df1fffb0727f0dd09 ]\n",
        "\n",
        "を参考に `df.hist(bins=100<区間の個数>)`してplotするだけでした…"
      ],
      "metadata": {
        "id": "NvdNnDEsGBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = pd.DataFrame(df.value_counts(\"base\"))\n",
        "print(type(top_words))\n",
        "\n",
        "\"\"\"\n",
        "for w in top_words.index:\n",
        "  print(w)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QztgN5yzGBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words.hist(bins=100)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "kaCHpx6rYd-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 39. Zipfの法則\n",
        "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．\n",
        "\n",
        "- ジップの法則(Wikipedia)\n",
        "  - https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%83%E3%83%97%E3%81%AE%E6%B3%95%E5%89%87\n",
        "- Pandasのplotの全引数を解説 - Rosyuku\n",
        "  - https://own-search-and-study.xyz/2016/08/03/pandas%E3%81%AEplot%E3%81%AE%E5%85%A8%E5%BC%95%E6%95%B0%E3%82%92%E4%BD%BF%E3%81%84%E3%81%93%E3%81%AA%E3%81%99/\n",
        "\n",
        "\n",
        ">「出現頻度順位」と「出現頻度」の必要な値は揃っているのであとは対数グラフにプロットするだけ。\n",
        "  調べたら`df.plot()`の引数で`(loglog=True)`とすると両対数グラフになるらしい。\n",
        "  そして`df.plot?`でヘルプが表示される。便利だねぇ"
      ],
      "metadata": {
        "id": "3uTbIWTWGBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"neko.txt.mecab\"\n",
        "pick_morpheme = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(pick_morpheme[0]) : word[0], # 表層形\n",
        "    str(pick_morpheme[1]) : morpheme[6], # 基本形\n",
        "    str(pick_morpheme[2]) : morpheme[0], # 品詞\n",
        "    str(pick_morpheme[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "sentence = []\n",
        "morphemes_dict = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        morphemes_dict.append(make_mapping(word))\n",
        "    else: \n",
        "      sentence.append(morphemes_dict)\n",
        "      morphemes_dict = []\n",
        "\n",
        "ans = []\n",
        "for s in sentence:\n",
        "  for m in s:\n",
        "    if m[\"pos\"]!=\"記号\" : ans.append(m.values())\n",
        "\n",
        "# 出現頻度の計算\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(ans, columns=[\"surface\", \"base\", \"pos\", \"pos1\"])\n",
        "top_words = pd.DataFrame(df.value_counts(\"base\"))\n",
        "#print(top_words)\n",
        "\n",
        "\n",
        "top_words.plot(loglog=True)"
      ],
      "metadata": {
        "id": "3k762OO3GBrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第5章: 係り受け解析\n",
        "\n",
        "日本語Wikipediaの「人工知能」に関する記事からテキスト部分を抜き出したファイルがai.ja.zipに収録されている． この文章をCaboChaやKNP等のツールを利用して係り受け解析を行い，その結果をai.ja.txt.parsedというファイルに保存せよ．このファイルを読み込み，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "- CaboCha/南瓜: Yet Another Japanese Dependency Structure Analyzer\n",
        "  - 公式ページ https://taku910.github.io/cabocha/\n",
        "  - 奈良先端技術大学院の工藤拓氏による日本語係り受け解析器"
      ],
      "metadata": {
        "id": "triCPO2JGCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 事前準備\n",
        "\n",
        "例によってまたデータがいるので拾って読み込んでくるまでをやる。\n",
        "必要に応じて適宜ライブラリの追加などもココでやる。\n",
        "\n",
        "↓の実行ボタンを押したら事前準備が完了するようになってる。はず。\n"
      ],
      "metadata": {
        "id": "7mrrvSw6Sjku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### やってること\n",
        "- `ai.ja.zip`の保存と解凍(データの準備)\n",
        "- ライブラリの導入\n",
        "  - MeCab (+MeCab-Python3)\n",
        "  - CaboCha \n",
        "    - 配布はGoogle Driveから [ https://drive.google.com/drive/folders/0B4y35FiV1wh7cGRCUUJHVTNJRnM ]\n",
        "    - 2022-03-23時点で最新はcabocha-0.69なのでそれを使用します\n",
        "  - CRF++ (CaboChaの依存パッケージとして必要)\n",
        "    - 配布はGoogle Driveから [ https://drive.google.com/drive/folders/0B4y35FiV1wh7fngteFhHQUN2Y1B5eUJBNHZUemJYQV9VWlBUb3JlX0xBdWVZTWtSbVBneU0 ]\n",
        "    - 2022-03-23時点で最新はCRF++-0.58なのでそれを使用します。(最終更新が2015年なので多分ずっとコレだと思うけど…)\n",
        "- グラフ描画用\n",
        "  - matplotlib の読み込み\n",
        "  - japanize_matplotlib の読み込み (日本語でグラフ作成)\n",
        "  - `%matplotlib inline` の実行(グラフをインラインで描画するように)\n",
        "- 係り受け解析\n",
        "  - 係り受け解析結果を`ai.ja.txt.parsed`として出力\n",
        "\n",
        "\n",
        "#### [参考]\n",
        "- Google Colab で MeCab と CaboCha を使う最強の方法 by @tomowarkar\n",
        "  - Qiita https://qiita.com/tomowarkar/items/b6a89145c06956618542\n",
        "\n",
        "- Google Colab で MeCab と CaboCha を使う。 by tomowarkar\n",
        "  - github Blog https://tomowarkar.github.io/blog/posts/colab_mecab/\n",
        "\n",
        "> ……で頑張ってたけどbashだとどうしてもエラーが出てうまくいかない！！\n",
        "\n",
        "- Colabratory に Cabocha をインストールする by @iimuz\n",
        "  - Qiita https://qiita.com/iimuz/items/30a7e02772ffd3445f3b \n",
        "- ▲心くじけず言語処理100本ノック＝＝5章下準備＝＝ by tbtech\n",
        "  - はてなブログ https://ds-blog.tbtech.co.jp/entry/2020/06/08/%E2%96%B2%E5%BF%83%E3%81%8F%E3%81%98%E3%81%91%E3%81%9A%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%EF%BC%9D%EF%BC%9D5%E7%AB%A0%E4%B8%8B%E6%BA%96%E5%82%99%EF%BC%9D%EF%BC%9D\n",
        "\n",
        "> MeCabは謎の記法してるけど、CRF++とCaboChaをPythonベースで導入してて此方の記事のおかげで無事に使えました…感謝🙏\n",
        "  ていうか環境構築周りだけはマジで言語処理ノック側でもサポートしてくれって思う、そんなとこで躓くのは本来の課題じゃないだろうに\n",
        "\n",
        "- curlやwgetで公開済みGoogle Driveデータをダウンロードする by @namakemono\n",
        "  - Qiita https://qiita.com/namakemono/items/c963e75e0af3f7eed732\n",
        "\n",
        "> Google Driveで大きいサイズのときに出るウイルスチェック出来ませんみたいな警告のせいであのクソ長いwget文になるみたい。\n",
        "  そのままDLすると `cabocha.tar.bz2 is not a bzip2 file` と表示されて!tarや!bzipで解凍できない。困る。\n",
        "  ていうかなんで他の人は普通にwgetで通るの？？\n",
        "\n",
        "結果として\n",
        "- `cabocha.tar.bz2`のDLまでは心くじけず～の部分、\n",
        "- tarの解凍～make～Pythonライブラリのインストールをtomowarkar氏のものでやったらうまくいきました。"
      ],
      "metadata": {
        "id": "tcDLyeh9XOu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なデータの準備\n",
        "!wget https://nlp100.github.io/data/ai.ja.zip -O ai.ja.zip\n",
        "!unzip -o ai.ja.zip\n",
        "!pwd\n",
        "!ls\n",
        "!head -10 ai.ja.txt"
      ],
      "metadata": {
        "id": "w7A8mPqDUfPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3633dbf8-c696-4be4-fbe3-dc540b8c9ee1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-04 05:50:57--  https://nlp100.github.io/data/ai.ja.zip\n",
            "Resolving nlp100.github.io (nlp100.github.io)... 185.199.108.153, 185.199.111.153, 185.199.109.153, ...\n",
            "Connecting to nlp100.github.io (nlp100.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17516 (17K) [application/zip]\n",
            "Saving to: ‘ai.ja.zip’\n",
            "\n",
            "ai.ja.zip           100%[===================>]  17.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-04 05:50:57 (46.7 MB/s) - ‘ai.ja.zip’ saved [17516/17516]\n",
            "\n",
            "Archive:  ai.ja.zip\n",
            "  inflating: ai.ja.txt               \n",
            "  inflating: readme.ai.ja.md         \n",
            "/content\n",
            "ai.ja.txt  ai.ja.zip  readme.ai.ja.md  sample_data\n",
            "人工知能\n",
            "\n",
            "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\n",
            "\n",
            "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\n",
            "人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある。1956年にダートマス会議でジョン・マッカーシーにより命名された。現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。\n",
            "\n",
            "プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている。人工的な知能の実現へのアプローチとしては、「ファジィ理論」や「ニューラルネットワーク」などのようなアプローチも知られているが、従来の人工知能である (Good Old Fashioned AI) との差は記述の記号的明示性にある。その後「サポートベクターマシン」が注目を集めた。また、自らの経験を元に学習を行う強化学習という手法もある。「この宇宙において、知性とは最も強力な形質である（レイ・カーツワイル）」という言葉通り、知性を機械的に表現し実装するということは極めて重要な作業である。\n",
            "\n",
            "2006年のディープラーニング（深層学習）の登場と2010年代以降のビッグデータの登場により、一過性の流行を超えて社会に浸透して行った。2016年から2017年にかけて、ディープラーニングを導入したAIが完全情報ゲームである囲碁などのトップ棋士、さらに不完全情報ゲームであるポーカーの世界トップクラスのプレイヤーも破り、麻雀では「Microsoft Suphx (Super Phoenix)」がAIとして初めて十段に到達するなど、時代の最先端技術となった。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリの導入\n",
        "!apt install aptitude\n",
        "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.7\n"
      ],
      "metadata": {
        "id": "kYJESPXxYs1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2226bac-a891-4b58-c6b3-ba48e45ae612"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 2s (2,163 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-jumandic\n",
            "  mecab-jumandic-utf8 mecab-utils\n",
            "0 upgraded, 8 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 29.0 MB of archives.\n",
            "After this operation, 277 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.0 MB in 2s (17.7 MB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 156669 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../3-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../4-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../5-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../6-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../7-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3==0.7\n",
            "  Downloading mecab-python3-0.7.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 443 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp37-cp37m-linux_x86_64.whl size=156603 sha256=91742e75040a6bd2e27729cf6fdeddea3d2c3b20cb853d91d2a76e3c2d1f4836\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/46/95/3748ec2c4936cb69ee4d248a85e862064ea1e84819344c5292\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CRF++のインストール\n",
        "import os\n",
        "filename_crfpp = 'crfpp.tar.gz'\n",
        "!wget \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "%cd CRF++-0.58\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "%cd ../\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib' "
      ],
      "metadata": {
        "id": "M9meLf7q1__t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7b4b4e-6a1f-4429-e1eb-5e44a6eea149"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-04 05:51:44--  https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.141.113, 142.250.141.100, 142.250.141.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.141.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bqkn974d03t7b4s90p7eiqjl5eg1ifmm/1649051475000/13553212398903315502/*/0B4y35FiV1wh7QVR6VXJ5dWExSTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-04 05:51:49--  https://doc-08-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bqkn974d03t7b4s90p7eiqjl5eg1ifmm/1649051475000/13553212398903315502/*/0B4y35FiV1wh7QVR6VXJ5dWExSTQ?e=download\n",
            "Resolving doc-08-74-docs.googleusercontent.com (doc-08-74-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-08-74-docs.googleusercontent.com (doc-08-74-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 790570 (772K) [application/x-gzip]\n",
            "Saving to: ‘crfpp.tar.gz’\n",
            "\n",
            "crfpp.tar.gz        100%[===================>] 772.04K  4.52MB/s    in 0.2s    \n",
            "\n",
            "2022-04-04 05:51:50 (4.52 MB/s) - ‘crfpp.tar.gz’ saved [790570/790570]\n",
            "\n",
            "CRF++-0.58/\n",
            "CRF++-0.58/INSTALL\n",
            "CRF++-0.58/python/\n",
            "CRF++-0.58/python/test.py\n",
            "CRF++-0.58/python/README\n",
            "CRF++-0.58/python/CRFPP.py\n",
            "CRF++-0.58/python/setup.py\n",
            "CRF++-0.58/python/CRFPP_wrap.cxx\n",
            "CRF++-0.58/Makefile.in\n",
            "CRF++-0.58/encoder.cpp\n",
            "CRF++-0.58/node.h\n",
            "CRF++-0.58/param.cpp\n",
            "CRF++-0.58/swig/\n",
            "CRF++-0.58/swig/CRFPP_wrap.c\n",
            "CRF++-0.58/swig/version.h\n",
            "CRF++-0.58/swig/version.h.in\n",
            "CRF++-0.58/swig/Makefile\n",
            "CRF++-0.58/swig/CRFPP.i\n",
            "CRF++-0.58/config.h.in\n",
            "CRF++-0.58/feature_cache.cpp\n",
            "CRF++-0.58/config.guess\n",
            "CRF++-0.58/scoped_ptr.h\n",
            "CRF++-0.58/node.cpp\n",
            "CRF++-0.58/README\n",
            "CRF++-0.58/timer.h\n",
            "CRF++-0.58/feature_index.h\n",
            "CRF++-0.58/config.sub\n",
            "CRF++-0.58/ltmain.sh\n",
            "CRF++-0.58/common.h\n",
            "CRF++-0.58/configure\n",
            "CRF++-0.58/crf_learn.cpp\n",
            "CRF++-0.58/darts.h\n",
            "CRF++-0.58/winmain.h\n",
            "CRF++-0.58/doc/\n",
            "CRF++-0.58/doc/html/\n",
            "CRF++-0.58/doc/html/search/\n",
            "CRF++-0.58/doc/html/search/nomatches.html\n",
            "CRF++-0.58/doc/html/search/search_r.png\n",
            "CRF++-0.58/doc/html/search/search_l.png\n",
            "CRF++-0.58/doc/html/search/mag_sel.png\n",
            "CRF++-0.58/doc/html/search/search.css\n",
            "CRF++-0.58/doc/html/search/search_m.png\n",
            "CRF++-0.58/doc/html/search/close.png\n",
            "CRF++-0.58/doc/html/search/search.js\n",
            "CRF++-0.58/doc/html/nav_f.png\n",
            "CRF++-0.58/doc/html/crfpp_8h_source.html\n",
            "CRF++-0.58/doc/html/jquery.js\n",
            "CRF++-0.58/doc/html/nav_h.png\n",
            "CRF++-0.58/doc/html/bc_s.png\n",
            "CRF++-0.58/doc/html/index.html\n",
            "CRF++-0.58/doc/html/closed.png\n",
            "CRF++-0.58/doc/html/tab_h.png\n",
            "CRF++-0.58/doc/html/tab_a.png\n",
            "CRF++-0.58/doc/html/tab_b.png\n",
            "CRF++-0.58/doc/html/installdox\n",
            "CRF++-0.58/doc/html/doxygen.css\n",
            "CRF++-0.58/doc/html/open.png\n",
            "CRF++-0.58/doc/html/tab_s.png\n",
            "CRF++-0.58/doc/html/files.html\n",
            "CRF++-0.58/doc/html/doxygen.png\n",
            "CRF++-0.58/doc/html/tabs.css\n",
            "CRF++-0.58/doc/latex/\n",
            "CRF++-0.58/doc/latex/refman.tex\n",
            "CRF++-0.58/doc/latex/doxygen.sty\n",
            "CRF++-0.58/doc/latex/Makefile\n",
            "CRF++-0.58/doc/index.html\n",
            "CRF++-0.58/doc/default.css\n",
            "CRF++-0.58/doc/doxygen/\n",
            "CRF++-0.58/doc/doxygen/tab_l.gif\n",
            "CRF++-0.58/doc/doxygen/namespacemembers.html\n",
            "CRF++-0.58/doc/doxygen/nav_f.png\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h_source.html\n",
            "CRF++-0.58/doc/doxygen/namespaces.html\n",
            "CRF++-0.58/doc/doxygen/nav_h.png\n",
            "CRF++-0.58/doc/doxygen/namespaceCRFPP.html\n",
            "CRF++-0.58/doc/doxygen/globals.html\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h-source.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Tagger-members.html\n",
            "CRF++-0.58/doc/doxygen/tab_b.gif\n",
            "CRF++-0.58/doc/doxygen/functions.html\n",
            "CRF++-0.58/doc/doxygen/tab_r.gif\n",
            "CRF++-0.58/doc/doxygen/bc_s.png\n",
            "CRF++-0.58/doc/doxygen/namespacemembers_func.html\n",
            "CRF++-0.58/doc/doxygen/index.html\n",
            "CRF++-0.58/doc/doxygen/closed.png\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Model.html\n",
            "CRF++-0.58/doc/doxygen/tab_h.png\n",
            "CRF++-0.58/doc/doxygen/functions_func.html\n",
            "CRF++-0.58/doc/doxygen/tab_a.png\n",
            "CRF++-0.58/doc/doxygen/globals_defs.html\n",
            "CRF++-0.58/doc/doxygen/classes.html\n",
            "CRF++-0.58/doc/doxygen/tab_b.png\n",
            "CRF++-0.58/doc/doxygen/globals_type.html\n",
            "CRF++-0.58/doc/doxygen/doxygen.css\n",
            "CRF++-0.58/doc/doxygen/open.png\n",
            "CRF++-0.58/doc/doxygen/tab_s.png\n",
            "CRF++-0.58/doc/doxygen/globals_func.html\n",
            "CRF++-0.58/doc/doxygen/crfpp_8h.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Model-members.html\n",
            "CRF++-0.58/doc/doxygen/classCRFPP_1_1Tagger.html\n",
            "CRF++-0.58/doc/doxygen/files.html\n",
            "CRF++-0.58/doc/doxygen/doxygen.png\n",
            "CRF++-0.58/doc/doxygen/tabs.css\n",
            "CRF++-0.58/doc/doxygen/annotated.html\n",
            "CRF++-0.58/doc/crfpp.cfg\n",
            "CRF++-0.58/crf_test.cpp\n",
            "CRF++-0.58/mmap.h\n",
            "CRF++-0.58/Makefile.msvc.in\n",
            "CRF++-0.58/ChangeLog\n",
            "CRF++-0.58/feature_index.cpp\n",
            "CRF++-0.58/COPYING\n",
            "CRF++-0.58/libcrfpp.cpp\n",
            "CRF++-0.58/NEWS\n",
            "CRF++-0.58/mkinstalldirs\n",
            "CRF++-0.58/freelist.h\n",
            "CRF++-0.58/AUTHORS\n",
            "CRF++-0.58/merge-models.pl\n",
            "CRF++-0.58/java/\n",
            "CRF++-0.58/java/test.java\n",
            "CRF++-0.58/java/README\n",
            "CRF++-0.58/java/.am\n",
            "CRF++-0.58/java/Makefile\n",
            "CRF++-0.58/java/org/\n",
            "CRF++-0.58/java/org/chasen/\n",
            "CRF++-0.58/java/org/chasen/crfpp/\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_float.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPPJNI.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_p_char.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPP.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/SWIGTYPE_p_int.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/CRFPPConstants.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/Model.java\n",
            "CRF++-0.58/java/org/chasen/crfpp/Tagger.java\n",
            "CRF++-0.58/java/CRFPP_wrap.cxx\n",
            "CRF++-0.58/encoder.h\n",
            "CRF++-0.58/crfpp.h\n",
            "CRF++-0.58/perl/\n",
            "CRF++-0.58/perl/test.pl\n",
            "CRF++-0.58/perl/README\n",
            "CRF++-0.58/perl/Makefile.PL\n",
            "CRF++-0.58/perl/CRFPP_wrap.cxx\n",
            "CRF++-0.58/perl/CRFPP.pm\n",
            "CRF++-0.58/aclocal.m4\n",
            "CRF++-0.58/lbfgs.cpp\n",
            "CRF++-0.58/lbfgs.h\n",
            "CRF++-0.58/install-sh\n",
            "CRF++-0.58/tagger.cpp\n",
            "CRF++-0.58/param.h\n",
            "CRF++-0.58/missing\n",
            "CRF++-0.58/feature_cache.h\n",
            "CRF++-0.58/feature.cpp\n",
            "CRF++-0.58/depcomp\n",
            "CRF++-0.58/stream_wrapper.h\n",
            "CRF++-0.58/sdk/\n",
            "CRF++-0.58/sdk/example.cpp\n",
            "CRF++-0.58/ruby/\n",
            "CRF++-0.58/ruby/extconf.rb\n",
            "CRF++-0.58/ruby/CRFPP_wrap.cpp\n",
            "CRF++-0.58/ruby/README\n",
            "CRF++-0.58/ruby/test.rb\n",
            "CRF++-0.58/configure.in\n",
            "CRF++-0.58/path.cpp\n",
            "CRF++-0.58/path.h\n",
            "CRF++-0.58/tagger.h\n",
            "CRF++-0.58/thread.h\n",
            "CRF++-0.58/example/\n",
            "CRF++-0.58/example/JapaneseNE/\n",
            "CRF++-0.58/example/JapaneseNE/train.data\n",
            "CRF++-0.58/example/JapaneseNE/test.data\n",
            "CRF++-0.58/example/JapaneseNE/exec.sh\n",
            "CRF++-0.58/example/JapaneseNE/template\n",
            "CRF++-0.58/example/chunking/\n",
            "CRF++-0.58/example/chunking/train.data\n",
            "CRF++-0.58/example/chunking/test.data\n",
            "CRF++-0.58/example/chunking/exec.sh\n",
            "CRF++-0.58/example/chunking/template\n",
            "CRF++-0.58/example/seg/\n",
            "CRF++-0.58/example/seg/train.data\n",
            "CRF++-0.58/example/seg/test.data\n",
            "CRF++-0.58/example/seg/exec.sh\n",
            "CRF++-0.58/example/seg/template\n",
            "CRF++-0.58/example/basenp/\n",
            "CRF++-0.58/example/basenp/train.data\n",
            "CRF++-0.58/example/basenp/test.data\n",
            "CRF++-0.58/example/basenp/exec.sh\n",
            "CRF++-0.58/example/basenp/template\n",
            "CRF++-0.58/Makefile.am\n",
            "/content/CRF++-0.58\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... gcc3\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... gcc3\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking for library containing strerror... none required\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... no\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7077: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... no\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for ANSI C header files... (cached) no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking math.h usability... yes\n",
            "checking math.h presence... yes\n",
            "checking for math.h... yes\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for size_t... yes\n",
            "checking for pow in -lm... yes\n",
            "checking for exp in -lm... yes\n",
            "checking for log in -lm... yes\n",
            "checking for pthread_create in -lpthread... yes\n",
            "checking for pthread_join in -lpthread... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking whether make is GNU Make... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports stl <sstream> (required)... yes\n",
            "checking if g++ supports stl <stdexcept> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports dynamic_cast<> (required)... yes\n",
            "checking if g++ supports exception handler (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ supports _SC_NPROCESSORS_CONF (optional)... yes\n",
            "checking if g++ environment provides all required features... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating Makefile.msvc\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "make  all-am\n",
            "make[1]: Entering directory '/content/CRF++-0.58'\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o libcrfpp.lo libcrfpp.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c libcrfpp.cpp  -fPIC -DPIC -o .libs/libcrfpp.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c libcrfpp.cpp -o libcrfpp.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o lbfgs.lo lbfgs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c lbfgs.cpp  -fPIC -DPIC -o .libs/lbfgs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c lbfgs.cpp -o lbfgs.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o encoder.lo encoder.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c encoder.cpp  -fPIC -DPIC -o .libs/encoder.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c encoder.cpp -o encoder.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature.lo feature.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature.cpp  -fPIC -DPIC -o .libs/feature.o\n",
            "In file included from \u001b[01m\u001b[Ktagger.h:14:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfeature.cpp:12\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:34:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget CRFPP::{anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature.cpp -o feature.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature_cache.lo feature_cache.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_cache.cpp  -fPIC -DPIC -o .libs/feature_cache.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_cache.cpp -o feature_cache.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o feature_index.lo feature_index.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_index.cpp  -fPIC -DPIC -o .libs/feature_index.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c feature_index.cpp -o feature_index.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o node.lo node.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c node.cpp  -fPIC -DPIC -o .libs/node.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c node.cpp -o node.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o path.lo path.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c path.cpp  -fPIC -DPIC -o .libs/path.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c path.cpp -o path.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o tagger.lo tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c tagger.cpp  -fPIC -DPIC -o .libs/tagger.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -O3 -Wall -c tagger.cpp -o tagger.o >/dev/null 2>&1\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o libcrfpp.la -rpath /usr/local/lib libcrfpp.lo lbfgs.lo param.lo encoder.lo feature.lo feature_cache.lo feature_index.lo node.lo path.lo tagger.lo  -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/libcrfpp.o .libs/lbfgs.o .libs/param.o .libs/encoder.o .libs/feature.o .libs/feature_cache.o .libs/feature_index.o .libs/node.o .libs/path.o .libs/tagger.o   -lpthread -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libcrfpp.so.0 -o .libs/libcrfpp.so.0.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcrfpp.so.0\" && ln -s \"libcrfpp.so.0.0.0\" \"libcrfpp.so.0\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcrfpp.so\" && ln -s \"libcrfpp.so.0.0.0\" \"libcrfpp.so\")\n",
            "libtool: link: ar cru .libs/libcrfpp.a  libcrfpp.o lbfgs.o param.o encoder.o feature.o feature_cache.o feature_index.o node.o path.o tagger.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libcrfpp.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libcrfpp.la\" && ln -s \"../libcrfpp.la\" \"libcrfpp.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o crf_learn.o crf_learn.cpp\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o crf_learn crf_learn.o libcrfpp.la -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++ -O3 -Wall -o .libs/crf_learn crf_learn.o  ./.libs/libcrfpp.so -lpthread -lm\n",
            "g++ -DHAVE_CONFIG_H -I.     -O3 -Wall -c -o crf_test.o crf_test.cpp\n",
            "/bin/bash ./libtool --tag=CXX   --mode=link g++  -O3 -Wall   -o crf_test crf_test.o libcrfpp.la  -lpthread -lpthread -lm -lm -lm \n",
            "libtool: link: g++ -O3 -Wall -o .libs/crf_test crf_test.o  ./.libs/libcrfpp.so -lpthread -lm\n",
            "make[1]: Leaving directory '/content/CRF++-0.58'\n",
            "make[1]: Entering directory '/content/CRF++-0.58'\n",
            "test -z \"/usr/local/lib\" || /bin/mkdir -p \"/usr/local/lib\"\n",
            " /bin/bash ./libtool   --mode=install /usr/bin/install -c   libcrfpp.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.so.0.0.0 /usr/local/lib/libcrfpp.so.0.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcrfpp.so.0.0.0 libcrfpp.so.0 || { rm -f libcrfpp.so.0 && ln -s libcrfpp.so.0.0.0 libcrfpp.so.0; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcrfpp.so.0.0.0 libcrfpp.so || { rm -f libcrfpp.so && ln -s libcrfpp.so.0.0.0 libcrfpp.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.lai /usr/local/lib/libcrfpp.la\n",
            "libtool: install: /usr/bin/install -c .libs/libcrfpp.a /usr/local/lib/libcrfpp.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libcrfpp.a\n",
            "libtool: install: ranlib /usr/local/lib/libcrfpp.a\n",
            "libtool: finish: PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            "  /bin/bash ./libtool   --mode=install /usr/bin/install -c crf_learn crf_test '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/crf_learn /usr/local/bin/crf_learn\n",
            "libtool: install: /usr/bin/install -c .libs/crf_test /usr/local/bin/crf_test\n",
            "test -z \"/usr/local/include\" || /bin/mkdir -p \"/usr/local/include\"\n",
            " /usr/bin/install -c -m 644 crfpp.h '/usr/local/include'\n",
            "make[1]: Leaving directory '/content/CRF++-0.58'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CaboChaのインストール\n",
        "## > @yamaru氏の言語処理100本ノック第5章記事より引用\n",
        "FILE_ID = \"0B4y35FiV1wh7SDd1Q1dUQkZQaUU\"\n",
        "FILE_NAME = \"cabocha.tar.bz2\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt\n",
        "!tar -xvf cabocha.tar.bz2\n",
        "%cd cabocha-0.69\n",
        "\n",
        "!./configure -with-charset=utf-8 && make && make check && make install && ldconfig\n",
        "\n",
        "%cd ~/../content"
      ],
      "metadata": {
        "id": "Kx7FR3SzOZZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9af41ac-e65c-4c27-84d5-a1ce965589f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-04 05:52:32--  https://docs.google.com/uc?export=download&confirm=t&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.102, 142.250.141.113, 142.250.141.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ufia5l5nfq06culc7r2atrh164rgmsgb/1649051550000/13553212398903315502/*/0B4y35FiV1wh7SDd1Q1dUQkZQaUU?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-04 05:52:32--  https://doc-04-74-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ufia5l5nfq06culc7r2atrh164rgmsgb/1649051550000/13553212398903315502/*/0B4y35FiV1wh7SDd1Q1dUQkZQaUU?e=download\n",
            "Resolving doc-04-74-docs.googleusercontent.com (doc-04-74-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-04-74-docs.googleusercontent.com (doc-04-74-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84638995 (81M) [application/x-bzip2]\n",
            "Saving to: ‘cabocha.tar.bz2’\n",
            "\n",
            "cabocha.tar.bz2     100%[===================>]  80.72M  46.7MB/s    in 1.7s    \n",
            "\n",
            "2022-04-04 05:52:35 (46.7 MB/s) - ‘cabocha.tar.bz2’ saved [84638995/84638995]\n",
            "\n",
            "cabocha-0.69/\n",
            "cabocha-0.69/cabocha-config.in\n",
            "cabocha-0.69/compile\n",
            "cabocha-0.69/swig/\n",
            "cabocha-0.69/swig/version.h.in\n",
            "cabocha-0.69/swig/Makefile\n",
            "cabocha-0.69/swig/version.h\n",
            "cabocha-0.69/swig/CaboCha.i\n",
            "cabocha-0.69/missing\n",
            "cabocha-0.69/java/\n",
            "cabocha-0.69/java/test.java\n",
            "cabocha-0.69/java/Makefile\n",
            "cabocha-0.69/java/org/\n",
            "cabocha-0.69/java/org/chasen/\n",
            "cabocha-0.69/java/org/chasen/cabocha/\n",
            "cabocha-0.69/java/org/chasen/cabocha/FormatType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/OutputLayerType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Token.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboChaConstants.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/ParserType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/ParsingAlgorithm.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Chunk.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/InputLayerType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboCha.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CaboChaJNI.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/PossetType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Tree.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/CharsetType.java\n",
            "cabocha-0.69/java/org/chasen/cabocha/Parser.java\n",
            "cabocha-0.69/java/CaboCha_wrap.cxx\n",
            "cabocha-0.69/ltmain.sh\n",
            "cabocha-0.69/config.guess\n",
            "cabocha-0.69/man/\n",
            "cabocha-0.69/man/Makefile.in\n",
            "cabocha-0.69/man/cabocha.1\n",
            "cabocha-0.69/man/Makefile.am\n",
            "cabocha-0.69/BSD\n",
            "cabocha-0.69/python/\n",
            "cabocha-0.69/python/test.py\n",
            "cabocha-0.69/python/CaboCha.py\n",
            "cabocha-0.69/python/CaboCha_wrap.cxx\n",
            "cabocha-0.69/python/setup.py\n",
            "cabocha-0.69/AUTHORS\n",
            "cabocha-0.69/ruby/\n",
            "cabocha-0.69/ruby/CaboCha_wrap.cpp\n",
            "cabocha-0.69/ruby/extconf.rb\n",
            "cabocha-0.69/ruby/test.rb\n",
            "cabocha-0.69/Makefile.in\n",
            "cabocha-0.69/NEWS\n",
            "cabocha-0.69/install-sh\n",
            "cabocha-0.69/cabocha.iss.in\n",
            "cabocha-0.69/ChangeLog\n",
            "cabocha-0.69/configure\n",
            "cabocha-0.69/src/\n",
            "cabocha-0.69/src/string_buffer.cpp\n",
            "cabocha-0.69/src/tree_allocator.cpp\n",
            "cabocha-0.69/src/dep.h\n",
            "cabocha-0.69/src/dep_learner.cpp\n",
            "cabocha-0.69/src/tree_allocator.h\n",
            "cabocha-0.69/src/svm.h\n",
            "cabocha-0.69/src/svm.cpp\n",
            "cabocha-0.69/src/ucstable.h\n",
            "cabocha-0.69/src/utils.h\n",
            "cabocha-0.69/src/selector.cpp\n",
            "cabocha-0.69/src/chunk_learner.cpp\n",
            "cabocha-0.69/src/string_buffer.h\n",
            "cabocha-0.69/src/ucs.cpp\n",
            "cabocha-0.69/src/ne.cpp\n",
            "cabocha-0.69/src/eval.cpp\n",
            "cabocha-0.69/src/cabocha.cpp\n",
            "cabocha-0.69/src/Makefile.in\n",
            "cabocha-0.69/src/scoped_ptr.h\n",
            "cabocha-0.69/src/chunker.h\n",
            "cabocha-0.69/src/normalizer.rule\n",
            "cabocha-0.69/src/common.h\n",
            "cabocha-0.69/src/normalizer_rule.sh\n",
            "cabocha-0.69/src/darts.h\n",
            "cabocha-0.69/src/learner.cpp\n",
            "cabocha-0.69/src/cabocha.h\n",
            "cabocha-0.69/src/morph.h\n",
            "cabocha-0.69/src/svm_learn.cpp\n",
            "cabocha-0.69/src/Makefile.msvc.in\n",
            "cabocha-0.69/src/timer.h\n",
            "cabocha-0.69/src/chunker.cpp\n",
            "cabocha-0.69/src/utils.cpp\n",
            "cabocha-0.69/src/param.h\n",
            "cabocha-0.69/src/winmain.h\n",
            "cabocha-0.69/src/normalizer.h\n",
            "cabocha-0.69/src/param.cpp\n",
            "cabocha-0.69/src/parser.cpp\n",
            "cabocha-0.69/src/ne.h\n",
            "cabocha-0.69/src/normalizer_rule.h\n",
            "cabocha-0.69/src/svm_learn.h\n",
            "cabocha-0.69/src/ucs.h\n",
            "cabocha-0.69/src/cabocha-model-index.cpp\n",
            "cabocha-0.69/src/mmap.h\n",
            "cabocha-0.69/src/analyzer.h\n",
            "cabocha-0.69/src/make.bat\n",
            "cabocha-0.69/src/tree.cpp\n",
            "cabocha-0.69/src/char_category.h\n",
            "cabocha-0.69/src/Makefile.am\n",
            "cabocha-0.69/src/dep.cpp\n",
            "cabocha-0.69/src/morph.cpp\n",
            "cabocha-0.69/src/selector_pat.h\n",
            "cabocha-0.69/src/cabocha-system-eval.cpp\n",
            "cabocha-0.69/src/cabocha-learn.cpp\n",
            "cabocha-0.69/src/stream_wrapper.h\n",
            "cabocha-0.69/src/selector.h\n",
            "cabocha-0.69/src/libcabocha.cpp\n",
            "cabocha-0.69/src/normalizer.cpp\n",
            "cabocha-0.69/src/freelist.h\n",
            "cabocha-0.69/perl/\n",
            "cabocha-0.69/perl/test.pl\n",
            "cabocha-0.69/perl/Makefile.PL\n",
            "cabocha-0.69/perl/CaboCha_wrap.o\n",
            "cabocha-0.69/perl/CaboCha.bs\n",
            "cabocha-0.69/perl/blib/\n",
            "cabocha-0.69/perl/blib/bin/\n",
            "cabocha-0.69/perl/blib/bin/.exists\n",
            "cabocha-0.69/perl/blib/arch/\n",
            "cabocha-0.69/perl/blib/arch/.exists\n",
            "cabocha-0.69/perl/blib/arch/auto/\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/.exists\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/CaboCha.so\n",
            "cabocha-0.69/perl/blib/arch/auto/CaboCha/CaboCha.bs\n",
            "cabocha-0.69/perl/blib/lib/\n",
            "cabocha-0.69/perl/blib/lib/.exists\n",
            "cabocha-0.69/perl/blib/lib/auto/\n",
            "cabocha-0.69/perl/blib/lib/auto/CaboCha/\n",
            "cabocha-0.69/perl/blib/lib/auto/CaboCha/.exists\n",
            "cabocha-0.69/perl/blib/lib/CaboCha.pm\n",
            "cabocha-0.69/perl/blib/man1/\n",
            "cabocha-0.69/perl/blib/man1/.exists\n",
            "cabocha-0.69/perl/blib/script/\n",
            "cabocha-0.69/perl/blib/script/.exists\n",
            "cabocha-0.69/perl/blib/man3/\n",
            "cabocha-0.69/perl/blib/man3/.exists\n",
            "cabocha-0.69/perl/CaboCha_wrap.cxx\n",
            "cabocha-0.69/perl/pm_to_blib\n",
            "cabocha-0.69/perl/CaboCha.pm\n",
            "cabocha-0.69/perl/MYMETA.yml\n",
            "cabocha-0.69/config.rpath\n",
            "cabocha-0.69/TODO\n",
            "cabocha-0.69/configure.in\n",
            "cabocha-0.69/config.sub\n",
            "cabocha-0.69/LGPL\n",
            "cabocha-0.69/tools/\n",
            "cabocha-0.69/tools/kc2cabocha.pl\n",
            "cabocha-0.69/tools/irex2cabocha.pl\n",
            "cabocha-0.69/tools/chasen2mecab.pl\n",
            "cabocha-0.69/tools/kc2juman.pl\n",
            "cabocha-0.69/tools/KyotoCorpus.pm\n",
            "cabocha-0.69/tools/KNBC2KC.pl\n",
            "cabocha-0.69/cabocharc.in\n",
            "cabocha-0.69/INSTALL\n",
            "cabocha-0.69/aclocal.m4\n",
            "cabocha-0.69/README\n",
            "cabocha-0.69/config.h.in\n",
            "cabocha-0.69/COPYING\n",
            "cabocha-0.69/example/\n",
            "cabocha-0.69/example/example2.cpp\n",
            "cabocha-0.69/example/example.c\n",
            "cabocha-0.69/Makefile.am\n",
            "cabocha-0.69/model/\n",
            "cabocha-0.69/model/dep.ipa.txt\n",
            "cabocha-0.69/model/ne.juman.txt\n",
            "cabocha-0.69/model/dep.juman.txt\n",
            "cabocha-0.69/model/Makefile.in\n",
            "cabocha-0.69/model/dep.unidic.txt\n",
            "cabocha-0.69/model/chunk.ipa.txt\n",
            "cabocha-0.69/model/chunk.unidic.txt\n",
            "cabocha-0.69/model/ne.ipa.txt\n",
            "cabocha-0.69/model/ne.unidic.txt\n",
            "cabocha-0.69/model/chunk.juman.txt\n",
            "cabocha-0.69/model/Makefile.am\n",
            "cabocha-0.69/doc/\n",
            "cabocha-0.69/doc/README.txt\n",
            "cabocha-0.69/doc/doxygen/\n",
            "cabocha-0.69/doc/doxygen/classes.html\n",
            "cabocha-0.69/doc/doxygen/ftv2plastnode.png\n",
            "cabocha-0.69/doc/doxygen/nav_g.png\n",
            "cabocha-0.69/doc/doxygen/files.html\n",
            "cabocha-0.69/doc/doxygen/tab_b.gif\n",
            "cabocha-0.69/doc/doxygen/nav_h.png\n",
            "cabocha-0.69/doc/doxygen/namespaceCaboCha.html\n",
            "cabocha-0.69/doc/doxygen/functions_vars.html\n",
            "cabocha-0.69/doc/doxygen/tab_s.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_eval.html\n",
            "cabocha-0.69/doc/doxygen/ftv2pnode.png\n",
            "cabocha-0.69/doc/doxygen/cabocha_8h.html\n",
            "cabocha-0.69/doc/doxygen/open.png\n",
            "cabocha-0.69/doc/doxygen/globals_func.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__token__t.html\n",
            "cabocha-0.69/doc/doxygen/doxygen.css\n",
            "cabocha-0.69/doc/doxygen/ftv2node.png\n",
            "cabocha-0.69/doc/doxygen/functions_func.html\n",
            "cabocha-0.69/doc/doxygen/ftv2mnode.png\n",
            "cabocha-0.69/doc/doxygen/ftv2doc.png\n",
            "cabocha-0.69/doc/doxygen/globals_enum.html\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Tree.html\n",
            "cabocha-0.69/doc/doxygen/functions.html\n",
            "cabocha-0.69/doc/doxygen/ftv2folderopen.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers.html\n",
            "cabocha-0.69/doc/doxygen/globals.html\n",
            "cabocha-0.69/doc/doxygen/ftv2link.png\n",
            "cabocha-0.69/doc/doxygen/ftv2folderclosed.png\n",
            "cabocha-0.69/doc/doxygen/structcabocha__token__t-members.html\n",
            "cabocha-0.69/doc/doxygen/bdwn.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_func.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__chunk__t.html\n",
            "cabocha-0.69/doc/doxygen/bc_s.png\n",
            "cabocha-0.69/doc/doxygen/cabocha_8h_source.html\n",
            "cabocha-0.69/doc/doxygen/globals_eval.html\n",
            "cabocha-0.69/doc/doxygen/ftv2mo.png\n",
            "cabocha-0.69/doc/doxygen/doxygen.png\n",
            "cabocha-0.69/doc/doxygen/index.html\n",
            "cabocha-0.69/doc/doxygen/tab_b.png\n",
            "cabocha-0.69/doc/doxygen/closed.png\n",
            "cabocha-0.69/doc/doxygen/nav_f.png\n",
            "cabocha-0.69/doc/doxygen/ftv2lastnode.png\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Tree-members.html\n",
            "cabocha-0.69/doc/doxygen/tabs.css\n",
            "cabocha-0.69/doc/doxygen/ftv2vertline.png\n",
            "cabocha-0.69/doc/doxygen/ftv2cl.png\n",
            "cabocha-0.69/doc/doxygen/tab_h.png\n",
            "cabocha-0.69/doc/doxygen/globals_type.html\n",
            "cabocha-0.69/doc/doxygen/structcabocha__chunk__t-members.html\n",
            "cabocha-0.69/doc/doxygen/globals_defs.html\n",
            "cabocha-0.69/doc/doxygen/annotated.html\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_type.html\n",
            "cabocha-0.69/doc/doxygen/tab_l.gif\n",
            "cabocha-0.69/doc/doxygen/tab_a.png\n",
            "cabocha-0.69/doc/doxygen/sync_off.png\n",
            "cabocha-0.69/doc/doxygen/ftv2ns.png\n",
            "cabocha-0.69/doc/doxygen/tab_r.gif\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Parser-members.html\n",
            "cabocha-0.69/doc/doxygen/ftv2splitbar.png\n",
            "cabocha-0.69/doc/doxygen/ftv2mlastnode.png\n",
            "cabocha-0.69/doc/doxygen/classCaboCha_1_1Parser.html\n",
            "cabocha-0.69/doc/doxygen/namespaces.html\n",
            "cabocha-0.69/doc/doxygen/sync_on.png\n",
            "cabocha-0.69/doc/doxygen/namespacemembers_enum.html\n",
            "cabocha-0.69/doc/doxygen/dir_68267d1309a1af8e8297ef4c3efbcdba.html\n",
            "cabocha-0.69/doc/doxygen/dynsections.js\n",
            "cabocha-0.69/doc/doxygen/ftv2blank.png\n",
            "cabocha-0.69/doc/cabocha.cfg\n",
            "/content/cabocha-0.69\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports nested variables... yes\n",
            "checking whether to enable maintainer-specific portions of Makefiles... no\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking whether gcc understands -c and -o together... yes\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... none\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... none\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... dlltool\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7604: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for library containing strerror... none required\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for shared library run path origin... done\n",
            "checking for iconv... yes\n",
            "checking for working iconv... yes\n",
            "checking for iconv declaration... \n",
            "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
            "checking for ANSI C header files... (cached) yes\n",
            "checking for an ANSI C-conforming const... yes\n",
            "checking whether byte ordering is bigendian... no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking dirent.h usability... yes\n",
            "checking dirent.h presence... yes\n",
            "checking for dirent.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking io.h usability... no\n",
            "checking io.h presence... no\n",
            "checking for io.h... no\n",
            "checking windows.h usability... no\n",
            "checking windows.h presence... no\n",
            "checking for windows.h... no\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for off_t... yes\n",
            "checking for size_t... yes\n",
            "checking size of char... 1\n",
            "checking size of short... 2\n",
            "checking size of int... 4\n",
            "checking size of long... 8\n",
            "checking size of long long... 8\n",
            "checking size of size_t... 8\n",
            "checking for size_t... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for getenv... yes\n",
            "checking for opendir... yes\n",
            "checking for snprintf... yes\n",
            "checking for mecab-config... /usr/bin/mecab-config\n",
            "checking whether iconv supports EUC-JP-MS and CP932... checking for main in -lstdc++... yes\n",
            "checking for crfpp_new in -lcrfpp... yes\n",
            "checking for mecab_new in -lmecab... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <list> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <queue> (required)... yes\n",
            "checking if g++ supports stl <functional> (required)... yes\n",
            "checking if g++ supports stl <algorithm> (required)... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <strstream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports dynamic_cast<> (required)... yes\n",
            "checking if g++ supports reinterpret_cast<> (required)... yes\n",
            "checking if g++ supports exception handler (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ environment provides all required features... yes\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating src/Makefile\n",
            "config.status: creating src/Makefile.msvc\n",
            "config.status: creating model/Makefile\n",
            "config.status: creating man/Makefile\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating cabocha-config\n",
            "config.status: creating cabocharc\n",
            "config.status: creating cabocha.iss\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "config.status: executing default commands\n",
            "make  all-recursive\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "Making all in src\n",
            "make[2]: Entering directory '/content/cabocha-0.69/src'\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o chunk_learner.lo chunk_learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunk_learner.cpp  -fPIC -DPIC -o .libs/chunk_learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunk_learner.cpp -o chunk_learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o chunker.lo chunker.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunker.cpp  -fPIC -DPIC -o .libs/chunker.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c chunker.cpp -o chunker.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o dep.lo dep.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep.cpp  -fPIC -DPIC -o .libs/dep.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep.cpp -o dep.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o dep_learner.lo dep_learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep_learner.cpp  -fPIC -DPIC -o .libs/dep_learner.o\n",
            "In file included from \u001b[01m\u001b[Kdep_learner.cpp:17:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c dep_learner.cpp -o dep_learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o eval.lo eval.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c eval.cpp  -fPIC -DPIC -o .libs/eval.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c eval.cpp -o eval.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o learner.lo learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c learner.cpp  -fPIC -DPIC -o .libs/learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c learner.cpp -o learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o libcabocha.lo libcabocha.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c libcabocha.cpp  -fPIC -DPIC -o .libs/libcabocha.o\n",
            "In file included from \u001b[01m\u001b[Klibcabocha.cpp:18:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c libcabocha.cpp -o libcabocha.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o morph.lo morph.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c morph.cpp  -fPIC -DPIC -o .libs/morph.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c morph.cpp -o morph.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o ne.lo ne.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ne.cpp  -fPIC -DPIC -o .libs/ne.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ne.cpp -o ne.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o normalizer.lo normalizer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c normalizer.cpp  -fPIC -DPIC -o .libs/normalizer.o\n",
            "\u001b[01m\u001b[Knormalizer.cpp:\u001b[m\u001b[K In static member function '\u001b[01m\u001b[Kstatic void CaboCha::Normalizer::normalize(int, const char*, size_t, std::__cxx11::string*)\u001b[m\u001b[K':\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "\u001b[01m\u001b[Knormalizer.cpp:113:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koffset '\u001b[01m\u001b[K-1\u001b[m\u001b[K' outside bounds of constant string\n",
            "       *output += &ctable[result];\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c normalizer.cpp -o normalizer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o parser.lo parser.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c parser.cpp  -fPIC -DPIC -o .libs/parser.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c parser.cpp -o parser.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o selector.lo selector.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c selector.cpp  -fPIC -DPIC -o .libs/selector.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c selector.cpp -o selector.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o tree_allocator.lo tree_allocator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree_allocator.cpp  -fPIC -DPIC -o .libs/tree_allocator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree_allocator.cpp -o tree_allocator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o string_buffer.lo string_buffer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c string_buffer.cpp  -fPIC -DPIC -o .libs/string_buffer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c string_buffer.cpp -o string_buffer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o svm.lo svm.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm.cpp  -fPIC -DPIC -o .libs/svm.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm.cpp -o svm.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o svm_learn.lo svm_learn.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm_learn.cpp  -fPIC -DPIC -o .libs/svm_learn.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c svm_learn.cpp -o svm_learn.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o tree.lo tree.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree.cpp  -fPIC -DPIC -o .libs/tree.o\n",
            "In file included from \u001b[01m\u001b[Kstring_buffer.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Ktree.cpp:14\u001b[m\u001b[K:\n",
            "utils.h: In instantiation of '\u001b[01m\u001b[Ksize_t CaboCha::tokenizeCSV(char*, Iterator, size_t) [with Iterator = char**; size_t = long unsigned int]\u001b[m\u001b[K':\n",
            "\u001b[01m\u001b[Ktree.cpp:480:57:\u001b[m\u001b[K   required from here\n",
            "\u001b[01m\u001b[Kutils.h:127:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable '\u001b[01m\u001b[Kinquote\u001b[m\u001b[K' set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     bool \u001b[01;35m\u001b[Kinquote\u001b[m\u001b[K = false;\n",
            "          \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c tree.cpp -o tree.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o ucs.lo ucs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ucs.cpp  -fPIC -DPIC -o .libs/ucs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c ucs.cpp -o ucs.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o utils.lo utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c utils.cpp  -fPIC -DPIC -o .libs/utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\\\"IPA\\\" -DCABOCHA_DEFAULT_CHARSET=\\\"utf-8\\\" -DMODEL_VERSION=102 -DCABOCHA_DEFAULT_RC=\\\"/usr/local/etc/cabocharc\\\" -O3 -Wno-deprecated -Wall -c utils.cpp -o utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall -no-undefined -version-info 5:0:0  -o libcabocha.la -rpath /usr/local/lib chunk_learner.lo chunker.lo dep.lo dep_learner.lo eval.lo learner.lo libcabocha.lo morph.lo ne.lo normalizer.lo param.lo parser.lo selector.lo tree_allocator.lo string_buffer.lo svm.lo svm_learn.lo tree.lo ucs.lo utils.lo  -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/chunk_learner.o .libs/chunker.o .libs/dep.o .libs/dep_learner.o .libs/eval.o .libs/learner.o .libs/libcabocha.o .libs/morph.o .libs/ne.o .libs/normalizer.o .libs/param.o .libs/parser.o .libs/selector.o .libs/tree_allocator.o .libs/string_buffer.o .libs/svm.o .libs/svm_learn.o .libs/tree.o .libs/ucs.o .libs/utils.o   /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libcabocha.so.5 -o .libs/libcabocha.so.5.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcabocha.so.5\" && ln -s \"libcabocha.so.5.0.0\" \"libcabocha.so.5\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libcabocha.so\" && ln -s \"libcabocha.so.5.0.0\" \"libcabocha.so\")\n",
            "libtool: link: ar cru .libs/libcabocha.a  chunk_learner.o chunker.o dep.o dep_learner.o eval.o learner.o libcabocha.o morph.o ne.o normalizer.o param.o parser.o selector.o tree_allocator.o string_buffer.o svm.o svm_learn.o tree.o ucs.o utils.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libcabocha.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libcabocha.la\" && ln -s \"../libcabocha.la\" \"libcabocha.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha.o cabocha.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha cabocha.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha cabocha.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-model-index.o cabocha-model-index.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-model-index cabocha-model-index.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-model-index cabocha-model-index.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-learn.o cabocha-learn.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-learn cabocha-learn.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-learn cabocha-learn.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DCABOCHA_DEFAULT_POSSET=\"\\\"IPA\"\\\" -DCABOCHA_DEFAULT_CHARSET=\"\\\"utf-8\"\\\" -DMODEL_VERSION=102  -DCABOCHA_DEFAULT_RC=\"\\\"/usr/local/etc/cabocharc\\\"\"    -O3 -Wno-deprecated -Wall -c -o cabocha-system-eval.o cabocha-system-eval.cpp\n",
            "/bin/bash ../libtool  --tag=CXX   --mode=link g++  -O3 -Wno-deprecated -Wall   -o cabocha-system-eval cabocha-system-eval.o libcabocha.la -lcrfpp -lmecab  -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++ \n",
            "libtool: link: g++ -O3 -Wno-deprecated -Wall -o .libs/cabocha-system-eval cabocha-system-eval.o  ./.libs/libcabocha.so /usr/local/lib/libcrfpp.so -L/usr/lib/x86_64-linux-gnu -lmecab -lstdc++\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making all in model\n",
            "make[2]: Entering directory '/content/cabocha-0.69/model'\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.ipa.txt chunk.ipa.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.juman.txt chunk.juman.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 chunk.unidic.txt chunk.unidic.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.ipa.txt dep.ipa.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2340864\n",
            "trie         size : 22100992\n",
            "feature size      : 122541\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 113308\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 1.98193e-07\n",
            "Done!\n",
            "12.93 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.juman.txt dep.juman.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2892800\n",
            "trie         size : 22047744\n",
            "feature size      : 149674\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 78200\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 2.27711e-07\n",
            "Done!\n",
            "12.76 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 dep.unidic.txt dep.unidic.model\n",
            "emitting dic    : 100% |###########################################| \n",
            "emitting trie   : 100% |###########################################| \n",
            "\n",
            "double array size : 2159616\n",
            "trie         size : 19891200\n",
            "feature size      : 121120\n",
            "freq feature size : 3000\n",
            "minsup            : 2\n",
            "bias              : 92412\n",
            "sigma             : 0.0001\n",
            "normalize factor  : 2.27189e-07\n",
            "Done!\n",
            "11.69 s\n",
            "\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.ipa.txt ne.ipa.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.juman.txt ne.juman.model\n",
            "../src/cabocha-model-index -f UTF8 -t utf-8 ne.unidic.txt ne.unidic.model\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making all in man\n",
            "make[2]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[2]: Entering directory '/content/cabocha-0.69'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "Making check in src\n",
            "make[1]: Entering directory '/content/cabocha-0.69/src'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making check in model\n",
            "make[1]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making check in man\n",
            "make[1]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "Making install in src\n",
            "make[1]: Entering directory '/content/cabocha-0.69/src'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/src'\n",
            " /bin/mkdir -p '/usr/local/lib'\n",
            " /bin/bash ../libtool   --mode=install /usr/bin/install -c   libcabocha.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.so.5.0.0 /usr/local/lib/libcabocha.so.5.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcabocha.so.5.0.0 libcabocha.so.5 || { rm -f libcabocha.so.5 && ln -s libcabocha.so.5.0.0 libcabocha.so.5; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libcabocha.so.5.0.0 libcabocha.so || { rm -f libcabocha.so && ln -s libcabocha.so.5.0.0 libcabocha.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.lai /usr/local/lib/libcabocha.la\n",
            "libtool: install: /usr/bin/install -c .libs/libcabocha.a /usr/local/lib/libcabocha.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libcabocha.a\n",
            "libtool: install: ranlib /usr/local/lib/libcabocha.a\n",
            "libtool: finish: PATH=\"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            " /bin/mkdir -p '/usr/local/bin'\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c cabocha '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha /usr/local/bin/cabocha\n",
            " /bin/mkdir -p '/usr/local/libexec/cabocha'\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c cabocha-model-index cabocha-learn cabocha-system-eval '/usr/local/libexec/cabocha'\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-model-index /usr/local/libexec/cabocha/cabocha-model-index\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-learn /usr/local/libexec/cabocha/cabocha-learn\n",
            "libtool: install: /usr/bin/install -c .libs/cabocha-system-eval /usr/local/libexec/cabocha/cabocha-system-eval\n",
            " /bin/mkdir -p '/usr/local/include'\n",
            " /usr/bin/install -c -m 644 cabocha.h '/usr/local/include'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/src'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/src'\n",
            "Making install in model\n",
            "make[1]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/model'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/lib/cabocha/model'\n",
            " /usr/bin/install -c -m 644 chunk.ipa.model chunk.juman.model chunk.unidic.model dep.ipa.model dep.juman.model dep.unidic.model ne.ipa.model ne.juman.model ne.unidic.model '/usr/local/lib/cabocha/model'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/model'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/model'\n",
            "Making install in man\n",
            "make[1]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Entering directory '/content/cabocha-0.69/man'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/share/man/man1'\n",
            " /usr/bin/install -c -m 644 cabocha.1 '/usr/local/share/man/man1'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69/man'\n",
            "make[1]: Entering directory '/content/cabocha-0.69'\n",
            "make[2]: Entering directory '/content/cabocha-0.69'\n",
            " /bin/mkdir -p '/usr/local/bin'\n",
            " /usr/bin/install -c cabocha-config '/usr/local/bin'\n",
            " /bin/mkdir -p '/usr/local/etc'\n",
            " /usr/bin/install -c -m 644 cabocharc '/usr/local/etc'\n",
            "make[2]: Leaving directory '/content/cabocha-0.69'\n",
            "make[1]: Leaving directory '/content/cabocha-0.69'\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cabocha-pythonのインストール\n",
        "%cd ~/../content/cabocha-0.69/python\n",
        "!python setup.py build_ext\n",
        "!python setup.py install\n",
        "!pwd\n",
        "!ls\n",
        "%cd ~/../content"
      ],
      "metadata": {
        "id": "1ojGkcV_PyUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d55e13-8f6e-4c7b-baea-3a76e4a3df6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cabocha-0.69/python\n",
            "running build_ext\n",
            "building '_CaboCha' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/include -I/usr/include/python3.7m -c CaboCha_wrap.cxx -o build/temp.linux-x86_64-3.7/CaboCha_wrap.o\n",
            "warning: no library file corresponding to '-L/usr/lib/x86_64inux-gnu' found (skipping)\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-dIfpci/python3.7-3.7.13=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/CaboCha_wrap.o -L/usr/local/lib -lcabocha -lcrfpp -lmecab -lmecab -lstdc++ -o build/lib.linux-x86_64-3.7/_CaboCha.cpython-37m-x86_64-linux-gnu.so\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "copying CaboCha.py -> build/lib.linux-x86_64-3.7\n",
            "running build_ext\n",
            "running install_lib\n",
            "copying build/lib.linux-x86_64-3.7/CaboCha.py -> /usr/local/lib/python3.7/dist-packages\n",
            "copying build/lib.linux-x86_64-3.7/_CaboCha.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/CaboCha.py to CaboCha.cpython-37.pyc\n",
            "running install_egg_info\n",
            "Writing /usr/local/lib/python3.7/dist-packages/cabocha_python-0.69.egg-info\n",
            "/content/cabocha-0.69/python\n",
            "build  CaboCha.py  CaboCha_wrap.cxx  setup.py  test.py\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語をmatplotlibで表示できるようにする\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# グラフ表示用\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "9-bEGSsEY0XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e689ad50-9de6-484b-b1d3-50b7cea52c60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from japanize-matplotlib) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize-matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->japanize-matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->japanize-matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120275 sha256=d578aa68c7908af705699f177b66e5f6a12be8d8f0d42c3c2f958623d70cd409\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/97/6b/e9e0cde099cc40f972b8dd23367308f7705ae06cd6d4714658\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 導入確認\n",
        "!mecab -v\n",
        "!pip show mecab-python3\n",
        "!cabocha -v\n",
        "!pip show cabocha-python"
      ],
      "metadata": {
        "id": "Pm42fDyZcnt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad205b23-5aa9-4f96-f9a5-d7705705d6d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mecab of 0.996\n",
            "\n",
            "Name: mecab-python3\n",
            "Version: 0.7\n",
            "Summary: python wrapper for mecab: Morphological Analysis engine\n",
            "Home-page: https://github.com/SamuraiT/mecab-python3\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "cabocha of 0.69\n",
            "Name: cabocha-python\n",
            "Version: 0.69\n",
            "Summary: UNKNOWN\n",
            "Home-page: UNKNOWN\n",
            "Author: UNKNOWN\n",
            "Author-email: UNKNOWN\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MeCab TEST CODE ##\n",
        "\n",
        "import MeCab\n",
        "tagger = MeCab.Tagger(\"-Odump\")\n",
        "print(tagger.parse(\"隣の客はよく柿食う客だ。\"))\n",
        "print(tagger.parse(\"今日は一日中家にいることにしている。\"))"
      ],
      "metadata": {
        "id": "chRTBEz96TvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049339e6-dadc-4816-8ed4-c56871603fa3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0\n",
            "1 隣 名詞,一般,*,*,*,*,隣,トナリ,トナリ 0 3 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 6894\n",
            "5 の 助詞,連体化,*,*,*,*,の,ノ,ノ 3 6 368 368 24 6 0 1 0.000000 0.000000 0.000000 7268\n",
            "8 客 名詞,一般,*,*,*,*,客,キャク,キャク 6 9 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 10360\n",
            "11 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 9 12 261 261 16 6 0 1 0.000000 0.000000 0.000000 10380\n",
            "23 よく 副詞,一般,*,*,*,*,よく,ヨク,ヨク 12 18 1281 1281 34 6 0 1 0.000000 0.000000 0.000000 12183\n",
            "31 柿 名詞,一般,*,*,*,*,柿,カキ,カキ 18 21 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 18979\n",
            "37 食う 動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ 21 27 817 817 31 2 0 1 0.000000 0.000000 0.000000 24452\n",
            "44 客 名詞,一般,*,*,*,*,客,キャク,キャク 27 30 1285 1285 38 2 0 1 0.000000 0.000000 0.000000 28626\n",
            "48 だ 助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ 30 33 453 453 25 6 0 1 0.000000 0.000000 0.000000 30648\n",
            "49 。 記号,句点,*,*,*,*,。,。,。 33 36 8 8 7 3 0 1 0.000000 0.000000 0.000000 26508\n",
            "51 EOS BOS/EOS,*,*,*,*,*,*,*,* 36 36 0 0 0 0 3 1 0.000000 0.000000 0.000000 24972\n",
            "\n",
            "0 BOS BOS/EOS,*,*,*,*,*,*,*,* 0 0 0 0 0 0 2 1 0.000000 0.000000 0.000000 0\n",
            "7 今日 名詞,副詞可能,*,*,*,*,今日,キョウ,キョー 0 6 1314 1314 67 2 0 1 0.000000 0.000000 0.000000 3947\n",
            "20 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 6 9 261 261 16 6 0 1 0.000000 0.000000 0.000000 4822\n",
            "28 一日中 名詞,一般,*,*,*,*,一日中,イチニチジュウ,イチニチジュー 9 18 1285 1285 38 8 0 1 0.000000 0.000000 0.000000 11799\n",
            "64 家 名詞,接尾,一般,*,*,*,家,カ,カ 18 21 1298 1298 51 2 0 1 0.000000 0.000000 0.000000 14853\n",
            "67 に 助詞,格助詞,一般,*,*,*,に,ニ,ニ 21 24 151 151 13 6 0 1 0.000000 0.000000 0.000000 14880\n",
            "80 いる 動詞,自立,*,*,一段,基本形,いる,イル,イル 24 30 619 619 31 6 0 1 0.000000 0.000000 0.000000 19162\n",
            "91 こと 名詞,非自立,一般,*,*,*,こと,コト,コト 30 36 1310 1310 63 6 0 1 0.000000 0.000000 0.000000 20026\n",
            "101 に 助詞,格助詞,一般,*,*,*,に,ニ,ニ 36 39 151 151 13 6 0 1 0.000000 0.000000 0.000000 19975\n",
            "112 し 動詞,自立,*,*,サ変・スル,連用形,する,シ,シ 39 42 610 610 31 6 0 1 0.000000 0.000000 0.000000 23328\n",
            "116 て 助詞,接続助詞,*,*,*,*,て,テ,テ 42 45 307 307 18 6 0 1 0.000000 0.000000 0.000000 21835\n",
            "128 いる 動詞,非自立,*,*,一段,基本形,いる,イル,イル 45 51 919 919 33 6 0 1 0.000000 0.000000 0.000000 22384\n",
            "134 。 記号,句点,*,*,*,*,。,。,。 51 54 8 8 7 3 0 1 0.000000 0.000000 0.000000 19114\n",
            "136 EOS BOS/EOS,*,*,*,*,*,*,*,* 54 54 0 0 0 0 3 1 0.000000 0.000000 0.000000 17578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CaboCha TEST CODE ##\n",
        "\n",
        "import CaboCha\n",
        "cp = CaboCha.Parser()\n",
        "sentence = '猫は道路を渡る犬を見た。'\n",
        "print(cp.parseToString(sentence))"
      ],
      "metadata": {
        "id": "M1QuOi6K1c-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19d06a7-bff4-4379-f0c6-16962f6941d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  猫は-------D\n",
            "  道路を-D   |\n",
            "      渡る-D |\n",
            "        犬を-D\n",
            "        見た。\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cabocha -f1 ai.ja.txt -o ai.ja.txt.parsed\n",
        "!head -25 ai.ja.txt.parsed"
      ],
      "metadata": {
        "id": "c49V6FTf4qVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6827e6a-6f16-4d39-ff89-3e35c8cbae52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* 0 -1D 1/1 0.000000\n",
            "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
            "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
            "EOS\n",
            "EOS\n",
            "* 0 17D 1/1 0.388993\n",
            "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
            "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
            "* 1 17D 2/3 0.613549\n",
            "（\t記号,括弧開,*,*,*,*,（,（,（\n",
            "じん\t名詞,一般,*,*,*,*,じん,ジン,ジン\n",
            "こうち\t名詞,一般,*,*,*,*,こうち,コウチ,コーチ\n",
            "のう\t助詞,終助詞,*,*,*,*,のう,ノウ,ノー\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "* 2 3D 0/0 0.758984\n",
            "AI\t名詞,一般,*,*,*,*,*\n",
            "* 3 17D 1/5 0.517898\n",
            "〈\t記号,括弧開,*,*,*,*,〈,〈,〈\n",
            "エーアイ\t名詞,固有名詞,一般,*,*,*,*\n",
            "〉\t記号,括弧閉,*,*,*,*,〉,〉,〉\n",
            "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
            "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CaboChaの出力形式\n",
        "cabochaを使うと以下のような形式で出力される\n",
        "\n",
        "    * 0 1D 0/1 2.206035\n",
        "    隣\t名詞,一般,*,*,*,*,隣,トナリ,トナリ\n",
        "    の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
        "    * 1 5D 0/1 -0.593304\n",
        "    客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
        "    は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
        "    * 2 4D 0/0 0.538813\n",
        "    よく\t副詞,一般,*,*,*,*,よく,ヨク,ヨク\n",
        "    * 3 4D 0/0 1.985106\n",
        "    柿\t名詞,一般,*,*,*,*,柿,カキ,カキ\n",
        "    * 4 5D 0/0 -0.593304\n",
        "    食う\t動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ\n",
        "    * 5 -1D 0/1 0.000000\n",
        "    客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
        "    だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
        "    EOS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhmGwHRUGhid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"隣の客はよく柿食う客だ\" | cabocha -f1 "
      ],
      "metadata": {
        "id": "oHuTIxZEIfpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1b7a00-071b-4974-fc60-768e11e2eb11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* 0 1D 0/1 2.206035\n",
            "隣\t名詞,一般,*,*,*,*,隣,トナリ,トナリ\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "* 1 5D 0/1 -0.593304\n",
            "客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "* 2 4D 0/0 0.538813\n",
            "よく\t副詞,一般,*,*,*,*,よく,ヨク,ヨク\n",
            "* 3 4D 0/0 1.985106\n",
            "柿\t名詞,一般,*,*,*,*,柿,カキ,カキ\n",
            "* 4 5D 0/0 -0.593304\n",
            "食う\t動詞,自立,*,*,五段・ワ行促音便,基本形,食う,クウ,クウ\n",
            "* 5 -1D 0/1 0.000000\n",
            "客\t名詞,一般,*,*,*,*,客,キャク,キャク\n",
            "だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
            "EOS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "これは各文節毎に区切ってあり、以下のように読む\n",
        "\n",
        "- 1行目\n",
        "  1. *\n",
        "  2. 文節番号\n",
        "  3. 係り先の文節番号(係り先なし:-1)\n",
        "  4. 主辞の形態素番号/機能語の形態素番号\n",
        "  5. 係り関係のスコア(大きい方が係りやすい)\n",
        "- 2行目\n",
        "  1. 表層形 （Tab区切り）\n",
        "  2. 品詞\n",
        "  3. 品詞細分類1\n",
        "  4. 品詞細分類2\n",
        "  5. 品詞細分類3\n",
        "  6. 活用形\n",
        "  7. 活用型\n",
        "  8. 原形\n",
        "  9. 読み\n",
        "  10. 発音\n",
        "\n",
        "#### [参考]\n",
        "- CaboChaで始める係り受け解析 by @nezuq\n",
        "  - Qiita https://qiita.com/nezuq/items/f481f07fc0576b38e81d"
      ],
      "metadata": {
        "id": "jW31L0z5JH_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 40. 係り受け解析結果の読み込み（形態素）\n",
        "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．\n",
        "\n"
      ],
      "metadata": {
        "id": "efvr7lJWGCvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsUvj8-OGCvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bc4e28-91ab-4b5a-e0b6-63046e45a5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'surface': '人工', 'base': '人工', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '知能', 'base': '知能', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '人工', 'base': '人工', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '知能', 'base': '知能', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '（', 'base': '（', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': 'じん', 'base': 'じん', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': 'こうち', 'base': 'こうち', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': 'のう', 'base': 'のう', 'pos': '助詞', 'pos1': '終助詞'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': 'AI', 'base': '*\\n', 'pos': '名詞', 'pos1': '一般'}\n",
            "{'surface': '〈', 'base': '〈', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': 'エーアイ', 'base': '*\\n', 'pos': '名詞', 'pos1': '固有名詞'}\n",
            "{'surface': '〉', 'base': '〉', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': '）', 'base': '）', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': 'と', 'base': 'と', 'pos': '助詞', 'pos1': '格助詞'}\n",
            "{'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}\n",
            "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
            "{'surface': '「', 'base': '「', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '『', 'base': '『', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '計算', 'base': '計算', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
            "{'surface': '（', 'base': '（', 'pos': '記号', 'pos1': '括弧開'}\n",
            "{'surface': '）', 'base': '）', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': '』', 'base': '』', 'pos': '記号', 'pos1': '括弧閉'}\n",
            "{'surface': 'という', 'base': 'という', 'pos': '助詞', 'pos1': '格助詞'}\n"
          ]
        }
      ],
      "source": [
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, morph):\n",
        "    self.surface = morph[\"surface\"]  # 表層形\n",
        "    self.base = morph[\"base\"]  # 基本形\n",
        "    self.pos = morph[\"pos\"]  # 品詞\n",
        "    self.pos1 = morph[\"pos1\"]  # 品詞細分類1\n",
        "\n",
        "\n",
        "def make_mapping(word):\n",
        "  morpheme = word[1].split(\",\")\n",
        "  d = {\n",
        "    str(morphemes[0]) : word[0], # 表層形\n",
        "    str(morphemes[1]) : morpheme[6], # 基本形\n",
        "    str(morphemes[2]) : morpheme[0], # 品詞\n",
        "    str(morphemes[3]) : morpheme[1] # 品詞細分類1\n",
        "  }\n",
        "  return d\n",
        "\n",
        "\n",
        "morph_list = []\n",
        "sentence = []\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines[:]:\n",
        "    if line != \"EOS\\n\":\n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"): \n",
        "        sentence.append(Morph(make_mapping(word)))\n",
        "\n",
        "\n",
        "for m in sentence[:25]:\n",
        "  print(vars(m))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
        "\n"
      ],
      "metadata": {
        "id": "NVOr6AErGCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 苦悩のログ\n",
        "> 1行ずつ読んで、Morphを作って、EOSか文節情報が来たら直前の文節を格納…ン？？？？どうやんの？？？？\n",
        "  → 1行ずつ読むのではなくて、`EOS\\n` で分割してブロックごとに処理すると以下のメリットが有る\n",
        "  - EOSは出てこない\n",
        "  - 文節ごとに処理するので行読みしても文節頭の係り受け情報が保持できる\n",
        "\n",
        "> あと文節の係り元の方では「DPじゃないし逆引き無理じゃない？」と思ってたら普通に他の人も1周for回してやってた。そらそうか\n",
        "\n",
        "> といって安直にforを回すと第2文以降でまたCaboChaによる係り番号が0にリセットされるので頭の方だけ以上に係り受けすることになってしまった。\n",
        "  - Class : Chunkに文節番号を保持する要素を付加することで半ば無理やり解決\n",
        "\n",
        "\n",
        "なぜか1文目の34語目までしか処理されず、35語以降が処理されていない…そんなことある？\n",
        "\n",
        "> $「…情報処理システムの設計や実現に関する/研究分野」ともされる。」$\n",
        "> の、`/`部分までしか処理されていない…\n",
        "\n",
        "    # 33  形態素: ['設計', 'や'],\t 係り先: 34, 係り元: []\n",
        "    # 34  形態素: ['実現', 'に関する'],\t 係り先: 35, 係り元: [33, 33]\n",
        "    #  1  形態素: ['『', '日本', '大', '百科全書', '(', 'ニッポニカ', ')』', 'の'],\t 係り先: 5, 係り元: []\n",
        "    #  2  形態素: ['解説', 'で', '、'],\t 係り先: 3, 係り元: []\n",
        "\n",
        ">> → 文章末尾の処理でforの内外の位置を間違えていただけでした\n",
        "\n",
        "> (環境構築含め)実質4日(週跨ぎなので6日？)くらいかけてようやくできた…\n",
        "  自作Classを含むClass、のリスト、のリストという面倒なことでしかも通し番号とか処理してたら本当に面倒になってた"
      ],
      "metadata": {
        "id": "68hUkbcLlnCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "i = 0\n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    #print(f\"block:{block}\")\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      #print(line)\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          #print(line)\n",
        "          #print(f\"morphemes:{[m.surface for m in morphemes]}\")\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        #print(word)\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      #print(f\"morphemes:{[m.surface for m in morphemes]}\")\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0 : all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(all_text[:10]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence:\n",
        "    print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4G8JMAmzGCvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bee0fa7-8494-4fda-cb0e-4244159a51fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "#  0  形態素: ['人工', '知能'],\t 係り先: -1, 係り元: []\n",
            "--- Sentence 0001 ---\n",
            "#  0  形態素: ['人工', '知能'],\t 係り先: 17, 係り元: []\n",
            "#  1  形態素: ['（', 'じん', 'こうち', 'のう', '、', '、'],\t 係り先: 17, 係り元: []\n",
            "#  2  形態素: ['AI'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['〈', 'エーアイ', '〉', '）', 'と', 'は', '、'],\t 係り先: 17, 係り元: [2]\n",
            "#  4  形態素: ['「', '『', '計算'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['（', '）', '』', 'という'],\t 係り先: 9, 係り元: [4]\n",
            "#  6  形態素: ['概念', 'と'],\t 係り先: 9, 係り元: []\n",
            "#  7  形態素: ['『', 'コンピュータ'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['（', '）', '』', 'という'],\t 係り先: 9, 係り元: [7]\n",
            "#  9  形態素: ['道具', 'を'],\t 係り先: 10, 係り元: [5, 6, 8]\n",
            "# 10  形態素: ['用い', 'て'],\t 係り先: 12, 係り元: [9]\n",
            "# 11  形態素: ['『', '知能', '』', 'を'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['研究', 'する'],\t 係り先: 13, 係り元: [10, 11]\n",
            "# 13  形態素: ['計算', '機', '科学'],\t 係り先: 14, 係り元: [12]\n",
            "# 14  形態素: ['（', '）', 'の'],\t 係り先: 15, 係り元: [13]\n",
            "# 15  形態素: ['一', '分野', '」', 'を'],\t 係り先: 16, 係り元: [14]\n",
            "# 16  形態素: ['指す'],\t 係り先: 17, 係り元: [15]\n",
            "# 17  形態素: ['語', '。'],\t 係り先: 34, 係り元: [0, 1, 3, 16]\n",
            "# 18  形態素: ['「', '言語', 'の'],\t 係り先: 20, 係り元: []\n",
            "# 19  形態素: ['理解', 'や'],\t 係り先: 20, 係り元: []\n",
            "# 20  形態素: ['推論', '、'],\t 係り先: 21, 係り元: [18, 19]\n",
            "# 21  形態素: ['問題', '解決', 'など', 'の'],\t 係り先: 22, 係り元: [20]\n",
            "# 22  形態素: ['知的', '行動', 'を'],\t 係り先: 24, 係り元: [21]\n",
            "# 23  形態素: ['人間', 'に'],\t 係り先: 24, 係り元: []\n",
            "# 24  形態素: ['代わっ', 'て'],\t 係り先: 26, 係り元: [22, 23]\n",
            "# 25  形態素: ['コンピューター', 'に'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['行わ', 'せる'],\t 係り先: 27, 係り元: [24, 25]\n",
            "# 27  形態素: ['技術', '」', '、', 'または', '、'],\t 係り先: 34, 係り元: [26]\n",
            "# 28  形態素: ['「', '計算', '機'],\t 係り先: 29, 係り元: []\n",
            "# 29  形態素: ['（', 'コンピュータ', '）', 'による'],\t 係り先: 31, 係り元: [28]\n",
            "# 30  形態素: ['知的', 'な'],\t 係り先: 31, 係り元: []\n",
            "# 31  形態素: ['情報処理', 'システム', 'の'],\t 係り先: 33, 係り元: [29, 30]\n",
            "# 32  形態素: ['設計', 'や'],\t 係り先: 33, 係り元: []\n",
            "# 33  形態素: ['実現', 'に関する'],\t 係り先: 34, 係り元: [31, 32]\n",
            "# 34  形態素: ['研究', '分野', '」', 'と', 'も'],\t 係り先: 35, 係り元: [17, 27, 33]\n",
            "# 35  形態素: ['さ', 'れる', '。'],\t 係り先: -1, 係り元: [34]\n",
            "--- Sentence 0002 ---\n",
            "#  0  形態素: ['『', '日本', '大', '百科全書', '(', 'ニッポニカ', ')』', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['解説', 'で', '、'],\t 係り先: 5, 係り元: [0]\n",
            "#  2  形態素: ['情報', '工学', '者', '・', '通信', '工学', '者', 'の'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['佐藤', '理', '史', 'は'],\t 係り先: 5, 係り元: [2]\n",
            "#  4  形態素: ['次', 'の', 'よう', 'に'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['述べ', 'て', 'いる', '。'],\t 係り先: -1, 係り元: [1, 3, 4]\n",
            "--- Sentence 0003 ---\n",
            "#  0  形態素: ['人間', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['知的', '能力', 'を'],\t 係り先: 3, 係り元: [0]\n",
            "#  2  形態素: ['コンピュータ', '上', 'で'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['実現', 'する', '、'],\t 係り先: 5, 係り元: [1, 2]\n",
            "#  4  形態素: ['様々', 'な'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['技術', '・', 'ソフトウェア', '・', 'コンピュータ', 'システム', '。'],\t 係り先: 19, 係り元: [3, 4]\n",
            "#  6  形態素: ['応用', '例', 'は'],\t 係り先: 19, 係り元: []\n",
            "#  7  形態素: ['自然', '言語', '処理'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['（', '機械', '翻訳', '・', 'かな漢字', '変換', '・', '構文', '解析', '等', '）', '、'],\t 係り先: 9, 係り元: [7]\n",
            "#  9  形態素: ['専門', '家', 'の'],\t 係り先: 10, 係り元: [8]\n",
            "# 10  形態素: ['推論', '・', '判断', 'を'],\t 係り先: 11, 係り元: [9]\n",
            "# 11  形態素: ['模倣', 'する'],\t 係り先: 12, 係り元: [10]\n",
            "# 12  形態素: ['エキスパート', 'システム', '、'],\t 係り先: 18, 係り元: [11]\n",
            "# 13  形態素: ['画像', 'データ', 'を'],\t 係り先: 14, 係り元: []\n",
            "# 14  形態素: ['解析', 'し', 'て'],\t 係り先: 17, 係り元: [13]\n",
            "# 15  形態素: ['特定', 'の'],\t 係り先: 16, 係り元: []\n",
            "# 16  形態素: ['パターン', 'を'],\t 係り先: 17, 係り元: [15]\n",
            "# 17  形態素: ['検出', '・', '抽出', 'し', 'たり', 'する'],\t 係り先: 18, 係り元: [14, 16]\n",
            "# 18  形態素: ['画像', '認識', '等', 'が'],\t 係り先: 19, 係り元: [12, 17]\n",
            "# 19  形態素: ['ある', '。'],\t 係り先: 23, 係り元: [5, 6, 18]\n",
            "# 20  形態素: ['1956', '年', 'に'],\t 係り先: 23, 係り元: []\n",
            "# 21  形態素: ['ダート', 'マス', '会議', 'で'],\t 係り先: 23, 係り元: []\n",
            "# 22  形態素: ['ジョン', '・', 'マッカーシー', 'により'],\t 係り先: 23, 係り元: []\n",
            "# 23  形態素: ['命名', 'さ', 'れ', 'た', '。'],\t 係り先: 35, 係り元: [19, 20, 21, 22]\n",
            "# 24  形態素: ['現在', 'で', 'は', '、'],\t 係り先: 35, 係り元: []\n",
            "# 25  形態素: ['記号', '処理', 'を'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['用い', 'た'],\t 係り先: 27, 係り元: [25]\n",
            "# 27  形態素: ['知能', 'の'],\t 係り先: 28, 係り元: [26]\n",
            "# 28  形態素: ['記述', 'を'],\t 係り先: 30, 係り元: [27]\n",
            "# 29  形態素: ['主体', 'と'],\t 係り先: 30, 係り元: []\n",
            "# 30  形態素: ['する'],\t 係り先: 32, 係り元: [28, 29]\n",
            "# 31  形態素: ['情報処理', 'や'],\t 係り先: 32, 係り元: []\n",
            "# 32  形態素: ['研究', 'で', 'の'],\t 係り先: 33, 係り元: [30, 31]\n",
            "# 33  形態素: ['アプローチ', 'という'],\t 係り先: 34, 係り元: [32]\n",
            "# 34  形態素: ['意味あい', 'でも'],\t 係り先: 35, 係り元: [33]\n",
            "# 35  形態素: ['使わ', 'れ', 'て', 'いる', '。'],\t 係り先: 43, 係り元: [23, 24, 34]\n",
            "# 36  形態素: ['家庭', '用', '電気', '機械', '器具', 'の'],\t 係り先: 37, 係り元: []\n",
            "# 37  形態素: ['制御', 'システム', 'や'],\t 係り先: 39, 係り元: [36]\n",
            "# 38  形態素: ['ゲーム', 'ソフト', 'の'],\t 係り先: 39, 係り元: []\n",
            "# 39  形態素: ['思考', 'ルーチン', 'も'],\t 係り先: 41, 係り元: [37, 38]\n",
            "# 40  形態素: ['こう'],\t 係り先: 41, 係り元: []\n",
            "# 41  形態素: ['呼ば', 'れる'],\t 係り先: 42, 係り元: [39, 40]\n",
            "# 42  形態素: ['こと', 'も'],\t 係り先: 43, 係り元: [41]\n",
            "# 43  形態素: ['ある', '。'],\t 係り先: -1, 係り元: [35, 42]\n",
            "--- Sentence 0004 ---\n",
            "#  0  形態素: ['プログラミング', '言語', 'による'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['「', '」', 'という'],\t 係り先: 2, 係り元: [0]\n",
            "#  2  形態素: ['カウンセラー', 'を'],\t 係り先: 3, 係り元: [1]\n",
            "#  3  形態素: ['模倣', 'し', 'た'],\t 係り先: 4, 係り元: [2]\n",
            "#  4  形態素: ['プログラム'],\t 係り先: 8, 係り元: [3]\n",
            "#  5  形態素: ['（', '人工', '無', '脳', '）', 'が'],\t 係り先: 8, 係り元: []\n",
            "#  6  形態素: ['しばしば'],\t 係り先: 8, 係り元: []\n",
            "#  7  形態素: ['引き合い', 'に'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['出さ', 'れる', 'が', '、'],\t 係り先: 27, 係り元: [4, 5, 6, 7]\n",
            "#  9  形態素: ['計算', '機', 'に'],\t 係り先: 13, 係り元: []\n",
            "# 10  形態素: ['人間', 'の'],\t 係り先: 11, 係り元: []\n",
            "# 11  形態素: ['専門', '家', 'の'],\t 係り先: 12, 係り元: [10]\n",
            "# 12  形態素: ['役割', 'を'],\t 係り先: 13, 係り元: [11]\n",
            "# 13  形態素: ['さ', 'せよ', 'う', 'という'],\t 係り先: 14, 係り元: [9, 12]\n",
            "# 14  形態素: ['「', 'エキスパート', 'システム', '」', 'と'],\t 係り先: 15, 係り元: [13]\n",
            "# 15  形態素: ['呼ば', 'れる'],\t 係り先: 16, 係り元: [14]\n",
            "# 16  形態素: ['研究', '・', '情報処理', 'システム', 'の'],\t 係り先: 17, 係り元: [15]\n",
            "# 17  形態素: ['実現', 'は', '、'],\t 係り先: 27, 係り元: [16]\n",
            "# 18  形態素: ['人間', 'が'],\t 係り先: 20, 係り元: []\n",
            "# 19  形態素: ['暗黙', 'に'],\t 係り先: 20, 係り元: []\n",
            "# 20  形態素: ['持つ'],\t 係り先: 21, 係り元: [18, 19]\n",
            "# 21  形態素: ['常識', 'の'],\t 係り先: 22, 係り元: [20]\n",
            "# 22  形態素: ['記述', 'が'],\t 係り先: 24, 係り元: [21]\n",
            "# 23  形態素: ['問題', 'と'],\t 係り先: 24, 係り元: []\n",
            "# 24  形態素: ['なり', '、'],\t 係り先: 27, 係り元: [22, 23]\n",
            "# 25  形態素: ['実用', 'へ', 'の'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['利用', 'が'],\t 係り先: 27, 係り元: [25]\n",
            "# 27  形態素: ['困難', '視', 'さ', 'れ', 'て', 'いる', '。'],\t 係り先: 42, 係り元: [8, 17, 24, 26]\n",
            "# 28  形態素: ['人工', '的', 'な'],\t 係り先: 29, 係り元: []\n",
            "# 29  形態素: ['知能', 'の'],\t 係り先: 30, 係り元: [28]\n",
            "# 30  形態素: ['実現', 'へ', 'の'],\t 係り先: 31, 係り元: [29]\n",
            "# 31  形態素: ['アプローチ', 'として', 'は', '、'],\t 係り先: 35, 係り元: [30]\n",
            "# 32  形態素: ['「', 'ファジィ', '理論', '」', 'や'],\t 係り先: 33, 係り元: []\n",
            "# 33  形態素: ['「', 'ニューラルネットワーク', '」', 'など', 'の', 'よう', 'な'],\t 係り先: 34, 係り元: [32]\n",
            "# 34  形態素: ['アプローチ', 'も'],\t 係り先: 35, 係り元: [33]\n",
            "# 35  形態素: ['知ら', 'れ', 'て', 'いる', 'が', '、'],\t 係り先: 42, 係り元: [31, 34]\n",
            "# 36  形態素: ['従来', 'の'],\t 係り先: 37, 係り元: []\n",
            "# 37  形態素: ['人工', '知能', 'で', 'ある'],\t 係り先: 38, 係り元: [36]\n",
            "# 38  形態素: ['(', 'Good', 'Old', 'Fashioned', 'AI', ')', 'と', 'の'],\t 係り先: 39, 係り元: [37]\n",
            "# 39  形態素: ['差', 'は'],\t 係り先: 42, 係り元: [38]\n",
            "# 40  形態素: ['記述', 'の'],\t 係り先: 41, 係り元: []\n",
            "# 41  形態素: ['記号', '的', '明示', '性', 'に'],\t 係り先: 42, 係り元: [40]\n",
            "# 42  形態素: ['ある', '。'],\t 係り先: 46, 係り元: [27, 35, 39, 41]\n",
            "# 43  形態素: ['その後'],\t 係り先: 46, 係り元: []\n",
            "# 44  形態素: ['「', 'サポートベクターマシン', '」', 'が'],\t 係り先: 46, 係り元: []\n",
            "# 45  形態素: ['注目', 'を'],\t 係り先: 46, 係り元: []\n",
            "# 46  形態素: ['集め', 'た', '。'],\t 係り先: 55, 係り元: [42, 43, 44, 45]\n",
            "# 47  形態素: ['また', '、'],\t 係り先: 55, 係り元: []\n",
            "# 48  形態素: ['自ら', 'の'],\t 係り先: 49, 係り元: []\n",
            "# 49  形態素: ['経験', 'を'],\t 係り先: 52, 係り元: [48]\n",
            "# 50  形態素: ['元', 'に'],\t 係り先: 52, 係り元: []\n",
            "# 51  形態素: ['学習', 'を'],\t 係り先: 52, 係り元: []\n",
            "# 52  形態素: ['行う'],\t 係り先: 53, 係り元: [49, 50, 51]\n",
            "# 53  形態素: ['強化', '学習', 'という'],\t 係り先: 54, 係り元: [52]\n",
            "# 54  形態素: ['手法', 'も'],\t 係り先: 55, 係り元: [53]\n",
            "# 55  形態素: ['ある', '。'],\t 係り先: 71, 係り元: [46, 47, 54]\n",
            "# 56  形態素: ['「', 'この'],\t 係り先: 57, 係り元: []\n",
            "# 57  形態素: ['宇宙', 'において', '、'],\t 係り先: 67, 係り元: [56]\n",
            "# 58  形態素: ['知性', 'と', 'は'],\t 係り先: 61, 係り元: []\n",
            "# 59  形態素: ['最も'],\t 係り先: 60, 係り元: []\n",
            "# 60  形態素: ['強力', 'な'],\t 係り先: 61, 係り元: [59]\n",
            "# 61  形態素: ['形質', 'で', 'ある'],\t 係り先: 62, 係り元: [58, 60]\n",
            "# 62  形態素: ['（', 'レイ', '・', 'カーツ', 'ワイル', '）', '」', 'という'],\t 係り先: 63, 係り元: [61]\n",
            "# 63  形態素: ['言葉', '通り', '、'],\t 係り先: 67, 係り元: [62]\n",
            "# 64  形態素: ['知性', 'を'],\t 係り先: 66, 係り元: []\n",
            "# 65  形態素: ['機械', '的', 'に'],\t 係り先: 66, 係り元: []\n",
            "# 66  形態素: ['表現', 'し'],\t 係り先: 67, 係り元: [64, 65]\n",
            "# 67  形態素: ['実装', 'する', 'という'],\t 係り先: 68, 係り元: [57, 63, 66]\n",
            "# 68  形態素: ['こと', 'は'],\t 係り先: 71, 係り元: [67]\n",
            "# 69  形態素: ['極めて'],\t 係り先: 70, 係り元: []\n",
            "# 70  形態素: ['重要', 'な'],\t 係り先: 71, 係り元: [69]\n",
            "# 71  形態素: ['作業', 'で', 'ある', '。'],\t 係り先: -1, 係り元: [55, 68, 70]\n",
            "--- Sentence 0005 ---\n",
            "#  0  形態素: ['2006', '年', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['ディープラーニング'],\t 係り先: 3, 係り元: [0]\n",
            "#  2  形態素: ['（', '深層', '学習', '）', 'の'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['登場', 'と'],\t 係り先: 7, 係り元: [1, 2]\n",
            "#  4  形態素: ['2010', '年代'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['以降', 'の'],\t 係り先: 6, 係り元: [4]\n",
            "#  6  形態素: ['ビッグ', 'データ', 'の'],\t 係り先: 7, 係り元: [5]\n",
            "#  7  形態素: ['登場', 'により', '、'],\t 係り先: 13, 係り元: [3, 6]\n",
            "#  8  形態素: ['一過', '性', 'の'],\t 係り先: 9, 係り元: []\n",
            "#  9  形態素: ['流行', 'を'],\t 係り先: 10, 係り元: [8]\n",
            "# 10  形態素: ['超え', 'て'],\t 係り先: 12, 係り元: [9]\n",
            "# 11  形態素: ['社会', 'に'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['浸透', 'し', 'て'],\t 係り先: 13, 係り元: [10, 11]\n",
            "# 13  形態素: ['行っ', 'た', '。'],\t 係り先: 36, 係り元: [7, 12]\n",
            "# 14  形態素: ['2016', '年', 'から'],\t 係り先: 15, 係り元: []\n",
            "# 15  形態素: ['2017', '年', 'にかけて', '、'],\t 係り先: 17, 係り元: [14]\n",
            "# 16  形態素: ['ディープラーニング', 'を'],\t 係り先: 17, 係り元: []\n",
            "# 17  形態素: ['導入', 'し', 'た'],\t 係り先: 18, 係り元: [15, 16]\n",
            "# 18  形態素: ['AI', 'が'],\t 係り先: 19, 係り元: [17]\n",
            "# 19  形態素: ['完全', '情報', 'ゲーム', 'で', 'ある'],\t 係り先: 20, 係り元: [18]\n",
            "# 20  形態素: ['囲碁', 'など', 'の'],\t 係り先: 21, 係り元: [19]\n",
            "# 21  形態素: ['トップ', '棋士', '、'],\t 係り先: 26, 係り元: [20]\n",
            "# 22  形態素: ['さらに'],\t 係り先: 23, 係り元: []\n",
            "# 23  形態素: ['不完全', '情報', 'ゲーム', 'で', 'ある'],\t 係り先: 24, 係り元: [22]\n",
            "# 24  形態素: ['ポーカー', 'の'],\t 係り先: 25, 係り元: [23]\n",
            "# 25  形態素: ['世界', 'トップクラス', 'の'],\t 係り先: 26, 係り元: [24]\n",
            "# 26  形態素: ['プレイヤー', 'も'],\t 係り先: 27, 係り元: [21, 25]\n",
            "# 27  形態素: ['破り', '、'],\t 係り先: 36, 係り元: [26]\n",
            "# 28  形態素: ['麻雀', 'で', 'は'],\t 係り先: 36, 係り元: []\n",
            "# 29  形態素: ['「', 'Microsoft', 'Suphx', '(', 'Super', 'Phoenix', ')」', 'が'],\t 係り先: 33, 係り元: []\n",
            "# 30  形態素: ['AI', 'として'],\t 係り先: 33, 係り元: []\n",
            "# 31  形態素: ['初めて'],\t 係り先: 33, 係り元: []\n",
            "# 32  形態素: ['十', '段', 'に'],\t 係り先: 33, 係り元: []\n",
            "# 33  形態素: ['到達', 'する', 'など', '、'],\t 係り先: 36, 係り元: [29, 30, 31, 32]\n",
            "# 34  形態素: ['時代', 'の'],\t 係り先: 35, 係り元: []\n",
            "# 35  形態素: ['最先端', '技術', 'と'],\t 係り先: 36, 係り元: [34]\n",
            "# 36  形態素: ['なっ', 'た', '。'],\t 係り先: -1, 係り元: [13, 27, 28, 33, 35]\n",
            "--- Sentence 0006 ---\n",
            "#  0  形態素: ['第', '２', '次', '人工', '知能', 'ブーム', 'で', 'の'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['人工', '知能', 'は'],\t 係り先: 3, 係り元: [0]\n",
            "#  2  形態素: ['機械', '学習', 'と'],\t 係り先: 3, 係り元: []\n",
            "#  3  形態素: ['呼ば', 'れ', '、'],\t 係り先: 6, 係り元: [1, 2]\n",
            "#  4  形態素: ['以下', 'の', 'よう', 'な'],\t 係り先: 5, 係り元: []\n",
            "#  5  形態素: ['もの', 'が'],\t 係り先: 6, 係り元: [4]\n",
            "#  6  形態素: ['ある', '。'],\t 係り先: -1, 係り元: [3, 5]\n",
            "--- Sentence 0007 ---\n",
            "#  0  形態素: ['一方', '、'],\t 係り先: 26, 係り元: []\n",
            "#  1  形態素: ['計算', '知能'],\t 係り先: 2, 係り元: []\n",
            "#  2  形態素: ['（', 'CI', '）', 'は'],\t 係り先: 12, 係り元: [1]\n",
            "#  3  形態素: ['開発', 'や'],\t 係り先: 4, 係り元: []\n",
            "#  4  形態素: ['学習', 'を'],\t 係り先: 5, 係り元: [3]\n",
            "#  5  形態素: ['繰り返す'],\t 係り先: 6, 係り元: [4]\n",
            "#  6  形態素: ['こと', 'を'],\t 係り先: 8, 係り元: [5]\n",
            "#  7  形態素: ['基本', 'と'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['し', 'て', 'いる'],\t 係り先: 9, 係り元: [6, 7]\n",
            "#  9  形態素: ['（', '例えば', '、'],\t 係り先: 12, 係り元: [8]\n",
            "# 10  形態素: ['パラメータ', '調整', '、'],\t 係り先: 12, 係り元: []\n",
            "# 11  形態素: ['コネクショニズム', 'の'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['システム', '）', '。'],\t 係り先: 21, 係り元: [2, 9, 10, 11]\n",
            "# 13  形態素: ['学習', 'は'],\t 係り先: 16, 係り元: []\n",
            "# 14  形態素: ['経験', 'に'],\t 係り先: 15, 係り元: []\n",
            "# 15  形態素: ['基づく'],\t 係り先: 16, 係り元: [14]\n",
            "# 16  形態素: ['手法', 'で', 'あり', '、'],\t 係り先: 21, 係り元: [13, 15]\n",
            "# 17  形態素: ['非', '記号', '的', 'AI', '、'],\t 係り先: 18, 係り元: []\n",
            "# 18  形態素: ['美しく', 'ない'],\t 係り先: 20, 係り元: [17]\n",
            "# 19  形態素: ['AI', '、'],\t 係り先: 20, 係り元: []\n",
            "# 20  形態素: ['ソフトコンピューティング', 'と'],\t 係り先: 21, 係り元: [18, 19]\n",
            "# 21  形態素: ['関係', 'し', 'て', 'いる', '。'],\t 係り先: 26, 係り元: [12, 16, 20]\n",
            "# 22  形態素: ['その'],\t 係り先: 23, 係り元: []\n",
            "# 23  形態素: ['手法', 'として', 'は', '、'],\t 係り先: 26, 係り元: [22]\n",
            "# 24  形態素: ['以下', 'の'],\t 係り先: 25, 係り元: []\n",
            "# 25  形態素: ['もの', 'が'],\t 係り先: 26, 係り元: [24]\n",
            "# 26  形態素: ['ある', '。'],\t 係り先: -1, 係り元: [0, 21, 23, 25]\n",
            "--- Sentence 0008 ---\n",
            "#  0  形態素: ['これら', 'を'],\t 係り先: 1, 係り元: []\n",
            "#  1  形態素: ['統合', 'し', 'た'],\t 係り先: 2, 係り元: [0]\n",
            "#  2  形態素: ['知的', 'システム', 'を'],\t 係り先: 3, 係り元: [1]\n",
            "#  3  形態素: ['作る'],\t 係り先: 4, 係り元: [2]\n",
            "#  4  形態素: ['試み', 'も'],\t 係り先: 5, 係り元: [3]\n",
            "#  5  形態素: ['なさ', 'れ', 'て', 'いる', '。'],\t 係り先: 13, 係り元: [4]\n",
            "#  6  形態素: ['ACT', '-', 'R', 'で', 'は', '、'],\t 係り先: 13, 係り元: []\n",
            "#  7  形態素: ['エキスパート', 'の'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['推論', 'ルール', 'を', '、'],\t 係り先: 13, 係り元: [7]\n",
            "#  9  形態素: ['統計', '的', '学習', 'を'],\t 係り先: 13, 係り元: []\n",
            "# 10  形態素: ['元', 'に'],\t 係り先: 13, 係り元: []\n",
            "# 11  形態素: ['ニューラルネットワーク', 'や'],\t 係り先: 12, 係り元: []\n",
            "# 12  形態素: ['生成', '規則', 'を通して'],\t 係り先: 13, 係り元: [11]\n",
            "# 13  形態素: ['生成', 'する', '。'],\t 係り先: -1, 係り元: [5, 6, 8, 9, 10, 12]\n",
            "--- Sentence 0009 ---\n",
            "#  0  形態素: ['第', '3', '次', '人工', '知能', 'ブーム', 'で', 'は', '、'],\t 係り先: 15, 係り元: []\n",
            "#  1  形態素: ['ディープラーニング', 'が'],\t 係り先: 2, 係り元: []\n",
            "#  2  形態素: ['画像', '認識', '、'],\t 係り先: 3, 係り元: [1]\n",
            "#  3  形態素: ['テキスト', '解析', '、'],\t 係り先: 4, 係り元: [2]\n",
            "#  4  形態素: ['音声', '認識', 'など'],\t 係り先: 6, 係り元: [3]\n",
            "#  5  形態素: ['様々', 'な'],\t 係り先: 6, 係り元: []\n",
            "#  6  形態素: ['領域', 'で'],\t 係り先: 9, 係り元: [4, 5]\n",
            "#  7  形態素: ['第', '2', '次', '人工', '知能', 'ブーム', 'の'],\t 係り先: 8, 係り元: []\n",
            "#  8  形態素: ['人工', '知能', 'を'],\t 係り先: 9, 係り元: [7]\n",
            "#  9  形態素: ['上回る'],\t 係り先: 10, 係り元: [6, 8]\n",
            "# 10  形態素: ['精度', 'を'],\t 係り先: 11, 係り元: [9]\n",
            "# 11  形態素: ['出し', 'て', 'おり', '、'],\t 係り先: 15, 係り元: [10]\n",
            "# 12  形態素: ['ディープラーニング', 'の'],\t 係り先: 13, 係り元: []\n",
            "# 13  形態素: ['研究', 'が'],\t 係り先: 15, 係り元: [12]\n",
            "# 14  形態素: ['盛ん', 'に'],\t 係り先: 15, 係り元: []\n",
            "# 15  形態素: ['行わ', 'れ', 'て', 'いる', '。'],\t 係り先: 26, 係り元: [0, 11, 13, 14]\n",
            "# 16  形態素: ['最近', 'で', 'は', '、'],\t 係り先: 26, 係り元: []\n",
            "# 17  形態素: ['DQN', '、'],\t 係り先: 18, 係り元: []\n",
            "# 18  形態素: ['CNN', '、'],\t 係り先: 19, 係り元: [17]\n",
            "# 19  形態素: ['RNN', '、'],\t 係り先: 20, 係り元: [18]\n",
            "# 20  形態素: ['GAN', 'と'],\t 係り先: 22, 係り元: [19]\n",
            "# 21  形態素: ['様々', 'な'],\t 係り先: 22, 係り元: []\n",
            "# 22  形態素: ['ディープラーニング', 'の'],\t 係り先: 23, 係り元: [20, 21]\n",
            "# 23  形態素: ['派生', 'が'],\t 係り先: 24, 係り元: [22]\n",
            "# 24  形態素: ['で', 'て'],\t 係り先: 26, 係り元: [23]\n",
            "# 25  形態素: ['各', '分野', 'で'],\t 係り先: 26, 係り元: []\n",
            "# 26  形態素: ['活躍', 'し', 'て', 'いる', '。'],\t 係り先: 42, 係り元: [15, 16, 24, 25]\n",
            "# 27  形態素: ['特に', '、'],\t 係り先: 42, 係り元: []\n",
            "# 28  形態素: ['GAN'],\t 係り先: 29, 係り元: []\n",
            "# 29  形態素: ['（', '敵対', '的', '生成', 'ネットワーク', '）', 'は', '、'],\t 係り先: 42, 係り元: [28]\n",
            "# 30  形態素: ['ディープラーニング', 'が'],\t 係り先: 35, 係り元: []\n",
            "# 31  形態素: ['認識', 'や'],\t 係り先: 32, 係り元: []\n",
            "# 32  形態素: ['予測', 'など', 'の'],\t 係り先: 33, 係り元: [31]\n",
            "# 33  形態素: ['分野', 'で'],\t 係り先: 35, 係り元: [32]\n",
            "# 34  形態素: ['成果', 'を'],\t 係り先: 35, 係り元: []\n",
            "# 35  形態素: ['だし', 'て', 'いる'],\t 係り先: 36, 係り元: [30, 33, 34]\n",
            "# 36  形態素: ['こと', 'に'],\t 係り先: 37, 係り元: [35]\n",
            "# 37  形態素: ['加え', 'て', '、'],\t 係り先: 42, 係り元: [36]\n",
            "# 38  形態素: ['画像', 'の'],\t 係り先: 39, 係り元: []\n",
            "# 39  形態素: ['生成', '技術', 'において'],\t 係り先: 42, 係り元: [38]\n",
            "# 40  形態素: ['大きな'],\t 係り先: 41, 係り元: []\n",
            "# 41  形態素: ['進化', 'を'],\t 係り先: 42, 係り元: [40]\n",
            "# 42  形態素: ['見せ', 'て', 'いる', '。'],\t 係り先: 55, 係り元: [26, 27, 29, 37, 39, 41]\n",
            "# 43  形態素: ['森', '正弥', 'は'],\t 係り先: 50, 係り元: []\n",
            "# 44  形態素: ['これら', 'の'],\t 係り先: 45, 係り元: []\n",
            "# 45  形態素: ['成果', 'を'],\t 係り先: 46, 係り元: [44]\n",
            "# 46  形態素: ['背景', 'に', '、'],\t 係り先: 50, 係り元: [45]\n",
            "# 47  形態素: ['従来', 'の'],\t 係り先: 48, 係り元: []\n",
            "# 48  形態素: ['人工', '知能', 'の'],\t 係り先: 49, 係り元: [47]\n",
            "# 49  形態素: ['応用', '分野', 'が'],\t 係り先: 50, 係り元: [48]\n",
            "# 50  形態素: ['広がっ', 'て', 'おり', '、'],\t 係り先: 55, 係り元: [43, 46, 49]\n",
            "# 51  形態素: ['Creative', 'AI', 'という'],\t 係り先: 52, 係り元: []\n",
            "# 52  形態素: ['コンテンツ', '生成', 'を'],\t 係り先: 53, 係り元: [51]\n",
            "# 53  形態素: ['行っ', 'て', 'いく'],\t 係り先: 54, 係り元: [52]\n",
            "# 54  形態素: ['応用', 'も'],\t 係り先: 55, 係り元: [53]\n",
            "# 55  形態素: ['始まっ', 'て', 'いる', 'と'],\t 係り先: 56, 係り元: [42, 50, 54]\n",
            "# 56  形態素: ['指摘', 'し', 'て', 'いる', '。'],\t 係り先: -1, 係り元: [55]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 42. 係り元と係り先の文節の表示\n",
        "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
        "\n",
        "> 41番が出来ていればforで探索してやるだけ！\n",
        "\n",
        "> 記号は指示があったからif内包で省いたものの省かなくても良い気がするな…"
      ],
      "metadata": {
        "id": "EkCDSwTMGCvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "\"\"\"\n",
        "# 確認\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:5]:\n",
        "    print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "\"\"\"\n",
        "\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if c.dst == -1:\n",
        "      continue\n",
        "    kakari_moto_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs])\n",
        "    kakari_saki_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[c.dst].morphs])\n",
        "    print(f\"#{c.num:3g}  係り元 : {kakari_moto_text} \\t->\\t 係り先 : {kakari_saki_text}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G_OM9atUGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e1e6ec-f325-49a9-9cdf-03c99cc8772e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "--- Sentence 0001 ---\n",
            "#  0  係り元 : 人工知能 \t->\t 係り先 : 語\n",
            "#  1  係り元 : じんこうちのう \t->\t 係り先 : 語\n",
            "#  2  係り元 : AI \t->\t 係り先 : エーアイとは\n",
            "#  3  係り元 : エーアイとは \t->\t 係り先 : 語\n",
            "#  4  係り元 : 計算 \t->\t 係り先 : という\n",
            "#  5  係り元 : という \t->\t 係り先 : 道具を\n",
            "#  6  係り元 : 概念と \t->\t 係り先 : 道具を\n",
            "#  7  係り元 : コンピュータ \t->\t 係り先 : という\n",
            "#  8  係り元 : という \t->\t 係り先 : 道具を\n",
            "#  9  係り元 : 道具を \t->\t 係り先 : 用いて\n",
            "# 10  係り元 : 用いて \t->\t 係り先 : 研究する\n",
            "# 11  係り元 : 知能を \t->\t 係り先 : 研究する\n",
            "# 12  係り元 : 研究する \t->\t 係り先 : 計算機科学\n",
            "# 13  係り元 : 計算機科学 \t->\t 係り先 : の\n",
            "# 14  係り元 : の \t->\t 係り先 : 一分野を\n",
            "# 15  係り元 : 一分野を \t->\t 係り先 : 指す\n",
            "# 16  係り元 : 指す \t->\t 係り先 : 語\n",
            "# 17  係り元 : 語 \t->\t 係り先 : 研究分野とも\n",
            "# 18  係り元 : 言語の \t->\t 係り先 : 推論\n",
            "# 19  係り元 : 理解や \t->\t 係り先 : 推論\n",
            "# 20  係り元 : 推論 \t->\t 係り先 : 問題解決などの\n",
            "# 21  係り元 : 問題解決などの \t->\t 係り先 : 知的行動を\n",
            "# 22  係り元 : 知的行動を \t->\t 係り先 : 代わって\n",
            "# 23  係り元 : 人間に \t->\t 係り先 : 代わって\n",
            "# 24  係り元 : 代わって \t->\t 係り先 : 行わせる\n",
            "# 25  係り元 : コンピューターに \t->\t 係り先 : 行わせる\n",
            "# 26  係り元 : 行わせる \t->\t 係り先 : 技術または\n",
            "# 27  係り元 : 技術または \t->\t 係り先 : 研究分野とも\n",
            "# 28  係り元 : 計算機 \t->\t 係り先 : コンピュータによる\n",
            "# 29  係り元 : コンピュータによる \t->\t 係り先 : 情報処理システムの\n",
            "# 30  係り元 : 知的な \t->\t 係り先 : 情報処理システムの\n",
            "# 31  係り元 : 情報処理システムの \t->\t 係り先 : 実現に関する\n",
            "# 32  係り元 : 設計や \t->\t 係り先 : 実現に関する\n",
            "# 33  係り元 : 実現に関する \t->\t 係り先 : 研究分野とも\n",
            "# 34  係り元 : 研究分野とも \t->\t 係り先 : される\n",
            "--- Sentence 0002 ---\n",
            "#  0  係り元 : 日本大百科全書(ニッポニカ)』の \t->\t 係り先 : 解説で\n",
            "#  1  係り元 : 解説で \t->\t 係り先 : 述べている\n",
            "#  2  係り元 : 情報工学者通信工学者の \t->\t 係り先 : 佐藤理史は\n",
            "#  3  係り元 : 佐藤理史は \t->\t 係り先 : 述べている\n",
            "#  4  係り元 : 次のように \t->\t 係り先 : 述べている\n",
            "--- Sentence 0003 ---\n",
            "#  0  係り元 : 人間の \t->\t 係り先 : 知的能力を\n",
            "#  1  係り元 : 知的能力を \t->\t 係り先 : 実現する\n",
            "#  2  係り元 : コンピュータ上で \t->\t 係り先 : 実現する\n",
            "#  3  係り元 : 実現する \t->\t 係り先 : 技術ソフトウェアコンピュータシステム\n",
            "#  4  係り元 : 様々な \t->\t 係り先 : 技術ソフトウェアコンピュータシステム\n",
            "#  5  係り元 : 技術ソフトウェアコンピュータシステム \t->\t 係り先 : ある\n",
            "#  6  係り元 : 応用例は \t->\t 係り先 : ある\n",
            "#  7  係り元 : 自然言語処理 \t->\t 係り先 : 機械翻訳かな漢字変換構文解析等\n",
            "#  8  係り元 : 機械翻訳かな漢字変換構文解析等 \t->\t 係り先 : 専門家の\n",
            "#  9  係り元 : 専門家の \t->\t 係り先 : 推論判断を\n",
            "# 10  係り元 : 推論判断を \t->\t 係り先 : 模倣する\n",
            "# 11  係り元 : 模倣する \t->\t 係り先 : エキスパートシステム\n",
            "# 12  係り元 : エキスパートシステム \t->\t 係り先 : 画像認識等が\n",
            "# 13  係り元 : 画像データを \t->\t 係り先 : 解析して\n",
            "# 14  係り元 : 解析して \t->\t 係り先 : 検出抽出したりする\n",
            "# 15  係り元 : 特定の \t->\t 係り先 : パターンを\n",
            "# 16  係り元 : パターンを \t->\t 係り先 : 検出抽出したりする\n",
            "# 17  係り元 : 検出抽出したりする \t->\t 係り先 : 画像認識等が\n",
            "# 18  係り元 : 画像認識等が \t->\t 係り先 : ある\n",
            "# 19  係り元 : ある \t->\t 係り先 : 命名された\n",
            "# 20  係り元 : 1956年に \t->\t 係り先 : 命名された\n",
            "# 21  係り元 : ダートマス会議で \t->\t 係り先 : 命名された\n",
            "# 22  係り元 : ジョンマッカーシーにより \t->\t 係り先 : 命名された\n",
            "# 23  係り元 : 命名された \t->\t 係り先 : 使われている\n",
            "# 24  係り元 : 現在では \t->\t 係り先 : 使われている\n",
            "# 25  係り元 : 記号処理を \t->\t 係り先 : 用いた\n",
            "# 26  係り元 : 用いた \t->\t 係り先 : 知能の\n",
            "# 27  係り元 : 知能の \t->\t 係り先 : 記述を\n",
            "# 28  係り元 : 記述を \t->\t 係り先 : する\n",
            "# 29  係り元 : 主体と \t->\t 係り先 : する\n",
            "# 30  係り元 : する \t->\t 係り先 : 研究での\n",
            "# 31  係り元 : 情報処理や \t->\t 係り先 : 研究での\n",
            "# 32  係り元 : 研究での \t->\t 係り先 : アプローチという\n",
            "# 33  係り元 : アプローチという \t->\t 係り先 : 意味あいでも\n",
            "# 34  係り元 : 意味あいでも \t->\t 係り先 : 使われている\n",
            "# 35  係り元 : 使われている \t->\t 係り先 : ある\n",
            "# 36  係り元 : 家庭用電気機械器具の \t->\t 係り先 : 制御システムや\n",
            "# 37  係り元 : 制御システムや \t->\t 係り先 : 思考ルーチンも\n",
            "# 38  係り元 : ゲームソフトの \t->\t 係り先 : 思考ルーチンも\n",
            "# 39  係り元 : 思考ルーチンも \t->\t 係り先 : 呼ばれる\n",
            "# 40  係り元 : こう \t->\t 係り先 : 呼ばれる\n",
            "# 41  係り元 : 呼ばれる \t->\t 係り先 : ことも\n",
            "# 42  係り元 : ことも \t->\t 係り先 : ある\n",
            "--- Sentence 0004 ---\n",
            "#  0  係り元 : プログラミング言語による \t->\t 係り先 : という\n",
            "#  1  係り元 : という \t->\t 係り先 : カウンセラーを\n",
            "#  2  係り元 : カウンセラーを \t->\t 係り先 : 模倣した\n",
            "#  3  係り元 : 模倣した \t->\t 係り先 : プログラム\n",
            "#  4  係り元 : プログラム \t->\t 係り先 : 出されるが\n",
            "#  5  係り元 : 人工無脳が \t->\t 係り先 : 出されるが\n",
            "#  6  係り元 : しばしば \t->\t 係り先 : 出されるが\n",
            "#  7  係り元 : 引き合いに \t->\t 係り先 : 出されるが\n",
            "#  8  係り元 : 出されるが \t->\t 係り先 : 困難視されている\n",
            "#  9  係り元 : 計算機に \t->\t 係り先 : させようという\n",
            "# 10  係り元 : 人間の \t->\t 係り先 : 専門家の\n",
            "# 11  係り元 : 専門家の \t->\t 係り先 : 役割を\n",
            "# 12  係り元 : 役割を \t->\t 係り先 : させようという\n",
            "# 13  係り元 : させようという \t->\t 係り先 : エキスパートシステムと\n",
            "# 14  係り元 : エキスパートシステムと \t->\t 係り先 : 呼ばれる\n",
            "# 15  係り元 : 呼ばれる \t->\t 係り先 : 研究情報処理システムの\n",
            "# 16  係り元 : 研究情報処理システムの \t->\t 係り先 : 実現は\n",
            "# 17  係り元 : 実現は \t->\t 係り先 : 困難視されている\n",
            "# 18  係り元 : 人間が \t->\t 係り先 : 持つ\n",
            "# 19  係り元 : 暗黙に \t->\t 係り先 : 持つ\n",
            "# 20  係り元 : 持つ \t->\t 係り先 : 常識の\n",
            "# 21  係り元 : 常識の \t->\t 係り先 : 記述が\n",
            "# 22  係り元 : 記述が \t->\t 係り先 : なり\n",
            "# 23  係り元 : 問題と \t->\t 係り先 : なり\n",
            "# 24  係り元 : なり \t->\t 係り先 : 困難視されている\n",
            "# 25  係り元 : 実用への \t->\t 係り先 : 利用が\n",
            "# 26  係り元 : 利用が \t->\t 係り先 : 困難視されている\n",
            "# 27  係り元 : 困難視されている \t->\t 係り先 : ある\n",
            "# 28  係り元 : 人工的な \t->\t 係り先 : 知能の\n",
            "# 29  係り元 : 知能の \t->\t 係り先 : 実現への\n",
            "# 30  係り元 : 実現への \t->\t 係り先 : アプローチとしては\n",
            "# 31  係り元 : アプローチとしては \t->\t 係り先 : 知られているが\n",
            "# 32  係り元 : ファジィ理論や \t->\t 係り先 : ニューラルネットワークなどのような\n",
            "# 33  係り元 : ニューラルネットワークなどのような \t->\t 係り先 : アプローチも\n",
            "# 34  係り元 : アプローチも \t->\t 係り先 : 知られているが\n",
            "# 35  係り元 : 知られているが \t->\t 係り先 : ある\n",
            "# 36  係り元 : 従来の \t->\t 係り先 : 人工知能である\n",
            "# 37  係り元 : 人工知能である \t->\t 係り先 : (GoodOldFashionedAI)との\n",
            "# 38  係り元 : (GoodOldFashionedAI)との \t->\t 係り先 : 差は\n",
            "# 39  係り元 : 差は \t->\t 係り先 : ある\n",
            "# 40  係り元 : 記述の \t->\t 係り先 : 記号的明示性に\n",
            "# 41  係り元 : 記号的明示性に \t->\t 係り先 : ある\n",
            "# 42  係り元 : ある \t->\t 係り先 : 集めた\n",
            "# 43  係り元 : その後 \t->\t 係り先 : 集めた\n",
            "# 44  係り元 : サポートベクターマシンが \t->\t 係り先 : 集めた\n",
            "# 45  係り元 : 注目を \t->\t 係り先 : 集めた\n",
            "# 46  係り元 : 集めた \t->\t 係り先 : ある\n",
            "# 47  係り元 : また \t->\t 係り先 : ある\n",
            "# 48  係り元 : 自らの \t->\t 係り先 : 経験を\n",
            "# 49  係り元 : 経験を \t->\t 係り先 : 行う\n",
            "# 50  係り元 : 元に \t->\t 係り先 : 行う\n",
            "# 51  係り元 : 学習を \t->\t 係り先 : 行う\n",
            "# 52  係り元 : 行う \t->\t 係り先 : 強化学習という\n",
            "# 53  係り元 : 強化学習という \t->\t 係り先 : 手法も\n",
            "# 54  係り元 : 手法も \t->\t 係り先 : ある\n",
            "# 55  係り元 : ある \t->\t 係り先 : 作業である\n",
            "# 56  係り元 : この \t->\t 係り先 : 宇宙において\n",
            "# 57  係り元 : 宇宙において \t->\t 係り先 : 実装するという\n",
            "# 58  係り元 : 知性とは \t->\t 係り先 : 形質である\n",
            "# 59  係り元 : 最も \t->\t 係り先 : 強力な\n",
            "# 60  係り元 : 強力な \t->\t 係り先 : 形質である\n",
            "# 61  係り元 : 形質である \t->\t 係り先 : レイカーツワイルという\n",
            "# 62  係り元 : レイカーツワイルという \t->\t 係り先 : 言葉通り\n",
            "# 63  係り元 : 言葉通り \t->\t 係り先 : 実装するという\n",
            "# 64  係り元 : 知性を \t->\t 係り先 : 表現し\n",
            "# 65  係り元 : 機械的に \t->\t 係り先 : 表現し\n",
            "# 66  係り元 : 表現し \t->\t 係り先 : 実装するという\n",
            "# 67  係り元 : 実装するという \t->\t 係り先 : ことは\n",
            "# 68  係り元 : ことは \t->\t 係り先 : 作業である\n",
            "# 69  係り元 : 極めて \t->\t 係り先 : 重要な\n",
            "# 70  係り元 : 重要な \t->\t 係り先 : 作業である\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
        "\n",
        "> やるだけ"
      ],
      "metadata": {
        "id": "tki4hy54GCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:5]:\n",
        "    kakari_moto = c\n",
        "    kakari_saki = sentence[c.dst]\n",
        "    #print(\"名詞\" in [m.pos for m in kakari_moto.morphs])\n",
        "    #print(\"動詞\" in [m.pos for m in kakari_saki.morphs])\n",
        "    if (\"名詞\" in [m.pos for m in kakari_moto.morphs]) \\\n",
        "        and (\"動詞\" in [m.pos for m in kakari_saki.morphs]):\n",
        "      kakari_moto_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs])\n",
        "      kakari_saki_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[c.dst].morphs])\n",
        "      print(f\"#{c.num:3g}  係り元 : {kakari_moto_text} \\t->\\t 係り先 : {kakari_saki_text}\")\n"
      ],
      "metadata": {
        "id": "9o102UtqGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa99cf1-8c37-43fd-d8ac-6bd6df916ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0000 ---\n",
            "--- Sentence 0001 ---\n",
            "--- Sentence 0002 ---\n",
            "#  1  係り元 : 解説で \t->\t 係り先 : 述べている\n",
            "#  3  係り元 : 佐藤理史は \t->\t 係り先 : 述べている\n",
            "#  4  係り元 : 次のように \t->\t 係り先 : 述べている\n",
            "--- Sentence 0003 ---\n",
            "#  1  係り元 : 知的能力を \t->\t 係り先 : 実現する\n",
            "#  2  係り元 : コンピュータ上で \t->\t 係り先 : 実現する\n",
            "--- Sentence 0004 ---\n",
            "#  2  係り元 : カウンセラーを \t->\t 係り先 : 模倣した\n",
            "#  4  係り元 : プログラム \t->\t 係り先 : 出されるが\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44. 係り受け木の可視化\n",
        "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．\n",
        "\n",
        "> 旧問題文では次のようになっている\n",
        "\n",
        "    可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．\n",
        "    また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．\n",
        "\n",
        "以下、必要な準備"
      ],
      "metadata": {
        "id": "tZR-DWmxGCvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ライブラリの導入\n",
        "`GrapghViz` と `pydot` を導入する。`pydot`はpython3互換の`pydot-ng`というライブラリがあるのでそちらを使用。\n",
        "- `GraphViz` : DOT言語のグラフ情報から画像を作るツール。\n",
        "- `pydot-ng` : GraphVizを使うためのライブラリ。\n",
        "  - `pyparsing` : pydot-ngの依存で、DOT言語を解析するためのライブラリ。\n",
        "\n",
        "> - 全部pipでインストール出来る\n",
        "> - なんならインストールしようと思ったらGoogle Colabolatory標準でインストールされてた……。\n",
        "\n",
        "\n",
        "#### 係り受け木 is 何？？？\n",
        "単語の係り受け関係を有向グラフとして表現したもの…多分！\n",
        "\n",
        "> 例 : [滝沢カレンの理解不能な文章を言語解析してみた。| Qiita](https://qiita.com/naoyu822/items/9d7a83879c161573f63c)\n",
        "\n"
      ],
      "metadata": {
        "id": "DntCsTHXuSnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DOT言語の表現ステップ\n",
        "基本的には以下の3ステップで行う\n",
        "\n",
        "> 参考 : [【Python】Graphviz の使い方とキーワードマップを描くコード例【Windows】 | シラベルノート](https://srbrnote.work/archives/4205)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_gcQUPXiCFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ステップ\n",
        "1. キーワードリストを用意\n",
        "  - List形式で`[A, B]`という要素を与えると`A-B`にノードとエッジのあるグラフを表現している\n",
        "\n",
        "2. graphvizでDOT言語データを作成\n",
        "  - コレに関しては参考リンクのものをほぼパクってます…\n",
        "\n",
        "\n",
        "    # DOT言語ファイルのファイル名を決める\n",
        "    dot_filename = 'keyword_map'\n",
        "    # DOT言語ファイルと画像の保存フォルダを決める\n",
        "    datas_dir = r'***\\datas'\n",
        "    # ダイグラフ Digraph のインスタンスを作成\n",
        "    dot = graphviz.Digraph(\n",
        "        # name=None,\n",
        "        comment='Graphvizでキーワードマップを作図する',\n",
        "        filename=dot_filename, # DOT言語ファイルのファイル名 (これがグラフ画像のファイル名にも使われる)\n",
        "        directory=datas_dir, # DOT言語ファイルと画像を保存するフォルダ\n",
        "        format='png', # グラフの保存形式\n",
        "        engine='dot',\n",
        "        # encoding='utf-8',\n",
        "        # graph_attr=(('fontname', 'MS Gothic'), ),\n",
        "        # node_attr=(('fontname', 'MS Gothic'), ('shape', 'box'), ('color', 'blue'), ('style', 'rounded')),\n",
        "        # edge_attr=(('fontname', 'MS Gothic'), ('penwidth', '1.5'), ('color', 'gray')),\n",
        "        # body=None,\n",
        "        # strict=False,\n",
        "        )\n",
        "    # フォントの名前 (英語名を使えばOK)\n",
        "    fontname = 'MS Gothic'\n",
        "    # グラフ フォントを指定する\n",
        "    dot.attr('graph', fontname=fontname)\n",
        "    # ノード フォント、枠線の形、枠線の色、枠線のスタイルを指定する\n",
        "    dot.attr('node', fontname=fontname, shape='box', color='blue', style='rounded')\n",
        "    # エッジ フォント、矢印の太さ、矢印の色を指定する\n",
        "    dot.attr('edge', fontname=fontname, penwidth='1.5', color='gray')\n",
        "    # キーワードを追加する\n",
        "    for ks in keyword_list:\n",
        "        # キーワードの数が2個未満のときは、ノードとして追加。\n",
        "        if len(ks) < 2:\n",
        "            for k in ks:\n",
        "                dot.node(k)\n",
        "        else:\n",
        "            # そうでなければ、エッジに追加。\n",
        "            for i in range(len(ks) - 1):\n",
        "                dot.edge(ks[i], ks[i+1])\n",
        "\n",
        "3. DOT言語ファイルをグラフ画像にする\n",
        "\n",
        "\n",
        "    # 先の graphviz.Digraph() で filename と directory を指定したので、ここは引数なしでOK。\n",
        "    # もし、graphviz.Digraph() で filename と directory を指定していなければ、ここで指定すればOK。\n",
        "    dot.render(\n",
        "        # filename=None,\n",
        "        # directory=None,\n",
        "        # view=False,\n",
        "        # cleanup=False,\n",
        "        # format=None,\n",
        "        # renderer=None,\n",
        "        # formatter=None,\n",
        "        # quiet=False,\n",
        "        # quiet_view=False,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "rmrfytcAh9fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### グラフの有向化\n",
        "`directed=True` オプションを付与することで有向グラフ化できる。方向は[A,B]のとき、A->B方向に付けられる\n",
        "\n",
        "    graph = pydot.graph_from_edges(edges, directed=True)\n",
        "\n",
        "\n",
        "#### dotファイルをインライン表示する\n",
        "以下のコマンドでインライン表示出来る\n",
        "\n",
        "    import pydot_ng\n",
        "    from IPython.display import Image\n",
        "\n",
        "    graph = <グラフの作成>\n",
        "    Image(graph.create_png())\n",
        "\n",
        "> [参考 : 決定木とかグラフ構造（dotファイル）をJupyter上で表示する | 粉末@それは風のように (日記)](https://funmatu.wordpress.com/2017/04/25/%E6%B1%BA%E5%AE%9A%E6%9C%A8%E3%81%A8%E3%81%8B%E3%82%B0%E3%83%A9%E3%83%95%E6%A7%8B%E9%80%A0%EF%BC%88dot%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%EF%BC%89%E3%82%92jupyter%E4%B8%8A%E3%81%A7%E8%A1%A8%E7%A4%BA/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DE2s5d33h1oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydot-ng\n",
        "!pip install graphviz\n",
        "!pip show pydot-ng\n",
        "!pip show graphviz\n",
        "!dot -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GpDjkZjvXPi",
        "outputId": "be2f33b4-cadc-4fbc-a550-f3613b5b6688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydot-ng in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pydot-ng) (3.0.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Name: pydot-ng\n",
            "Version: 2.0.0\n",
            "Summary: Python interface to Graphviz's Dot\n",
            "Home-page: https://github.com/pydot/pydot-ng\n",
            "Author: Ero Carrera\n",
            "Author-email: ero@dkbza.org\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: pyparsing\n",
            "Required-by: \n",
            "Name: graphviz\n",
            "Version: 0.10.1\n",
            "Summary: Simple Python interface for Graphviz\n",
            "Home-page: https://github.com/xflr6/graphviz\n",
            "Author: Sebastian Bank\n",
            "Author-email: sebastian.bank@uni-leipzig.de\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "dot - graphviz version 2.40.1 (20161225.0304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "import pydot_ng as pydot\n",
        "# グラフ表示用\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "edges=[(1,2), (1,3), (1,4), (3,4)]\n",
        "g=pydot.graph_from_edges(edges)\n",
        "#g.write_jpeg('graph_from_edges_dot.jpg', prog='dot') \n",
        "Image(g.create_png())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "sT44T9YtAKXY",
        "outputId": "beb5f5c0-b695-46a4-b71d-ddeb26227187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44 回答\n",
        "\n",
        "- 他の方々の回答同様に、係り受けの組をstrとしてListを作り、`pydot.graph_from_edges()`に投げる形をとっているのだが、なぜかグラフ作成で\n",
        "  > `Warning: node 2, port AI unrecognized`\n",
        "\n",
        "  という警告が出て描画されない。\n",
        "  - ちなみに他のプログラムを流用したら同じ形式にもかかわらず警告は出なかった\n",
        "\n",
        "  > $[SentenceList[ChunkList[係り受けの組(Str)]]]$\n",
        "<br>という感じの構造\n",
        "\n",
        "- あとこれは散々既出のようだが日本語が表示されないらしい。\n",
        "\n",
        "> 少し先見たら木構造のグラフ作成するのはこの問題だけみたいなのでもうコレでいいということにします……\n",
        "  「グラフを作ること」で躓くのは問題の本質ではないので…"
      ],
      "metadata": {
        "id": "1Q1TeJ6M_ZY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "import pydot_ng as pydot\n",
        "import graphviz\n",
        "\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "\n",
        "C_edges = []\n",
        "\n",
        "\n",
        "for i,sentence in enumerate(all_text[1:2]):\n",
        "  #print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if c.dst == -1:\n",
        "      continue\n",
        "    kakari_moto_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in c.morphs])\n",
        "    kakari_saki_text = \"\".join([(m.surface if m.pos!=\"記号\" else \"\") for m in sentence[c.dst].morphs])\n",
        "    #print(f\"#{c.num:3g}  係り元 : {kakari_moto_text} \\t->\\t 係り先 : {kakari_saki_text}\")\n",
        "    C_edges.append([\n",
        "               f\"{c.num}:{kakari_moto_text}\",\n",
        "               f\"{sentence[c.dst].num}:{kakari_saki_text}\"\n",
        "              ])\n",
        "  print(C_edges)\n",
        "  print(type(C_edges))\n",
        "  n = pydot.Node('node')\n",
        "  n.fontname = 'IPAGothic'\n",
        "  C_Graph = pydot.graph_from_edges(C_edges, directed=True)\n",
        "  C_Graph.add_node(n)\n",
        "  C_Graph.write_png(\"C_G.png\")\n",
        "  Image(C_Graph.create_png())\n",
        "  \"\"\"\n",
        "  display_png(Image('./C_G.png'))\n",
        "  \"\"\"\n",
        "  C_edges = []\n"
      ],
      "metadata": {
        "id": "3NJui4rdGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "c6c5a0da-3852-40bc-9b23-74dbc1d481a6"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['0:人工知能', '17:語'], ['1:じんこうちのう', '17:語'], ['2:AI', '3:エーアイとは'], ['3:エーアイとは', '17:語'], ['4:計算', '5:という'], ['5:という', '9:道具を'], ['6:概念と', '9:道具を'], ['7:コンピュータ', '8:という'], ['8:という', '9:道具を'], ['9:道具を', '10:用いて'], ['10:用いて', '12:研究する'], ['11:知能を', '12:研究する'], ['12:研究する', '13:計算機科学'], ['13:計算機科学', '14:の'], ['14:の', '15:一分野を'], ['15:一分野を', '16:指す'], ['16:指す', '17:語'], ['17:語', '34:研究分野とも'], ['18:言語の', '20:推論'], ['19:理解や', '20:推論'], ['20:推論', '21:問題解決などの'], ['21:問題解決などの', '22:知的行動を'], ['22:知的行動を', '24:代わって'], ['23:人間に', '24:代わって'], ['24:代わって', '26:行わせる'], ['25:コンピューターに', '26:行わせる'], ['26:行わせる', '27:技術または'], ['27:技術または', '34:研究分野とも'], ['28:計算機', '29:コンピュータによる'], ['29:コンピュータによる', '31:情報処理システムの'], ['30:知的な', '31:情報処理システムの'], ['31:情報処理システムの', '33:実現に関する'], ['32:設計や', '33:実現に関する'], ['33:実現に関する', '34:研究分野とも'], ['34:研究分野とも', '35:される']]\n",
            "<class 'list'>\n",
            "Warning: node 2, port AI unrecognized\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-9ea07b03a1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mC_Graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0mC_Graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C_G.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_Graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m   \"\"\"\n\u001b[1;32m     99\u001b[0m   \u001b[0mdisplay_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./C_G.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 45. 動詞の格パターンの抽出\n",
        "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
        "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
        "- 述語に係る助詞を格とする\n",
        "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "    作り出す\tで は を\n",
        "\n",
        "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
        "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
        "\n",
        "> 実際やることは43番と同じなので、やるだけ\n",
        "\n",
        "> いい加減に覚えたいメモ\n",
        "\n",
        "#### pandasで特定の要素を抽出したい(それ以外を消したい)時\n",
        "- `df[df[\"column\"]==hoge]` で抽出できる\n",
        "  - `df[\"column\"]==hoge` でcolumn列の値がhogeの場所のみTrueとなるdfができるのでそれでマスクしている\n",
        "\n",
        "#### pandasで要素の頻度を見たい時\n",
        "- `df.value_counts(\"column\")` でcolumn列の要素を集計できる"
      ],
      "metadata": {
        "id": "KsTUgQ7qGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Doushi_Kaku = []\n",
        "\n",
        "for i,sentence in enumerate(all_text[:]):\n",
        "  #print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if (\"動詞\" in [m.pos for m in c.morphs]):  # 文節に動詞を含む場合のみ\n",
        "      kakari_moto_num = c.srcs\n",
        "      for c_joshi in [sentence[c_num] for c_num in kakari_moto_num]:\n",
        "        kakari_moto_joshi = \" \".join([m.surface for m in c_joshi.morphs if m.pos==\"助詞\"])\n",
        "      doushi_text = [m.base for m in c.morphs if m.pos==\"動詞\"][0]\n",
        "      #print(f\"#{c.num:3g} \\t 係り先 : {doushi_text} \\t 係り元 : {kakari_moto_joshi}\")\n",
        "      output_Doushi_Kaku.append(f\"{doushi_text}\\t{kakari_moto_joshi}\")\n",
        "\n",
        "with open(\"output_Doushi_Kaku.txt\",mode=\"w\") as out:\n",
        "  for line in output_Doushi_Kaku:\n",
        "    out.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "grwlaBb9GCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認用(UNIXコマンドじゃないけど許して～～～)\n",
        "import pandas as pd\n",
        "with open(\"output_Doushi_Kaku.txt\",mode=\"r\") as f:\n",
        "  df = pd.read_table(f, names=[\"動詞\",\"助詞\"])\n",
        "\n",
        "print(df)\n",
        "\n",
        "for V in [\"行う\",\"なる\",\"与える\"]:\n",
        "  df_count = df[df[\"動詞\"]==V]\n",
        "  print(f\"--- 動詞「{V}」にかかる助詞の出現頻度 ---\")\n",
        "  print(df_count.value_counts(\"助詞\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iKLikjHMYEv",
        "outputId": "866ed6fd-9c2f-4764-bfd5-f5110d8acf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      動詞     助詞\n",
            "0    用いる      を\n",
            "1     する      を\n",
            "2     指す      を\n",
            "3    代わる      に\n",
            "4     行う      に\n",
            "..   ...    ...\n",
            "703   思う  に と は\n",
            "704   つく      が\n",
            "705   する    か と\n",
            "706   つく      は\n",
            "707  答える    ね と\n",
            "\n",
            "[708 rows x 2 columns]\n",
            "--- 動詞「行う」にかかる助詞の出現頻度 ---\n",
            "助詞\n",
            "を        16\n",
            "に         4\n",
            "が         2\n",
            "から        1\n",
            "は         1\n",
            "をめぐって     1\n",
            "dtype: int64\n",
            "--- 動詞「なる」にかかる助詞の出現頻度 ---\n",
            "助詞\n",
            "に      11\n",
            "と       9\n",
            "も       2\n",
            "と は     1\n",
            "に は     1\n",
            "は       1\n",
            "dtype: int64\n",
            "--- 動詞「与える」にかかる助詞の出現頻度 ---\n",
            "助詞\n",
            "など に    1\n",
            "に       1\n",
            "を       1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 46. 動詞の格フレーム情報の抽出\n",
        "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
        "\n",
        "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
        "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "    作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n",
        "\n",
        "> さっき出来てなかったけど、動詞だけで係っている助詞がないパターンを処理するのを忘れていたのでifを追加した。\n",
        "\n",
        "> なんかすぐできたかと思ったけど、他の方の回答と見比べると、\n",
        "\n",
        "    他の方 >> 代わる   に を 知的行動を 人間に\n",
        "    自分   >> # 24 係り先:代わる 係り元助詞:に 係り元文節:人間に\n",
        "    他の方 >> 述べる   で に の は 解説で 佐藤理史は 次のように\n",
        "    自分   >> #  5 係り先:述べる 係り元助詞:の に 係り元文節:次のように\n",
        "\n",
        "といった具合で少し拾えてないところがある事に気づいた。ちょっと調整する必要がありそう。\n",
        "\n",
        "> chunkオブジェクトがイテラブルじゃないのでforで回してリストを作ってから結合するというすごく手間なことをしてしまった。"
      ],
      "metadata": {
        "id": "ZY2_LUvmGCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "    \n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Doushi_Kaku = []\n",
        "\n",
        "for i,sentence in enumerate(all_text[:5]):\n",
        "  #print(f\"--- Sentence {i:04g} ---\")\n",
        "  for c in sentence[:]:\n",
        "    if (\"動詞\" in [m.pos for m in c.morphs]):  # 文節に動詞を含む場合のみ\n",
        "      kakari_moto_num = c.srcs\n",
        "      kakari_moto_joshi_L = []\n",
        "      kakari_moto_chunk_L = []\n",
        "      for c_joshi in [sentence[c_n] for c_n in kakari_moto_num]:\n",
        "        kakari_moto_joshi_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos==\"助詞\"]))\n",
        "        kakari_moto_chunk_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos!=\"記号\"]))\n",
        "      #print(kakari_moto_joshi_L)\n",
        "      #print(kakari_moto_chunk_L)\n",
        "      kakari_moto_joshi = \" \".join(kakari_moto_joshi_L)\n",
        "      kakari_moto_chunk = \" \".join(kakari_moto_chunk_L)\n",
        "      if kakari_moto_joshi:\n",
        "        doushi_text = [m.base for m in c.morphs if m.pos==\"動詞\"][0]\n",
        "        print(f\"#{c.num:3g} \\t 係り先 : {doushi_text} \\t 係り元助詞 : {kakari_moto_joshi} \\t 係り元文節 : {kakari_moto_chunk}\")\n",
        "        output_Doushi_Kaku.append(f\"{doushi_text}\\t{kakari_moto_joshi}\\t{kakari_moto_chunk}\")\n",
        "\n",
        "\n",
        "with open(\"output_Doushi_Kou.txt\",mode=\"w\") as out:\n",
        "  for line in output_Doushi_Kaku:\n",
        "    out.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "e7oStrybGCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaa64cf-b885-4799-aa76-861ae69c0664"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 10 \t 係り先 : 用いる \t 係り元助詞 : を \t 係り元文節 : 道具を\n",
            "# 12 \t 係り先 : する \t 係り元助詞 : て を \t 係り元文節 : 用いて 知能を\n",
            "# 16 \t 係り先 : 指す \t 係り元助詞 : を \t 係り元文節 : 一分野を\n",
            "# 24 \t 係り先 : 代わる \t 係り元助詞 : を に \t 係り元文節 : 知的行動を 人間に\n",
            "# 26 \t 係り先 : 行う \t 係り元助詞 : て に \t 係り元文節 : 代わって コンピューターに\n",
            "# 35 \t 係り先 : する \t 係り元助詞 : とも \t 係り元文節 : 研究分野とも\n",
            "#  5 \t 係り先 : 述べる \t 係り元助詞 : で は のに \t 係り元文節 : 解説で 佐藤理史は 次のように\n",
            "#  3 \t 係り先 : する \t 係り元助詞 : を で \t 係り元文節 : 知的能力を コンピュータ上で\n",
            "# 11 \t 係り先 : する \t 係り元助詞 : を \t 係り元文節 : 推論判断を\n",
            "# 14 \t 係り先 : する \t 係り元助詞 : を \t 係り元文節 : 画像データを\n",
            "# 17 \t 係り先 : する \t 係り元助詞 : て を \t 係り元文節 : 解析して パターンを\n",
            "# 19 \t 係り先 : ある \t 係り元助詞 :  は が \t 係り元文節 : 技術ソフトウェアコンピュータシステム 応用例は 画像認識等が\n",
            "# 23 \t 係り先 : する \t 係り元助詞 :  に で により \t 係り元文節 : ある 1956年に ダートマス会議で ジョンマッカーシーにより\n",
            "# 26 \t 係り先 : 用いる \t 係り元助詞 : を \t 係り元文節 : 記号処理を\n",
            "# 30 \t 係り先 : する \t 係り元助詞 : を と \t 係り元文節 : 記述を 主体と\n",
            "# 35 \t 係り先 : 使う \t 係り元助詞 :  では でも \t 係り元文節 : 命名された 現在では 意味あいでも\n",
            "# 41 \t 係り先 : 呼ぶ \t 係り元助詞 : も  \t 係り元文節 : 思考ルーチンも こう\n",
            "# 43 \t 係り先 : ある \t 係り元助詞 : て も \t 係り元文節 : 使われている ことも\n",
            "#  3 \t 係り先 : する \t 係り元助詞 : を \t 係り元文節 : カウンセラーを\n",
            "#  8 \t 係り先 : 出す \t 係り元助詞 :  が  に \t 係り元文節 : プログラム 人工無脳が しばしば 引き合いに\n",
            "# 13 \t 係り先 : する \t 係り元助詞 : に を \t 係り元文節 : 計算機に 役割を\n",
            "# 15 \t 係り先 : 呼ぶ \t 係り元助詞 : と \t 係り元文節 : エキスパートシステムと\n",
            "# 20 \t 係り先 : 持つ \t 係り元助詞 : が に \t 係り元文節 : 人間が 暗黙に\n",
            "# 24 \t 係り先 : なる \t 係り元助詞 : が と \t 係り元文節 : 記述が 問題と\n",
            "# 27 \t 係り先 : する \t 係り元助詞 : が は  が \t 係り元文節 : 出されるが 実現は なり 利用が\n",
            "# 35 \t 係り先 : 知る \t 係り元助詞 : としては も \t 係り元文節 : アプローチとしては アプローチも\n",
            "# 42 \t 係り先 : ある \t 係り元助詞 : て てが は に \t 係り元文節 : 困難視されている 知られているが 差は 記号的明示性に\n",
            "# 46 \t 係り先 : 集める \t 係り元助詞 :   が を \t 係り元文節 : ある その後 サポートベクターマシンが 注目を\n",
            "# 52 \t 係り先 : 行う \t 係り元助詞 : を に を \t 係り元文節 : 経験を 元に 学習を\n",
            "# 55 \t 係り先 : ある \t 係り元助詞 :   も \t 係り元文節 : 集めた また 手法も\n",
            "# 66 \t 係り先 : する \t 係り元助詞 : を に \t 係り元文節 : 知性を 機械的に\n",
            "# 67 \t 係り先 : する \t 係り元助詞 : において   \t 係り元文節 : 宇宙において 言葉通り 表現し\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認用(UNIXコマンドじゃないけど許して～～～)\n",
        "import pandas as pd\n",
        "with open(\"output_Doushi_Kou.txt\",mode=\"r\") as f:\n",
        "  df = pd.read_table(f, names=[\"動詞\",\"助詞\",\"文節\"])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "VTMMQ4-USKOF",
        "outputId": "782c1567-1772-4d11-be08-789cf472be36"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     動詞        助詞                               文節\n",
              "0   用いる         を                              道具を\n",
              "1    する       て を                          用いて 知能を\n",
              "2    指す         を                             一分野を\n",
              "3   代わる       を に                        知的行動を 人間に\n",
              "4    行う       て に                    代わって コンピューターに\n",
              "5    する        とも                           研究分野とも\n",
              "6   述べる    で は のに                  解説で 佐藤理史は 次のように\n",
              "7    する       を で                   知的能力を コンピュータ上で\n",
              "8    する         を                            推論判断を\n",
              "9    する         を                           画像データを\n",
              "10   する       て を                       解析して パターンを\n",
              "11   ある       は が   技術ソフトウェアコンピュータシステム 応用例は 画像認識等が\n",
              "12   する   に で により  ある 1956年に ダートマス会議で ジョンマッカーシーにより\n",
              "13  用いる         を                            記号処理を\n",
              "14   する       を と                          記述を 主体と\n",
              "15   使う     では でも                命名された 現在では 意味あいでも\n",
              "16   呼ぶ        も                        思考ルーチンも こう\n",
              "17   ある       て も                       使われている ことも\n",
              "18   する         を                          カウンセラーを\n",
              "19   出す      が  に           プログラム 人工無脳が しばしば 引き合いに\n",
              "20   する       に を                         計算機に 役割を\n",
              "21   呼ぶ         と                      エキスパートシステムと\n",
              "22   持つ       が に                          人間が 暗黙に\n",
              "23   なる       が と                          記述が 問題と\n",
              "24   する    が は  が                 出されるが 実現は なり 利用が\n",
              "25   知る    としては も                 アプローチとしては アプローチも\n",
              "26   ある  て てが は に      困難視されている 知られているが 差は 記号的明示性に\n",
              "27  集める       が を          ある その後 サポートベクターマシンが 注目を\n",
              "28   行う     を に を                       経験を 元に 学習を\n",
              "29   ある         も                       集めた また 手法も\n",
              "30   する       を に                         知性を 機械的に\n",
              "31   する    において                    宇宙において 言葉通り 表現し"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccdea85d-d67d-4a7d-96f4-f178301e9937\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>動詞</th>\n",
              "      <th>助詞</th>\n",
              "      <th>文節</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>用いる</td>\n",
              "      <td>を</td>\n",
              "      <td>道具を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>する</td>\n",
              "      <td>て を</td>\n",
              "      <td>用いて 知能を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>指す</td>\n",
              "      <td>を</td>\n",
              "      <td>一分野を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>代わる</td>\n",
              "      <td>を に</td>\n",
              "      <td>知的行動を 人間に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>行う</td>\n",
              "      <td>て に</td>\n",
              "      <td>代わって コンピューターに</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>する</td>\n",
              "      <td>とも</td>\n",
              "      <td>研究分野とも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>述べる</td>\n",
              "      <td>で は のに</td>\n",
              "      <td>解説で 佐藤理史は 次のように</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>する</td>\n",
              "      <td>を で</td>\n",
              "      <td>知的能力を コンピュータ上で</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>する</td>\n",
              "      <td>を</td>\n",
              "      <td>推論判断を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>する</td>\n",
              "      <td>を</td>\n",
              "      <td>画像データを</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>する</td>\n",
              "      <td>て を</td>\n",
              "      <td>解析して パターンを</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ある</td>\n",
              "      <td>は が</td>\n",
              "      <td>技術ソフトウェアコンピュータシステム 応用例は 画像認識等が</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>する</td>\n",
              "      <td>に で により</td>\n",
              "      <td>ある 1956年に ダートマス会議で ジョンマッカーシーにより</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>用いる</td>\n",
              "      <td>を</td>\n",
              "      <td>記号処理を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>する</td>\n",
              "      <td>を と</td>\n",
              "      <td>記述を 主体と</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>使う</td>\n",
              "      <td>では でも</td>\n",
              "      <td>命名された 現在では 意味あいでも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>呼ぶ</td>\n",
              "      <td>も</td>\n",
              "      <td>思考ルーチンも こう</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ある</td>\n",
              "      <td>て も</td>\n",
              "      <td>使われている ことも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>する</td>\n",
              "      <td>を</td>\n",
              "      <td>カウンセラーを</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>出す</td>\n",
              "      <td>が  に</td>\n",
              "      <td>プログラム 人工無脳が しばしば 引き合いに</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>する</td>\n",
              "      <td>に を</td>\n",
              "      <td>計算機に 役割を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>呼ぶ</td>\n",
              "      <td>と</td>\n",
              "      <td>エキスパートシステムと</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>持つ</td>\n",
              "      <td>が に</td>\n",
              "      <td>人間が 暗黙に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>なる</td>\n",
              "      <td>が と</td>\n",
              "      <td>記述が 問題と</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>する</td>\n",
              "      <td>が は  が</td>\n",
              "      <td>出されるが 実現は なり 利用が</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>知る</td>\n",
              "      <td>としては も</td>\n",
              "      <td>アプローチとしては アプローチも</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ある</td>\n",
              "      <td>て てが は に</td>\n",
              "      <td>困難視されている 知られているが 差は 記号的明示性に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>集める</td>\n",
              "      <td>が を</td>\n",
              "      <td>ある その後 サポートベクターマシンが 注目を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>行う</td>\n",
              "      <td>を に を</td>\n",
              "      <td>経験を 元に 学習を</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ある</td>\n",
              "      <td>も</td>\n",
              "      <td>集めた また 手法も</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>する</td>\n",
              "      <td>を に</td>\n",
              "      <td>知性を 機械的に</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>する</td>\n",
              "      <td>において</td>\n",
              "      <td>宇宙において 言葉通り 表現し</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccdea85d-d67d-4a7d-96f4-f178301e9937')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ccdea85d-d67d-4a7d-96f4-f178301e9937 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ccdea85d-d67d-4a7d-96f4-f178301e9937');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 47. 機能動詞構文のマイニング\n",
        "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
        "\n",
        "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
        "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
        "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
        "\n",
        "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
        "\n",
        "    学習を行う\tに を\t元に 経験を\n",
        "\n",
        "> 過去にやった「連接」の探索と同じようにしようとしても、chunk/morphオブジェクトはイテラブルではないので詰む。\n",
        "  他の方の回答を見ると、「動詞の係り元chunkから助詞を探す」という風にしていた。かしこ～\n",
        "\n",
        "> 動詞→係り元取得→「サ変+を」の取得までは出来たけど、(「サ変+を」→係り先→動詞探索にしても)動詞から助詞を含む係り元を探して格納する時に「サ変+を」が入ってる文節を省くのが出来ねえ…単純に`not \"サ変接続\" in pos1`で省いて良いものか？\n",
        "\n",
        "少なくとも、上記の例では「行う」に「学習を(サ変+を)」が係っていて、それ以外の「元に/経験を」を出力しているが…\n",
        "\n",
        "    # 53 \t 係り先 : コンテンツ生成を行う \t 係り元助詞 : を \t 係り元文節 : コンテンツ生成を\n",
        "みたいな例もあるし「サ変+を」の文節を消していいとも思えん…\n",
        "でも二重になってるのも気に入らんからわからんわ～とりあえず消したものを回答としておきます。\n",
        "\n",
        "\n",
        "> `df.value_counts`に何も引数を与えずに呼び出すと全部の列を見て一致するものをユニークとしたDataFrameを返す。ので、不要な列を予めDropしておいてコレを叩けば良いっぽい。"
      ],
      "metadata": {
        "id": "0dFw4MJIGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "file = \"ai.ja.txt.parsed\"\n",
        "morphemes = (\"surface\", \"base\", \"pos\", \"pos1\")\n",
        "\n",
        "class Morph:  # 形態素\n",
        "  def __init__(self, word, morphs):\n",
        "    morph = morphs.split(\",\")\n",
        "    self.surface = word[0]  # 表層形\n",
        "    self.base = morph[6]  # 基本形\n",
        "    self.pos = morph[0]  # 品詞\n",
        "    self.pos1 = morph[1]  # 品詞細分類1\n",
        "\n",
        "\n",
        "class Chunk:  # 文節\n",
        "  def __init__(self, morphs, dst, num=None):\n",
        "    self.morphs = morphs  # 形態素集合\n",
        "    self.dst = int(dst)  # 係り先文節\n",
        "    self.srcs = []  # 係り元文節\n",
        "    self.num = num  # 係り受け表現のために付加した文節番号\n",
        "    # print(f\"形態素(Object):{self.morphs} / 係り先:{self.dst} /係り元:{self.srcs}\")\n",
        "\n",
        "\n",
        "all_text = []  # 全体を纏めるブロックのList \n",
        "sentences = []  # 文章ごとの文節のList\n",
        "morphemes = []  # 文節ごとの形態素のList\n",
        "block_chunks = []  # 文毎に処理する用のList \n",
        "\n",
        "\n",
        "with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  blocks = f.read().split(\"EOS\\n\")\n",
        "  \n",
        "  for block in blocks[:]:  # \"EOS\\n\"で分割した文節ごとの数行からなるブロック\n",
        "    lines = block.split(\"\\n\")\n",
        "\n",
        "    for line in lines[:]:  # 1行ごとに単語単位で分割されている処理\n",
        "      if line.startswith(\"*\"):  # 係り受け表現の行の処理\n",
        "        if len(morphemes)>0:\n",
        "          block_chunks.append(Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1])))\n",
        "        kakari = line.split(\" \")\n",
        "        morphemes = []\n",
        "        continue\n",
        "      \n",
        "      word = line.split(\"\\t\")\n",
        "      if (len(word)==2) & (word[0]!=\"\"):  # 単語以外の場合はスキップ\n",
        "        morphemes.append(Morph(word, word[1]))\n",
        "    # --- for(lines) END --- #\n",
        "\n",
        "    # 文節末まで来たら、文節の係り受け情報を付加してChunkを作成\n",
        "    if len(morphemes)>0:\n",
        "      c = Chunk(morphs=morphemes, dst=kakari[2].replace(\"D\",\"\"), num=int(kakari[1]))\n",
        "      block_chunks.append(c)\n",
        "\n",
        "\n",
        "    # 係り先しか登録されてないので、係り元を追加していく\n",
        "    for c in block_chunks:\n",
        "      if not c.dst == -1:\n",
        "        block_chunks[c.dst].srcs.append(int(c.num))\n",
        "      sentences.append(c)\n",
        "      #print(f\"#{c.num:3g}  形態素: {[m.surface for m in c.morphs]},\\t 係り先: {c.dst}, 係り元: {c.srcs}\")\n",
        "    if len(block_chunks)>0:  # 空行は追加しないようにした\n",
        "      all_text.append(sentences)\n",
        "    \n",
        "\n",
        "    block_chunks = []\n",
        "    morphemes = []\n",
        "    sentences = []\n",
        "    kakari = []\n",
        "  # --- for(Block) END --- #\n",
        "\n",
        "output_Kinou_Doushi = []\n",
        "\n",
        "for i,sentence in enumerate(all_text[:]):\n",
        "  for c in sentence:\n",
        "    kakari_moto_joshi_L = []\n",
        "    kakari_moto_chunk_L = []\n",
        "    kakari_moto_text = \"\"\n",
        "    c_sahen_wo = None\n",
        "    if (\"動詞\" in [m.pos for m in c.morphs]):  # 文節に動詞を含む場合のみ\n",
        "      for c_src in c.srcs:\n",
        "        if(\n",
        "            (\"サ変接続\" in [m.pos1 for m in sentence[c_src].morphs])\n",
        "            and (\"を\" in [m.surface for m in sentence[c_src].morphs])\n",
        "            ):\n",
        "          kakari_moto_text = (\n",
        "              \"\".join([m.surface for m in sentence[c_src].morphs if m.pos!=\"記号\"])  # 「サ変名詞+を」を含む係り元\n",
        "                + [m.base for m in c.morphs if m.pos==\"動詞\"][0]  # 最左の動詞のみ抜き出す\n",
        "              )\n",
        "          c_sahen_wo = c_src\n",
        "    if kakari_moto_text:\n",
        "      print(f\"--- Sentence {i:04g} ---\")\n",
        "      for c_joshi in [sentence[c_n] for c_n in c.srcs if not c_n==c_sahen_wo]:  # 「サ変+を」の文節と同一ではないものを対象に\n",
        "        kakari_moto_joshi_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos==\"助詞\"]))\n",
        "        kakari_moto_chunk_L.append(\"\".join([m.surface for m in c_joshi.morphs if m.pos!=\"記号\"]))\n",
        "      kakari_moto_joshi = \" \".join(kakari_moto_joshi_L)\n",
        "      kakari_moto_chunk = \" \".join(kakari_moto_chunk_L)\n",
        "      print(f\"#{c.num:3g} \\t 係り先 : {kakari_moto_text} \\t 係り元助詞 : {kakari_moto_joshi} \\t 係り元文節 : {kakari_moto_chunk}\")\n",
        "      output_Kinou_Doushi.append(f\"{kakari_moto_text}\\t{kakari_moto_joshi}\\t{kakari_moto_chunk}\")\n",
        "\n",
        "\n",
        "with open(\"output_Kinou_Doushi.txt\",mode=\"w\") as out:\n",
        "  for line in output_Kinou_Doushi:\n",
        "    out.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "EgJZi8auGCvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7688a3c8-ae2c-48e2-b9ab-78ff5f6e84bd"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence 0001 ---\n",
            "# 24 \t 係り先 : 知的行動を代わる \t 係り元助詞 : に \t 係り元文節 : 人間に\n",
            "--- Sentence 0003 ---\n",
            "# 11 \t 係り先 : 推論判断をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0003 ---\n",
            "# 26 \t 係り先 : 記号処理を用いる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0003 ---\n",
            "# 30 \t 係り先 : 記述をする \t 係り元助詞 : と \t 係り元文節 : 主体と\n",
            "--- Sentence 0004 ---\n",
            "# 46 \t 係り先 : 注目を集める \t 係り元助詞 :   が \t 係り元文節 : ある その後 サポートベクターマシンが\n",
            "--- Sentence 0004 ---\n",
            "# 52 \t 係り先 : 学習を行う \t 係り元助詞 : を に \t 係り元文節 : 経験を 元に\n",
            "--- Sentence 0005 ---\n",
            "# 10 \t 係り先 : 流行を超える \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0007 ---\n",
            "#  5 \t 係り先 : 学習を繰り返す \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0008 ---\n",
            "# 13 \t 係り先 : 統計的学習をする \t 係り元助詞 : て では を に を通して \t 係り元文節 : なされている ACT-Rでは 推論ルールを 元に 生成規則を通して\n",
            "--- Sentence 0009 ---\n",
            "# 42 \t 係り先 : 進化を見せる \t 係り元助詞 : て  は て において \t 係り元文節 : 活躍している 特に 敵対的生成ネットワークは 加えて 生成技術において\n",
            "--- Sentence 0009 ---\n",
            "# 53 \t 係り先 : コンテンツ生成を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0011 ---\n",
            "# 14 \t 係り先 : 機械式計算機をする \t 係り元助詞 :  は  \t 係り元文節 : 機械論 ブレーズ・パスカルは 1642年\n",
            "--- Sentence 0011 ---\n",
            "# 20 \t 係り先 : 開発を行う \t 係り元助詞 :  は \t 係り元文節 : 製作した エイダ・ラブレスは\n",
            "--- Sentence 0013 ---\n",
            "# 15 \t 係り先 : プログラミング言語をする \t 係り元助詞 :  は \t 係り元文節 : 作り出した 彼はまた\n",
            "--- Sentence 0013 ---\n",
            "# 18 \t 係り先 : テストをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0013 ---\n",
            "# 27 \t 係り先 : 来談者中心療法を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0015 ---\n",
            "# 19 \t 係り先 : プログラミング言語をする \t 係り元助詞 :   は \t 係り元文節 : 示した 示し アランカルメラウアーは\n",
            "--- Sentence 0019 ---\n",
            "# 16 \t 係り先 : バックギャモン専用コンピュータTDギャモンをする \t 係り元助詞 : に は \t 係り元文節 : 1992年に IBMは\n",
            "--- Sentence 0019 ---\n",
            "# 45 \t 係り先 : 投資全額を上回る \t 係り元助詞 :  が \t 係り元文節 : 使い コストが\n",
            "--- Sentence 0020 ---\n",
            "# 11 \t 係り先 : 意味付けをする \t 係り元助詞 : から に対して \t 係り元文節 : ここから 非構造化データに対して\n",
            "--- Sentence 0020 ---\n",
            "# 13 \t 係り先 : 処理を行う \t 係り元助詞 :  \t 係り元文節 : 適用し\n",
            "--- Sentence 0020 ---\n",
            "# 22 \t 係り先 : 知的処理を行う \t 係り元助詞 : に により に \t 係り元文節 : 同年に ティム・バーナーズリーにより Webに\n",
            "--- Sentence 0020 ---\n",
            "# 30 \t 係り先 : 意味をする \t 係り元助詞 : に \t 係り元文節 : データに\n",
            "--- Sentence 0020 ---\n",
            "# 33 \t 係り先 : 知的処理を行う \t 係り元助詞 : て に \t 係り元文節 : 付加して コンピュータに\n",
            "--- Sentence 0023 ---\n",
            "#  8 \t 係り先 : 研究を進める \t 係り元助詞 : て \t 係り元文節 : 費やして\n",
            "--- Sentence 0023 ---\n",
            "# 38 \t 係り先 : 命令をする \t 係り元助詞 :  で \t 係り元文節 : 直接 機構で\n",
            "--- Sentence 0024 ---\n",
            "# 48 \t 係り先 : 運転をする \t 係り元助詞 :  に \t 係り元文節 : 増やし 元に\n",
            "--- Sentence 0025 ---\n",
            "#  6 \t 係り先 : 特許をする \t 係り元助詞 : までに が \t 係り元文節 : 2018年までに 日本が\n",
            "--- Sentence 0026 ---\n",
            "#  9 \t 係り先 : 研究をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0026 ---\n",
            "# 25 \t 係り先 : 運転をする \t 係り元助詞 : て に \t 係り元文節 : 基づいて 柔軟に\n",
            "--- Sentence 0026 ---\n",
            "# 46 \t 係り先 : 注目を集める \t 係り元助詞 :  から は  \t 係り元文節 : 世界初であった ことから ファジィは 関わらず\n",
            "--- Sentence 0026 ---\n",
            "# 64 \t 係り先 : ニューロファジィ制御をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0026 ---\n",
            "# 80 \t 係り先 : 成功を受ける \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0026 ---\n",
            "# 84 \t 係り先 : 知的制御を用いる \t 係り元助詞 : て も \t 係り元文節 : 受けて 他社も\n",
            "--- Sentence 0027 ---\n",
            "# 37 \t 係り先 : 開発工数を抑える \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0027 ---\n",
            "# 57 \t 係り先 : 制御をする \t 係り元助詞 : から  \t 係り元文節 : 少なさから 多少\n",
            "--- Sentence 0028 ---\n",
            "# 13 \t 係り先 : 知的制御をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0028 ---\n",
            "# 46 \t 係り先 : 表現するをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0029 ---\n",
            "# 11 \t 係り先 : 進歩を担う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0032 ---\n",
            "# 23 \t 係り先 : 精度改善を果たす \t 係り元助詞 : に で が \t 係り元文節 : 2012年に 画像処理コンテストで チームが\n",
            "--- Sentence 0033 ---\n",
            "# 15 \t 係り先 : 専用プログラムを使う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0035 ---\n",
            "#  4 \t 係り先 : 研究を続ける \t 係り元助詞 : が て \t 係り元文節 : ジェフホーキンスが 向けて\n",
            "--- Sentence 0037 ---\n",
            "# 22 \t 係り先 : 行動型システムを用いる \t 係り元助詞 : て は は \t 係り元文節 : 登場している これは ものではなく\n",
            "--- Sentence 0039 ---\n",
            "#  6 \t 係り先 : 関連性を導き出す \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0039 ---\n",
            "# 20 \t 係り先 : ワンショット学習をする \t 係り元助詞 : が \t 係り元文節 : データが\n",
            "--- Sentence 0039 ---\n",
            "# 27 \t 係り先 : 認識能力を持つ \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0039 ---\n",
            "# 33 \t 係り先 : 記号接地問題(シンボルグラウンディング問題)をする \t 係り元助詞 :  には \t 係り元文節 : 開発 8月には\n",
            "--- Sentence 0040 ---\n",
            "# 25 \t 係り先 : 注目を集める \t 係り元助詞 : に \t 係り元文節 : 急速に\n",
            "--- Sentence 0040 ---\n",
            "# 33 \t 係り先 : 普及を受ける \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0040 ---\n",
            "# 62 \t 係り先 : 機械学習を組み合わせる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0043 ---\n",
            "# 21 \t 係り先 : 投資を行う \t 係り元助詞 : に で \t 係り元文節 : 全世界的に 民間企業主導で\n",
            "--- Sentence 0047 ---\n",
            "# 11 \t 係り先 : 探索を行う \t 係り元助詞 :  で \t 係り元文節 : 実装し 無報酬で\n",
            "--- Sentence 0048 ---\n",
            "#  9 \t 係り先 : 推論をする \t 係り元助詞 : て \t 係り元文節 : 経て\n",
            "--- Sentence 0051 ---\n",
            "# 25 \t 係り先 : 共同研究を始める \t 係り元助詞 : は とも \t 係り元文節 : Googleは マックスプランク研究所とも\n",
            "--- Sentence 0051 ---\n",
            "# 31 \t 係り先 : 研究を行う \t 係り元助詞 :  て \t 係り元文節 : いう 始めており\n",
            "--- Sentence 0052 ---\n",
            "# 11 \t 係り先 : 研究開発をする \t 係り元助詞 : では  で \t 係り元文節 : 中国では 立ち上げ 官民一体で\n",
            "--- Sentence 0052 ---\n",
            "# 34 \t 係り先 : 実験をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0052 ---\n",
            "# 41 \t 係り先 : 研究開発をする \t 係り元助詞 : で \t 係り元文節 : 日本で\n",
            "--- Sentence 0055 ---\n",
            "#  4 \t 係り先 : 投資をする \t 係り元助詞 : は までに \t 係り元文節 : 韓国は 2022年までに\n",
            "--- Sentence 0058 ---\n",
            "#  1 \t 係り先 : 深層学習をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0059 ---\n",
            "#  1 \t 係り先 : 脳シミュレーションを行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0060 ---\n",
            "#  9 \t 係り先 : 反乱を起こす \t 係り元助詞 : て に対して \t 係り元文節 : 於いて 人間に対して\n",
            "--- Sentence 0061 ---\n",
            "#  6 \t 係り先 : 弾圧を併せ持つ \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0061 ---\n",
            "# 67 \t 係り先 : 監視を行う \t 係り元助詞 :  まで に \t 係り元文節 : 推し進められ 歩行者まで 人工知能に\n",
            "--- Sentence 0061 ---\n",
            "# 90 \t 係り先 : 法的手続きを経る \t 係り元助詞 : を \t 係り元文節 : ウイグル族を\n",
            "--- Sentence 0061 ---\n",
            "#125 \t 係り先 : 監視社会化を恐れる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0061 ---\n",
            "#131 \t 係り先 : 監視カメラをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0061 ---\n",
            "#140 \t 係り先 : AI監視技術をする \t 係り元助詞 : は に \t 係り元文節 : 中国は 世界各国に\n",
            "--- Sentence 0062 ---\n",
            "# 26 \t 係り先 : 差別を認める \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0063 ---\n",
            "# 17 \t 係り先 : 研究をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0064 ---\n",
            "# 38 \t 係り先 : 展開を変える \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0064 ---\n",
            "# 40 \t 係り先 : 戦争をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0065 ---\n",
            "# 14 \t 係り先 : ファジィ制御をする \t 係り元助詞 : は \t 係り元文節 : AIプログラムは\n",
            "--- Sentence 0065 ---\n",
            "# 28 \t 係り先 : 判断を介す \t 係り元助詞 : から \t 係り元文節 : 観点から\n",
            "--- Sentence 0065 ---\n",
            "# 32 \t 係り先 : 開発禁止令を出す \t 係り元助詞 : に \t 係り元文節 : 2012年に\n",
            "--- Sentence 0067 ---\n",
            "# 65 \t 係り先 : 禁止を求める \t 係り元助詞 :  には が \t 係り元文節 : 記された 4月には ヒューマン・ライツ・ウォッチが\n",
            "--- Sentence 0067 ---\n",
            "# 79 \t 係り先 : 運用をめぐる \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "# 11 \t 係り先 : 開発競争を行う \t 係り元助詞 : は をめぐって \t 係り元文節 : 米国中国ロシアは 軍事利用をめぐって\n",
            "--- Sentence 0068 ---\n",
            "# 25 \t 係り先 : 記録をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "# 39 \t 係り先 : 自律無人艇を使う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "# 42 \t 係り先 : 試験を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "#112 \t 係り先 : メイヴン計画を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0068 ---\n",
            "#140 \t 係り先 : 追及を受ける \t 係り元助詞 :  て では とともに と で \t 係り元文節 : なった 暴露されており 公聴会では とともに 拒否すると 整合性で\n",
            "--- Sentence 0068 ---\n",
            "#159 \t 係り先 : 共同研究をする \t 係り元助詞 : が \t 係り元文節 : Microsoftが\n",
            "--- Sentence 0068 ---\n",
            "#171 \t 係り先 : 監視国家をする \t 係り元助詞 : が によって \t 係り元文節 : 中国が AIによって\n",
            "--- Sentence 0068 ---\n",
            "#177 \t 係り先 : 無人攻撃機をする \t 係り元助詞 : で に \t 係り元文節 : 中東で 大量に\n",
            "--- Sentence 0070 ---\n",
            "# 15 \t 係り先 : 反科学反マイノリティ・地球温暖化懐疑論等をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0070 ---\n",
            "# 20 \t 係り先 : 解任をする \t 係り元助詞 :   て は \t 係り元文節 : 発表した しかし 含まれており Google社員らは\n",
            "--- Sentence 0070 ---\n",
            "# 31 \t 係り先 : 解散をする \t 係り元助詞 :   は が で \t 係り元文節 : 要請した 4月4日 Googleは 倫理委員会が 理由で\n",
            "--- Sentence 0071 ---\n",
            "# 16 \t 係り先 : 霊的存在を見いだす \t 係り元助詞 : に \t 係り元文節 : ものに\n",
            "--- Sentence 0072 ---\n",
            "#  9 \t 係り先 : 道)」をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0072 ---\n",
            "# 17 \t 係り先 : 実現をする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0073 ---\n",
            "# 20 \t 係り先 : 話をする \t 係り元助詞 : は  \t 係り元文節 : 哲学者は みんな\n",
            "--- Sentence 0076 ---\n",
            "# 29 \t 係り先 : 意思疎通を行う \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0078 ---\n",
            "# 48 \t 係り先 : 勘違いをする \t 係り元助詞 :  \t 係り元文節 : \n",
            "--- Sentence 0082 ---\n",
            "#  6 \t 係り先 : 議論を行う \t 係り元助詞 : まで   \t 係り元文節 : これまで けっこう 長時間\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認用(UNIXコマンドじゃないけど許して～～～)\n",
        "import pandas as pd\n",
        "with open(\"output_Kinou_Doushi.txt\",mode=\"r\") as f:\n",
        "  df = pd.read_table(f, names=[\"動詞\",\"助詞\",\"文節\"])\n",
        "\n",
        "print(df)\n",
        "print(\"----- -----\")\n",
        "\n",
        "print(\"頻出述語(サ変+を+動詞)\")\n",
        "print(df.value_counts(\"動詞\"))\n",
        "\n",
        "print(\"----- -----\")\n",
        "\n",
        "print(\"頻出述語パターン\")\n",
        "print(df.drop(columns=\"文節\").value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buW2NidokQS9",
        "outputId": "fcc1c0a6-64a9-4e35-e61f-d4d8eee7a0c5"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          動詞    助詞                   文節\n",
            "0   知的行動を代わる     に                  人間に\n",
            "1    推論判断をする   NaN                  NaN\n",
            "2   記号処理を用いる   NaN                  NaN\n",
            "3      記述をする     と                  主体と\n",
            "4     注目を集める     が  ある その後 サポートベクターマシンが\n",
            "..       ...   ...                  ...\n",
            "91     実現をする   NaN                  NaN\n",
            "92      話をする    は              哲学者は みんな\n",
            "93   意思疎通を行う   NaN                  NaN\n",
            "94    勘違いをする   NaN                  NaN\n",
            "95     議論を行う  まで          これまで けっこう 長時間\n",
            "\n",
            "[96 rows x 3 columns]\n",
            "----- -----\n",
            "頻出述語(サ変+を+動詞)\n",
            "動詞\n",
            "注目を集める          3\n",
            "プログラミング言語をする    2\n",
            "知的処理を行う         2\n",
            "運転をする           2\n",
            "研究開発をする         2\n",
            "               ..\n",
            "意思疎通を行う         1\n",
            "意味付けをする         1\n",
            "意味をする           1\n",
            "弾圧を併せ持つ         1\n",
            "霊的存在を見いだす       1\n",
            "Length: 89, dtype: int64\n",
            "----- -----\n",
            "頻出述語パターン\n",
            "動詞                         助詞            \n",
            "AI監視技術をする                  は に               1\n",
            "バックギャモン専用コンピュータTDギャモンをする   に は               1\n",
            "知的処理を行う                    に により に           1\n",
            "知的制御を用いる                   て も               1\n",
            "知的行動を代わる                   に                 1\n",
            "研究を続ける                     が て               1\n",
            "研究を行う                       て                1\n",
            "研究を進める                     て                 1\n",
            "研究開発をする                    で                 1\n",
            "                           では  で             1\n",
            "禁止を求める                      には が             1\n",
            "精度改善を果たす                   に で が             1\n",
            "統計的学習をする                   て では を に を通して     1\n",
            "行動型システムを用いる                て は は             1\n",
            "解任をする                        て は             1\n",
            "解散をする                        は が で           1\n",
            "記号接地問題(シンボルグラウンディング問題)をする   には               1\n",
            "記述をする                      と                 1\n",
            "話をする                       は                 1\n",
            "議論を行う                      まで                1\n",
            "追及を受ける                      て では とともに と で    1\n",
            "進化を見せる                     て  は て において       1\n",
            "運転をする                       に                1\n",
            "                           て に               1\n",
            "開発を行う                       は                1\n",
            "開発禁止令を出す                   に                 1\n",
            "開発競争を行う                    は をめぐって           1\n",
            "知的処理を行う                    て に               1\n",
            "監視国家をする                    が によって            1\n",
            "監視を行う                       まで に             1\n",
            "意味をする                      に                 1\n",
            "ファジィ制御をする                  は                 1\n",
            "プログラミング言語をする                 は               1\n",
            "                            は                1\n",
            "ワンショット学習をする                が                 1\n",
            "共同研究をする                    が                 1\n",
            "共同研究を始める                   は とも              1\n",
            "判断を介す                      から                1\n",
            "制御をする                      から                1\n",
            "反乱を起こす                     て に対して            1\n",
            "命令をする                       で                1\n",
            "学習を行う                      を に               1\n",
            "意味付けをする                    から に対して           1\n",
            "特許をする                      までに が             1\n",
            "投資をする                      は までに             1\n",
            "投資を行う                      に で               1\n",
            "投資全額を上回る                    が                1\n",
            "探索を行う                       で                1\n",
            "推論をする                      て                 1\n",
            "機械式計算機をする                   は                1\n",
            "法的手続きを経る                   を                 1\n",
            "注目を集める                       が               1\n",
            "                            から は             1\n",
            "                           に                 1\n",
            "無人攻撃機をする                   で に               1\n",
            "霊的存在を見いだす                  に                 1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 48. 名詞から根へのパスの抽出\n",
        "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
        "- 各文節は（表層形の）形態素列で表現する\n",
        "- パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    ジョンマッカーシーは -> 作り出した\n",
        "    AIに関する -> 最初の -> 会議で -> 作り出した\n",
        "    最初の -> 会議で -> 作り出した\n",
        "    会議で -> 作り出した\n",
        "    人工知能という -> 用語を -> 作り出した\n",
        "    用語を -> 作り出した\n",
        "\n",
        "KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    ジョンマッカーシーは -> 作り出した\n",
        "    ＡＩに -> 関する -> 会議で -> 作り出した\n",
        "    会議で -> 作り出した\n",
        "    人工知能と -> いう -> 用語を -> 作り出した\n",
        "    用語を -> 作り出した"
      ],
      "metadata": {
        "id": "ASM7F7ZMGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XfoDNc3IGCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 49. 名詞間の係り受けパスの抽出\n",
        "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj`(i<j)`のとき，係り受けパスは以下の仕様を満たすものとする．\n",
        "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
        "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
        "\n",
        "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
        "\n",
        "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
        "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
        "\n",
        "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
        "    Xは | Yの -> 会議で | 作り出した\n",
        "    Xは | Yで | 作り出した\n",
        "    Xは | Yという -> 用語を | 作り出した\n",
        "    Xは | Yを | 作り出した\n",
        "    Xに関する -> Yの\n",
        "    Xに関する -> 最初の -> Yで\n",
        "    Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
        "    Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
        "    Xの -> Yで\n",
        "    Xの -> 会議で | Yという -> 用語を | 作り出した\n",
        "    Xの -> 会議で | Yを | 作り出した\n",
        "    Xで | Yという -> 用語を | 作り出した\n",
        "    Xで | Yを | 作り出した\n",
        "    Xという -> Yを\n",
        "\n",
        "KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
        "\n",
        "    Xは | Yに -> 関する -> 会議で | 作り出した。\n",
        "    Xは | Yで | 作り出した。\n",
        "    Xは | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xは | Yを | 作り出した。\n",
        "    Xに -> 関する -> Yで\n",
        "    Xに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xに -> 関する -> 会議で | Yを | 作り出した。\n",
        "    Xで | Yと -> いう -> 用語を | 作り出した。\n",
        "    Xで | Yを | 作り出した。\n",
        "    Xと -> いう -> Yを"
      ],
      "metadata": {
        "id": "Ely3RRkBGCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gsNXyAThGCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第6章: 機械学習"
      ],
      "metadata": {
        "id": "cNWf4DGTGUnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "zH78h42hGUnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxeItMqtGUn0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "jGlc3MXtGUn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VFTzIkC3GUn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "D6bg-elpGUn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d2jCPkqqGUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "DyUYiplYGUn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "py3mHPPMGUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "WBGDyhppGUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RohKtHJUGUn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "3W8gVsOqGUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HaEtVYZJGUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "hgOJHSCXGUn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cdan2_sdGUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "0QXqzLhTGUn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I81B5QhTGUn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Kmc1CZljGUn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FHuNDnDRGUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "JDxWM0EnGUn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QsC7jvbgGUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第7章: 単語ベクトル"
      ],
      "metadata": {
        "id": "fFWgl4_HGUn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "OVbyTLWpGUn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZ2_h-nGUn7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "5XMltajHGUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bgObeRJZGUn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "6f9guhGvGUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8rmuH7_YGUn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Qti4fMtgGUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2yn69ypIGUn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "0ZM9MJ8XGUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JooczH7ZGUn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "78YJXkQCGUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ar4cHATNGUn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "rEuas98LGUn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BjyVGREXGUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "emZ2W5TrGUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6NzushwwGUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "Yvs0rDyzGUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZYWCcB3QGUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "WfwSmXGcGUoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3ZqFnAP9GUoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第8章: ニューラルネット"
      ],
      "metadata": {
        "id": "NMOIVGPMGUoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "nUHYvHeRGUoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgvBFenNGUoB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "wguxJnK7GUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PKcYF6hqGUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "jCJCzpXnGUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XdTFCjbkGUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "pdkstkG4GUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y_KUWM71GUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "IHANJMStGUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9mLCYkR8GUoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "l892pPbAGUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LcBewpIiGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "udN-yYPuGUoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vPCNLDTzGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "aY6Qb7Z-GUoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0-IBSIsfGUoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "4sdE0PmCGUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3VQMfEdOGUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "ujNB-Nt6GUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8YTVV0N0GUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第9章: RNNとCNN"
      ],
      "metadata": {
        "id": "A_fskrYuGDe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "uVFVICBTGDe3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCe-W2miGDe4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "j_ek77AlGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RS4Aon27GDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "BpfRkFReGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d02RDDCSGDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "x9Zr__PdGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QRGzx3P3GDe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "HTbmQJbsGDe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MwLJUY7rGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "I26_r7nhGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wu-ZxkLBGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "RTrizMr7GDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JFMLkF8fGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "v49GPvdkGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YU9CoWcTGDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "RvlqPNHfGDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CH0Jxw1sGDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "S_n5DroHGDe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9O2SdvbNGDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第10章: 機械翻訳"
      ],
      "metadata": {
        "id": "HxRHhP-yGd-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### "
      ],
      "metadata": {
        "id": "EPolx2ELGd-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4X5oTb2Gd-T"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "JpIxmETQGd-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oz0QHcCAGd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "GsEmSl2yGd-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_iNZMWz3Gd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "M9x9C5g-Gd-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OI3QSandGd-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "TJY2NdzDGd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TzFDDn8WGd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "na9B1uP3Gd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gRpFpJbbGd-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "VgUT4sUGGd-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wbx-9ATdGd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "f_NdGNIrGd-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V4XQOeVtGd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "E4A_wEhxGd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHj95k5AGd-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "c-wQyjZOGd-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4wFHPnPmGd-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}